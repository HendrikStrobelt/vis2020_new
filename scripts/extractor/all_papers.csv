Conference,Year,Title,DOI,Link,FirstPage,LastPage,PaperType,Abstract,AuthorNames-Deduped,AuthorNames,AuthorAffiliation,InternalReferences,AuthorKeywords,AminerCitationCount_02-2020,AminerCitationCount_06-2020,XploreCitationCount - 2020-01,PubsCited,Award,pdf_local
VAST,2018,TPFlow: Progressive Partition and Multidimensional Pattern Extraction for Large-Scale Spatio-Temporal Data Analysis,10.1109/TVCG.2018.2865018,http://dx.doi.org/10.1109/TVCG.2018.2865018,1,11,J,"Consider a multi-dimensional spatio-temporal (ST) dataset where each entry is a numerical measure defined by the corresponding temporal, spatial and other domain-specific dimensions. A typical approach to explore such data utilizes interactive visualizations with multiple coordinated views. Each view displays the aggregated measures along one or two dimensions. By brushing on the views, analysts can obtain detailed information. However, this approach often cannot provide sufficient guidance for analysts to identify patterns hidden within subsets of data. Without a priori hypotheses, analysts need to manually select and iterate through different slices to search for patterns, which can be a tedious and lengthy process. In this work, we model multidimensional ST data as tensors and propose a novel piecewise rank-one tensor decomposition algorithm which supports automatically slicing the data into homogeneous partitions and extracting the latent patterns in each partition for comparison and visual summarization. The algorithm optimizes a quantitative measure about how faithfully the extracted patterns visually represent the original data. Based on the algorithm we further propose a visual analytics framework that supports a top-down, progressive partitioning workflow for level-of-detail multidimensional ST data exploration. We demonstrate the general applicability and effectiveness of our technique on three datasets from different application domains: regional sales trend analysis, customer traffic analysis in department stores, and taxi trip analysis with origin-destination (OD) data. We further interview domain experts to verify the usability of the prototype.",Dongyu Liu;Panpan Xu;Liu Ren,Dongyu Liu;Panpan Xu;Liu Ren,"Hong Kong University of Science and Technology;Bosch Research North America, Sunnyvale, CA;Bosch Research North America, Sunnyvale, CA",10.1109/TVCG.2016.2598862;10.1109/TVCG.2017.2744419;10.1109/TVCG.2006.161;10.1109/TVCG.2014.2346449;10.1109/TVCG.2013.226;10.1109/TVCG.2013.179;10.1109/TVCG.2016.2598432;10.1109/TVCG.2016.2598831;10.1109/TVCG.2016.2598624;10.1109/TVCG.2017.2744805;10.1109/TVCG.2015.2467112;10.1109/TVCG.2014.2346574;10.1109/INFVIS.2003.1249018;10.1109/INFVIS.1999.801851;10.1109/TVCG.2015.2468111;10.1109/TVCG.2018.2865126;10.1109/TVCG.2007.70515,"Spatio-temporal data,tensor decomposition,interactive exploration,automatic pattern discoveries",0.0,13.0,2.0,67.0,BP,papers/vast-tvcg/1_liu.pdf
InfoVis,2018,Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco,10.1109/TVCG.2018.2865240,http://dx.doi.org/10.1109/TVCG.2018.2865240,438,448,J,"There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.",Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer,Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer,University of Washington;University of Washington;University of Washington;University of Washington;University of California Santa Cruz;University of Washington;University of Washington,10.1109/INFVIS.2005.1532136;10.1109/TVCG.2014.2346984;10.1109/TVCG.2013.183;10.1109/TVCG.2014.2346979;10.1109/TVCG.2007.70594;10.1109/TVCG.2017.2744320;10.1109/TVCG.2017.2744198;10.1109/TVCG.2017.2744198;10.1109/TVCG.2016.2599030;10.1109/TVCG.2017.2744359;10.1109/TVCG.2015.2467191,"Automated Visualization Design,Perceptual Effectiveness,Constraints,Knowledge Bases,Answer Set Programming",1.0,30.0,4.0,67.0,BP,papers/infovis-tvcg/438_moritz.pdf
SciVis,2018,Deadeye: A Novel Preattentive Visualization Technique Based on Dichoptic Presentation,10.1109/TVCG.2018.2864498,http://dx.doi.org/10.1109/TVCG.2018.2864498,936,945,J,"Preattentive visual features such as hue or flickering can effectively draw attention to an object of interest - for instance, an important feature in a scientific visualization. These features appear to pop out and can be recognized by our visual system, independently from the number of distractors. Most cues do not take advantage of the fact that most humans have two eyes. In cases where binocular vision is applied, it is almost exclusively used to convey depth by exposing stereo pairs. We present Deadeye, a novel preattentive visualization technique based on presenting different stimuli to each eye. The target object is rendered for one eye only and is instantly detected by our visual system. In contrast to existing cues, Deadeye does not modify any visual properties of the target and, thus, is particularly suited for visualization applications. Our evaluation confirms that Deadeye is indeed perceived preattentively. We also explore a conjunction search based on our technique and show that, in contrast to 3D depth, the task cannot be processed in parallel.",Andrey Krekhov;Jens H. Krüger,Andrey Krekhov;Jens Krüger,Center of Visual Data Analysis and Computer Graphics (COVIDAG)University of Duisburg-Essen;Center of Visual Data Analysis and Computer Graphics (COVIDAG)University of Duisburg-Essen,10.1109/TVCG.2011.234;10.1109/VISUAL.2005.1532838;10.1109/TVCG.2014.2346352,"Popout,preattentive vision,comparative visualization,dichoptic presentation",0.0,2.0,1.0,62.0,BP,papers/scivis-tvcg/936_krekhov.pdf
VAST,2018,Evaluating Multi-Dimensional Visualizations for Understanding Fuzzy Clusters,10.1109/TVCG.2018.2865020,http://dx.doi.org/10.1109/TVCG.2018.2865020,12,21,J,"Fuzzy clustering assigns a probability of membership for a datum to a cluster, which veritably reflects real-world clustering scenarios but significantly increases the complexity of understanding fuzzy clusters. Many studies have demonstrated that visualization techniques for multi-dimensional data are beneficial to understand fuzzy clusters. However, no empirical evidence exists on the effectiveness and efficiency of these visualization techniques in solving analytical tasks featured by fuzzy clusters. In this paper, we conduct a controlled experiment to evaluate the ability of fuzzy clusters analysis to use four multi-dimensional visualization techniques, namely, parallel coordinate plot, scatterplot matrix, principal component analysis, and Radviz. First, we define the analytical tasks and their representative questions specific to fuzzy clusters analysis. Then, we design objective questionnaires to compare the accuracy, time, and satisfaction in using the four techniques to solve the questions. We also design subjective questionnaires to collect the experience of the volunteers with the four techniques in terms of ease of use, informativeness, and helpfulness. With a complete experiment process and a detailed result analysis, we test against four hypotheses that are formulated on the basis of our experience, and provide instructive guidance for analysts in selecting appropriate and efficient visualization techniques to analyze fuzzy clusters.",Ying Zhao 0001;Feng Luo;Minghui Chen;Yingchao Wang;Jiazhi Xia;Fangfang Zhou;Yunhai Wang;Yi Chen 0007;Wei Chen 0001,Ying Zhao;Feng Luo;Minghui Chen;Yingchao Wang;Jiazhi Xia;Fangfang Zhou;Yunhai Wang;Yi Chen;Wei Chen,Central South University;Central South University;Central South University;Central South University;Central South University;Central South University;Shandong University;Beijing TechnologyBusiness University;State Key Lab of CAD & CGZhejiang University,10.1109/INFVIS.2005.1532136;10.1109/INFVIS.1998.729559;10.1109/TVCG.2017.2745138;10.1109/VAST.2010.5652450;10.1109/VISUAL.1997.663916;10.1109/TVCG.2009.153;10.1109/TVCG.2016.2598831;10.1109/INFVIS.2004.15;10.1109/TVCG.2017.2744198;10.1109/TVCG.2015.2467324;10.1109/TVCG.2013.153;10.1109/TVCG.2008.173;10.1109/VISUAL.1990.146375;10.1109/TVCG.2017.2744098;10.1109/TVCG.2016.2598479;10.1109/INFVIS.2003.1249015,"Evaluation,multi-dimensional visualization,fuzzy clustering,parallel coordinate plot,scatterplot matrix,principal component analysis,radviz",0.0,16.0,6.0,63.0,nan,papers/vast-tvcg/12_zhao.pdf
VAST,2018,The Effect of Proximity in Social Data Charts on Perceived Unity,10.1109/VAST.2018.8802449,http://dx.doi.org/10.1109/VAST.2018.8802449,1,12,C,"Social data charts - visual presentations of quantitative data about peer behaviors - may offer new means to motivate individuals to participate in group goals. However, to do so these charts need to create a semantic response of `unity' among the chart viewers in order to overcome the problems of social loafing where people act selfishly and undervalue the group's goal. In this paper, we focus on two properties of social data charts that may affect a viewer's perceptions of unity: (1) The skewness in the data structure - the statistical distribution of the social data, and (2) the proximity in the visual structure of the chart - the spatial organization of the data points. We performed a controlled perceptual experiment to examine the effect of proximity and skewness on four different semantic facets of perceived group unity: similarity, entitativity, rapport, and centrality. We exposed 179 participants on Amazon Mechanical Turk to different group charts using a 2 x 2 factorial design, varying both proximity and skewness. Our two-way ANCOVA analyses reveal three important findings: (1) Across all conditions, proximity has a strong positive effect on perceived group unity and conveys a social meaning of group entitativity, as well as, member similarity and rapport; (2) Skewness and proximity interact in a non-linear way, suggesting that skewness creates a negative force that modifies the semantic responses to high proximity; and (3) The perceptual responses to proximity have different semantic facets that are either stable or sensitive to skewness. These findings contribute to perceptual as well as social InfoVis literature.",Marlen Promann;Sabine Brunswicker,Marlen Promann;Sabine Brunswicker,Purdue University;Purdue University,10.1109/TVCG.2017.2746018;10.1109/TVCG.2008.171;10.1109/TVCG.2014.2346419;10.1109/TVCG.2007.70541;10.1109/TVCG.2013.234;10.1109/TVCG.2010.174;10.1109/TVCG.2012.221;10.1109/TVCG.2017.2745240,"Social visualization,information visualization,Human-centered computing,Empirical studies in HCI,Displays and imagers,Social engineering (social sciences),Visualization theory, concepts and paradigms,Gestalt theory,controlled experiment,data density,data spread,perception,group identity,unity,priming",nan,0.0,27.0,0.0,nan,papers/vast-conference/promann.pdf
VAST,2018,Futzing and Moseying: Interviews with Professional Data Analysts on Exploration Practices,10.1109/TVCG.2018.2865040,http://dx.doi.org/10.1109/TVCG.2018.2865040,22,31,J,"We report the results of interviewing thirty professional data analysts working in a range of industrial, academic, and regulatory environments. This study focuses on participants' descriptions of exploratory activities and tool usage in these activities. Highlights of the findings include: distinctions between exploration as a precursor to more directed analysis versus truly open-ended exploration; confirmation that some analysts see “finding something interesting” as a valid goal of data exploration while others explicitly disavow this goal; conflicting views about the role of intelligent tools in data exploration; and pervasive use of visualization for exploration, but with only a subset using direct manipulation interfaces. These findings provide guidelines for future tool development, as well as a better understanding of the meaning of the term “data exploration” based on the words of practitioners “in the wild”.",Sara Alspaugh;Nava Zokaei;Andrea Liu;Cindy Jin;Marti A. Hearst,Sara Alspaugh;Nava Zokaei;Andrea Liu;Cindy Jin;Marti A. Hearst,"UC, Berkeley;UC, Berkeley;UC, Berkeley;UC, Berkeley;UC, Berkeley",10.1109/VAST.2008.4677365;10.1109/INFVIS.1997.636793;10.1109/TVCG.2012.219;10.1109/VAST.2011.6102438;10.1109/TVCG.2006.122;10.1109/TVCG.2015.2467191,"EDA,exploratory data analysis,interview study,visual analytics tools",nan,7.0,3.0,26.0,nan,papers/vast-tvcg/22_alspaugh.pdf
VAST,2018,The Effect of Semantic Interaction on Foraging in Text Analysis,10.1109/VAST.2018.8802424,http://dx.doi.org/10.1109/VAST.2018.8802424,13,24,C,"Completing text analysis tasks is a continuous sensemaking loop of foraging for information and incrementally synthesizing it into hypotheses. Past research has shown the advantages of using spatial workspaces as a means for synthesizing information through externalizing hypotheses and creating spatial schemas. However, spatializing the entirety of datasets becomes prohibitive as the number of documents available to the analysts grows, particularly when only a small subset are relevant to the task at hand. StarSPIRE is a visual analytics tool designed to explore collections of documents, leveraging users' semantic interactions to steer (1) a synthesis model that aids in document layout, and (2) a foraging model to automatically retrieve new relevant information. In contrast to traditional keyword search foraging (KSF), “semantic interaction foraging” (SIF) occurs as a result of the user's synthesis actions. To quantify the value of semantic interaction foraging, we use StarSPIRE to evaluate its utility for an intelligence analysis sensemaking task. Semantic interaction foraging accounted for 26% of useful documents found, and it also resulted in increased synthesis interactions and improved sensemaking task performance by users in comparison to only using keyword search.",John E. Wenskovitch;Lauren Bradel;Michelle Dowling;Leanna House;Chris North,John Wenskovitch;Lauren Bradel;Michelle Dowling;Leanna House;Chris North,Virginia Tech Computer Science;Department of Defense;Virginia Tech Computer Science;Virginia Tech Statistics;Virginia Tech Computer Science,10.1109/TVCG.2006.120;10.1109/VAST.2012.6400559;10.1109/TVCG.2013.205;10.1109/VAST.2008.4677362;10.1109/VAST.2014.7042492;10.1109/VAST.2012.6400486;10.1109/TVCG.2012.260;10.1109/VAST.2011.6102449;10.1109/INFVIS.2005.1532126;10.1109/INFVIS.1995.528686;10.1109/INFVIS.2004.37,"Human-centered computing,Visualization,Empirical studies in visualization,Human-centered computing,Visualization,Visual analytics",nan,1.0,29.0,0.0,nan,papers/vast-conference/wenskovitch.pdf
VAST,2018,An Information-Theoretic Approach to the Cost-benefit Analysis of Visualization in Virtual Environments,10.1109/TVCG.2018.2865025,http://dx.doi.org/10.1109/TVCG.2018.2865025,32,42,J,"Visualization and virtual environments (VEs) have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice have yet to benefit from the capacity of VEs. In this paper, we examine this status quo from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems (including augmented and mixed reality, theater-based systems, and large powerwalls), to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs.",Min Chen 0001;Kelly P. Gaither;Nigel W. John;Brian McCann,Min Chen;Kelly Gaither;Nigel W. John;Brian Mccann,"University of Oxford, UK;University of Texas, Austin, USA;University of Chester, UK;University of Texas, Austin, USA",10.1109/TVCG.2010.131;10.1109/TVCG.2008.142;10.1109/TVCG.2011.231;10.1109/TVCG.2014.2346325;10.1109/TVCG.2013.127;10.1109/TVCG.2016.2598829;10.1109/INFVIS.2004.59;10.1109/VISUAL.2005.1532781;10.1109/TVCG.2010.131;10.1109/TVCG.2006.184,"Theory of visualization,virtual environments,four levels of visualization,virtual reality,augmented reality,mixed reality,cost-benefit analysis,information theory,cognitive sciences,visualization applications,immersive analytics",0.0,2.0,1.0,119.0,nan,papers/vast-tvcg/32_chen.pdf
InfoVis,2018,Shape-preserving Star Coordinates,10.1109/TVCG.2018.2865118,http://dx.doi.org/10.1109/TVCG.2018.2865118,449,458,J,"Dimensionality reduction is commonly applied to multidimensional data to reduce the complexity of their analysis. In visual analysis systems, projections embed multidimensional data into 2D or 3D spaces for graphical representation. To facilitate a robust and accurate analysis, essential characteristics of the multidimensional data shall be preserved when projecting. Orthographic star coordinates is a state-of-the-art linear projection method that avoids distortion of multidimensional clusters by restricting interactive exploration to orthographic projections. However, existing numerical methods for computing orthographic star coordinates have a number of limitations when putting them into practice. We overcome these limitations by proposing the novel concept of shape-preserving star coordinates where shape preservation is assured using a superset of orthographic projections. Our scheme is explicit, exact, simple, fast, parameter-free, and stable. To maintain a valid shape-preserving star-coordinates configuration during user interaction with one of the star-coordinates axes, we derive an algorithm that only requires us to modify the configuration of one additional compensatory axis. Different design goals can be targeted by using different strategies for selecting the compensatory axis. We propose and discuss four strategies including a strategy that approximates orthographic star coordinates very well and a data-driven strategy. We further present shape-preserving morphing strategies between two shape-preserving configurations, which can be adapted for the generation of data tours. We apply our concept to multiple data analysis scenarios to document its applicability and validate its desired properties.",Vladimir Molchanov;Lars Linsen,Vladimir Molchanov;Lars Linsen,"Westfälische Wilhelms-Universität Münster, Germany;Westfälische Wilhelms-Universität Münster, Germany",10.1109/TVCG.2010.209;10.1109/TVCG.2013.182;10.1109/TVCG.2015.2467132;10.1109/TVCG.2015.2467324;10.1109/TVCG.2014.2346258,"Star coordinates,multidimensional data projection,multivariate data visualization",0.0,1.0,0.0,30.0,nan,papers/infovis-tvcg/449_molchanov.pdf
InfoVis,2018,SRVis: Towards Better Spatial Integration in Ranking Visualization,10.1109/TVCG.2018.2865126,http://dx.doi.org/10.1109/TVCG.2018.2865126,459,469,J,"Interactive ranking techniques have substantially promoted analysts' ability in making judicious and informed decisions effectively based on multiple criteria. However, the existing techniques cannot satisfactorily support the analysis tasks involved in ranking large-scale spatial alternatives, such as selecting optimal locations for chain stores, where the complex spatial contexts involved are essential to the decision-making process. Limitations observed in the prior attempts of integrating rankings with spatial contexts motivate us to develop a context-integrated visual ranking technique. Based on a set of generic design requirements we summarized by collaborating with domain experts, we propose SRVis, a novel spatial ranking visualization technique that supports efficient spatial multi-criteria decision-making processes by addressing three major challenges in the aforementioned context integration, namely, a) the presentation of spatial rankings and contexts, b) the scalability of rankings' visual representations, and c) the analysis of context-integrated spatial rankings. Specifically, we encode massive rankings and their cause with scalable matrix-based visualizations and stacked bar charts based on a novel two-phase optimization framework that minimizes the information loss, and the flexible spatial filtering and intuitive comparative analysis are adopted to enable the in-depth evaluation of the rankings and assist users in selecting the best spatial alternative. The effectiveness of the proposed technique has been evaluated and demonstrated with an empirical study of optimization methods, two case studies, and expert interviews.",Di Weng;Ran Chen;Zikun Deng;Feiran Wu;Jingmin Chen;Yingcai Wu,Di Weng;Ran Chen;Zikun Deng;Feiran Wu;Jingmin Chen;Yingcai Wu,"State Key Lab of CAD & CGZhejiang UniversityAlibaba-Zhejiang University JointInstitute of Frontier Technologies;State Key Lab of CAD & CGZhejiang UniversityAlibaba-Zhejiang University JointInstitute of Frontier Technologies;State Key Lab of CAD & CGZhejiang UniversityAlibaba-Zhejiang University JointInstitute of Frontier Technologies;Alibaba Group, Hangzhou, China;Alibaba Group, Hangzhou, China;State Key Lab of CAD & CGZhejiang UniversityAlibaba-Zhejiang University JointInstitute of Frontier Technologies",10.1109/TVCG.2016.2598416;10.1109/TVCG.2013.193;10.1109/TVCG.2011.185;10.1109/TVCG.2008.166;10.1109/TVCG.2014.2346594;10.1109/TVCG.2013.173;10.1109/TVCG.2015.2467771;10.1109/TVCG.2008.181;10.1109/TVCG.2016.2598432;10.1109/TVCG.2018.2865018;10.1109/VAST.2011.6102455;10.1109/TVCG.2016.2598831;10.1109/TVCG.2016.2598585;10.1109/TVCG.2009.111;10.1109/TVCG.2015.2467112;10.1109/TVCG.2012.253;10.1109/TVCG.2015.2467717;10.1109/TVCG.2017.2745078;10.1109/TVCG.2014.2346913,"Spatial ranking,visualization",0.0,2.0,4.0,60.0,nan,papers/infovis-tvcg/459_weng.pdf
InfoVis,2018,A Declarative Rendering Model for Multiclass Density Maps,10.1109/TVCG.2018.2865141,http://dx.doi.org/10.1109/TVCG.2018.2865141,470,480,J,"Multiclass maps are scatterplots, multidimensional projections, or thematic geographic maps where data points have a categorical attribute in addition to two quantitative attributes. This categorical attribute is often rendered using shape or color, which does not scale when overplotting occurs. When the number of data points increases, multiclass maps must resort to data aggregation to remain readable. We present multiclass density maps: multiple 2D histograms computed for each of the category values. Multiclass density maps are meant as a building block to improve the expressiveness and scalability of multiclass map visualization. In this article, we first present a short survey of aggregated multiclass maps, mainly from cartography. We then introduce a declarative model-a simple yet expressive JSON grammar associated with visual semantics-that specifies a wide design space of visualizations for multiclass density maps. Our declarative model is expressive and can be efficiently implemented in visualization front-ends such as modern web browsers. Furthermore, it can be reconfigured dynamically to support data exploration tasks without recomputing the raw data. Finally, we demonstrate how our model can be used to reproduce examples from the past and support exploring data at scale.",Jaemin Jo;Frédéric Vernier;Pierre Dragicevic;Jean-Daniel Fekete,Jaemin Jo;Frédéric Vernier;Pierre Dragicevic;Jean-Daniel Fekete,"Seoul National University, Republic of Korea;LIMSICNRSUniv. Paris-SudUniversité Paris-Saclay;Inria;Inria",10.1109/TVCG.2011.185;10.1109/INFVIS.2002.1173156;10.1109/TVCG.2017.2744199;10.1109/TVCG.2007.70623;10.1109/TVCG.2013.179;10.1109/TVCG.2013.130;10.1109/TVCG.2017.2744184;10.1109/TVCG.2016.2599030;10.1109/TVCG.2011.197;10.1109/VISUAL.1993.398863;10.1109/TVCG.2009.175,"Scalability,multiclass scatterplots,density maps,aggregation,declarative specification,visualization grammar",nan,5.0,5.0,54.0,nan,papers/infovis-tvcg/470_jo.pdf
InfoVis,2018,DimReader: Axis lines that explain non-linear projections,10.1109/TVCG.2018.2865194,http://dx.doi.org/10.1109/TVCG.2018.2865194,481,490,J,"Non-linear dimensionality reduction (NDR) methods such as LLE and t-SNE are popular with visualization researchers and experienced data analysts, but present serious problems of interpretation. In this paper, we present DimReader, a technique that recovers readable axes from such techniques. DimReader is based on analyzing infinitesimal perturbations of the dataset with respect to variables of interest. The perturbations define exactly how we want to change each point in the original dataset and we measure the effect that these changes have on the projection. The recovered axes are in direct analogy with the axis lines (grid lines) of traditional scatterplots. We also present methods for discovering perturbations on the input data that change the projection the most. The calculation of the perturbations is efficient and easily integrated into programs written in modern programming languages. We present results of DimReader on a variety of NDR methods and datasets both synthetic and real-life, and show how it can be used to compare different NDR methods. Finally, we discuss limitations of our proposal and situations where further research is needed.",Rebecca Faust;David Glickenstein;Carlos Scheidegger,Rebecca Faust;David Glickenstein;Carlos Scheidegger,University of Arizona;University of Arizona;University of Arizona,10.1109/TVCG.2014.2346984;10.1109/TVCG.2014.2346431;10.1109/TVCG.2013.124;10.1109/VISUAL.1996.567787;10.1109/VAST.2010.5652460;10.1109/TVCG.2015.2467552;10.1109/VAST.2011.6102449;10.1109/TVCG.2013.157;10.1109/TVCG.2011.220;10.1109/TVCG.2014.2346325;10.1109/INFVIS.2002.1173161;10.1109/TVCG.2016.2598495;10.1109/TVCG.2013.153;10.1109/TVCG.2015.2467717,"Non-linear dimensionality reduction,auto-differentiation",0.0,0.0,1.0,52.0,nan,papers/infovis-tvcg/481_faust.pdf
SciVis,2018,Recirculation Surfaces for Flow Visualization,10.1109/TVCG.2018.2864813,http://dx.doi.org/10.1109/TVCG.2018.2864813,946,955,J,"We present a formal approach to the visual analysis of recirculation in flows by introducing recirculation surfaces for 3D unsteady flow fields. Recirculation surfaces are the loci where massless particle integration returns to its starting point after some variable, finite integration. We give a rigorous definition of recirculation surfaces as 2-manifolds embedded in 5D space and study their properties. Based on this we construct an algorithm for their extraction, which searches for intersections of a recirculation surface with lines defined in 3D. This reduces the problem to a repeated search for critical points in 3D vector fields. We provide a uniform sampling of the search space paired with a surface reconstruction and visualize results. This way, we present the first algorithm for a comprehensive feature extraction in the 5D flow map of a 3D flow. The problem of finding isolated closed orbits in steady vector fields occurs as a special case of recirculation surfaces. This includes isolated closed orbits with saddle behavior. We show recirculation surfaces for a number of artificial and real flow data sets.",Thomas Wilde;Christian Rossi;Holger Theisel,Thomas Wilde;Christian Rössi;Holger Theisel,Visual Computing groupUniversity of Magdeburg;Visual Computing groupUniversity of Magdeburg;Visual Computing groupUniversity of Magdeburg,10.1109/VISUAL.2004.107;10.1109/TVCG.2009.177;10.1109/VISUAL.2004.59;10.1109/VISUAL.2002.1183786;10.1109/TVCG.2008.163,"Flow visualization,recirculation,unsteady flow",0.0,2.0,1.0,41.0,nan,papers/scivis-tvcg/946_wilde.pdf
SciVis,2018,Objective Vortex Corelines of Finite-sized Objects in Fluid Flows,10.1109/TVCG.2018.2864828,http://dx.doi.org/10.1109/TVCG.2018.2864828,956,966,J,"Vortices are one of the most-frequently studied phenomena in fluid flows. The center of the rotating motion is called the vortex coreline and its successful detection strongly depends on the choice of the reference frame. The optimal frame moves with the center of the vortex, which incidentally makes the observed fluid flow steady and thus standard vortex coreline extractors such as Sujudi-Haimes become applicable. Recently, an objective optimization framework was proposed that determines a near-steady reference frame for tracer particles. In this paper, we extend this technique to the detection of vortex corelines of inertial particles. An inertial particle is a finite-sized object that is carried by a fluid flow. In contrast to the usual tracer particles, they do not move tangentially with the flow, since they are subject to gravity and exhibit mass-dependent inertia. Their particle state is determined by their position and own velocity, which makes the search for the optimal frame a high-dimensional problem. We demonstrate in this paper that the objective detection of an inertial vortex coreline can be reduced in 2D to a critical point search in 2D. For 3D flows, however, the vortex coreline criterion remains a parallel vectors condition in 6D. To detect the vortex corelines we propose a recursive subdivision approach that is tailored to the underlying structure of the 6D vectors. The resulting algorithm is objective, and we demonstrate the vortex coreline extraction in a number of 2D and 3D vector fields.",Tobias Günther;Holger Theisel,Tobias Günther;Holger Theisel,Computer Graphics LaboratoryETH Zürich;Visual Computing GroupUniversity of Magdeburg,10.1109/VISUAL.1991.175773;10.1109/TVCG.2015.2467200;10.1109/TVCG.2014.2346415;10.1109/TVCG.2016.2599016;10.1109/VISUAL.1997.663910;10.1109/VISUAL.1999.809896;10.1109/VISUAL.1998.745296;10.1109/TVCG.2016.2599018;10.1109/VISUAL.2005.1532851;10.1109/TVCG.2007.70545,"Vortex extraction,inertial particles,objectivity,vortex coreline",nan,3.0,0.0,81.0,nan,papers/scivis-tvcg/956_guenther.pdf
VAST,2018,Visual Abstraction of Large Scale Geospatial Origin-Destination Movement Data,10.1109/TVCG.2018.2864503,http://dx.doi.org/10.1109/TVCG.2018.2864503,43,53,J,"A variety of human movement datasets are represented in an Origin-Destination(OD) form, such as taxi trips, mobile phone locations, etc. As a commonly-used method to visualize OD data, flow map always fails to discover patterns of human mobility, due to massive intersections and occlusions of lines on a 2D geographical map. A large number of techniques have been proposed to reduce visual clutter of flow maps, such as filtering, clustering and edge bundling, but the correlations of OD flows are often neglected, which makes the simplified OD flow map present little semantic information. In this paper, a characterization of OD flows is established based on an analogy between OD flows and natural language processing (NPL) terms. Then, an iterative multi-objective sampling scheme is designed to select OD flows in a vectorized representation space. To enhance the readability of sampled OD flows, a set of meaningful visual encodings are designed to present the interactions of OD flows. We design and implement a visual exploration system that supports visual inspection and quantitative evaluation from a variety of perspectives. Case studies based on real-world datasets and interviews with domain experts have demonstrated the effectiveness of our system in reducing the visual clutter and enhancing correlations of OD flows.",Zhiguang Zhou;Linhao Meng;Cheng Tang;Ying Zhao 0001;Zhiyong Guo;Miaoxin Hu;Wei Chen 0001,Zhiguang Zhou;Linhao Meng;Cheng Tang;Ying Zhao;Zhiyong Guo;Miaoxin Hu;Wei Chen,School of InformationZhejiang University of Finance and Economics;State Key Lab of CAD & CGZhejiang University;Information SchoolZhejiang Sci-tech University;Central South University;School of InformationZhejiang University of Finance and Economics;School of InformationZhejiang University of Finance and Economics;State Key Lab of CAD & CGZhejiang University,10.1109/TVCG.2017.2744322;10.1109/TVCG.2016.2598667;10.1109/TVCG.2011.202;10.1109/TVCG.2014.2346594;10.1109/TVCG.2008.135;10.1109/TVCG.2011.233;10.1109/TVCG.2013.226;10.1109/TVCG.2009.143;10.1109/TVCG.2014.2346271;10.1109/TVCG.2016.2598432;10.1109/TVCG.2013.196;10.1109/INFVIS.2005.1532150;10.1109/TVCG.2015.2467691;10.1109/TVCG.2014.2346746;10.1109/TVCG.2016.2598885,"Visual abstraction,human mobility,origin-destination,flow map,representation learning",0.0,12.0,12.0,51.0,nan,papers/vast-tvcg/43_zhou.pdf
VAST,2018,Analysis of Flight Variability: a Systematic Approach,10.1109/TVCG.2018.2864811,http://dx.doi.org/10.1109/TVCG.2018.2864811,54,64,J,"In movement data analysis, there exists a problem of comparing multiple trajectories of moving objects to common or distinct reference trajectories. We introduce a general conceptual framework for comparative analysis of trajectories and an analytical procedure, which consists of (1) finding corresponding points in pairs of trajectories, (2) computation of pairwise difference measures, and (3) interactive visual analysis of the distributions of the differences with respect to space, time, set of moving objects, trajectory structures, and spatio-temporal context. We propose a combination of visualisation, interaction, and data transformation techniques supporting the analysis and demonstrate the use of our approach for solving a challenging problem from the aviation domain.",Natalia V. Andrienko;Gennady L. Andrienko;Jose Manuel Cordero Garcia;David Scarlatti,Natalia Andrienko;Gennady Andrienko;Jose Manuel Cordero Garcia;David Scarlatti,"Fraunhofer IAISCity, University of London;Fraunhofer IAISCity, University of London;CRIDA (Reference Center for Research, Development and Innovation in ATM);Boeing Research & Development Europe",10.1109/VAST.2008.4677356;10.1109/VAST.2010.5653580;10.1109/TVCG.2017.2744322;10.1109/TVCG.2013.193;10.1109/TVCG.2015.2467851;10.1109/TVCG.2011.233;10.1109/TVCG.2012.265;10.1109/TVCG.2015.2468078,"Visual analytics,movement data,flight trajectories",0.0,5.0,2.0,55.0,nan,papers/vast-tvcg/54_andrienko.pdf
VAST,2018,ForVizor: Visualizing Spatio-Temporal Team Formations in Soccer,10.1109/TVCG.2018.2865041,http://dx.doi.org/10.1109/TVCG.2018.2865041,65,75,J,"Regarded as a high-level tactic in soccer, a team formation assigns players different tasks and indicates their active regions on the pitch, thereby influencing the team performance significantly. Analysis of formations in soccer has become particularly indispensable for soccer analysts. However, formations of a team are intrinsically time-varying and contain inherent spatial information. The spatio-temporal nature of formations and other characteristics of soccer data, such as multivariate features, make analysis of formations in soccer a challenging problem. In this study, we closely worked with domain experts to characterize domain problems of formation analysis in soccer and formulated several design goals. We design a novel spatio-temporal visual representation of changes in team formation, allowing analysts to visually analyze the evolution of formations and track the spatial flow of players within formations over time. Based on the new design, we further design and develop ForVizor, a visual analytics system, which empowers users to track the spatio-temporal changes in formation and understand how and why such changes occur. With ForVizor, domain experts conduct formation analysis of two games. Analysis results with insights and useful feedback are summarized in two case studies.",Yingcai Wu;Xiao Xie;Jiachen Wang;Dazhen Deng;Hongye Liang;Hui Zhang;Shoubin Cheng;Wei Chen 0001,Yingcai Wu;Xiao Xie;Jiachen Wang;Dazhen Deng;Hongye Liang;Hui Zhang;Shoubin Cheng;Wei Chen,State Key Lab of CAD&CGZhejiang University;State Key Lab of CAD&CGZhejiang University;State Key Lab of CAD&CGZhejiang University;State Key Lab of CAD&CGZhejiang University;State Key Lab of CAD&CGZhejiang University;Department of Sport ScienceZhejiang University;Department of Sport ScienceZhejiang University;State Key Lab of CAD&CGZhejiang University,10.1109/VAST.2008.4677356;10.1109/TVCG.2011.239;10.1109/TVCG.2014.2346433;10.1109/VAST.2014.7042477;10.1109/TVCG.2018.2865018;10.1109/TVCG.2016.2598831;10.1109/TVCG.2013.192;10.1109/TVCG.2014.2346445;10.1109/TVCG.2017.2745181;10.1109/TVCG.2017.2744218,"Soccer data,formation analysis,spatio-temporal visualization",0.0,11.0,5.0,48.0,nan,papers/vast-tvcg/65_wu.pdf
VAST,2018,MotionRugs: Visualizing Collective Trends in Space and Time,10.1109/TVCG.2018.2865049,http://dx.doi.org/10.1109/TVCG.2018.2865049,76,86,J,"Understanding the movement patterns of collectives, such as flocks of birds or fish swarms, is an interesting open research question. The collectives are driven by mutual objectives or react to individual direction changes and external influence factors and stimuli. The challenge in visualizing collective movement data is to show space and time of hundreds of movements at the same time to enable the detection of spatiotemporal patterns. In this paper, we propose MotionRugs, a novel space efficient technique for visualizing moving groups of entities. Building upon established space-partitioning strategies, our approach reduces the spatial dimensions in each time step to a one-dimensional ordered representation of the individual entities. By design, MotionRugs provides an overlap-free, compact overview of the development of group movements over time and thus, enables analysts to visually identify and explore group-specific temporal patterns. We demonstrate the usefulness of our approach in the field of fish swarm analysis and report on initial feedback of domain experts from the field of collective behavior.",Juri Buchmüller;Dominik Jäckle;Eren Cakmak;Ulrik Brandes;Daniel A. Keim,Juri Buchmüller;Dominik Jäckle;Eren Cakmak;Ulrik Brandes;Daniel A. Keim,"University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;ETH Zurich, Switzerland;University of Konstanz, Germany",10.1109/TVCG.2013.193;10.1109/TVCG.2011.226;10.1109/VAST.2009.5332593;10.1109/TVCG.2009.145;10.1109/VAST.2014.7042477;10.1109/TVCG.2006.193;10.1109/TVCG.2013.192;10.1109/TVCG.2008.125;10.1109/TVCG.2012.265,"Spatio-Temporal Visualization,Spatial Abstraction,Spatial Index Structures,Collective Movement",0.0,5.0,4.0,54.0,nan,papers/vast-tvcg/76_buchmuller.pdf
VAST,2018,Identification of Temporally Varying Areas of Interest in Long-Duration Eye-Tracking Data Sets,10.1109/TVCG.2018.2865042,http://dx.doi.org/10.1109/TVCG.2018.2865042,87,97,J,"Eye-tracking has become an invaluable tool for the analysis of working practices in many technological fields of activity. Typically studies focus on short tasks and use static expected areas of interest (AoI) in the display to explore subjects' behaviour, making the analyst's task quite straightforward. In long-duration studies, where the observations may last several hours over a complete work session, the AoIs may change over time in response to altering workload, emergencies or other variables making the analysis more difficult. This work puts forward a novel method to automatically identify spatial AoIs changing over time through a combination of clustering and cluster merging in the temporal domain. A visual analysis system based on the proposed methods is also presented. Finally, we illustrate our approach within the domain of air traffic control, a complex task sensitive to prevailing conditions over long durations, though it is applicable to other domains such as monitoring of complex systems.",Prithiviraj K. Muthumanickam;Katerina Vrotsou;Aida Nordman;Jimmy Johansson;Matthew D. Cooper,Prithiviraj K. Muthumanickam;Katerina Vrotsou;Aida Nordman;Jimmy Johansson;Matthew Cooper,Linköping University;Linköping University;Linköping University;Linköping University;Linköping University,10.1109/TVCG.2012.276;10.1109/TVCG.2015.2468091;10.1109/TVCG.2016.2598695;10.1109/TVCG.2013.194;10.1109/TVCG.2017.2743939;10.1109/TVCG.2009.117,"Eye-tracking data,areas of interest,clustering,minimum spanning tree,temporal data,spatio-temporal data",0.0,4.0,2.0,49.0,nan,papers/vast-tvcg/87_muthumanickam.pdf
InfoVis,2018,A Heuristic Approach to Value-Driven Evaluation of Visualizations,10.1109/TVCG.2018.2865146,http://dx.doi.org/10.1109/TVCG.2018.2865146,491,500,J,"Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization's value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.",Emily Wall;Meeshu Agnihotri;Laura E. Matzen;Kristin Divis;Michael J. Haass;Alex Endert;John T. Stasko,Emily Wall;Meeshu Agnihotri;Laura Matzen;Kristin Divis;Michael Haass;Alex Endert;John Stasko,"Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Sandia National Laboratories, Albuquerque, NM, USA;Sandia National Laboratories, Albuquerque, NM, USA;Sandia National Laboratories, Albuquerque, NM, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA",10.1109/INFVIS.2001.963289;10.1109/VISUAL.2003.1250401,"Visualization evaluation,heuristics,value of visualization",0.0,2.0,2.0,35.0,nan,papers/infovis-tvcg/491_wall.pdf
InfoVis,2018,Patterns and Pace: Quantifying Diverse Exploration Behavior with Visualizations on the Web,10.1109/TVCG.2018.2865117,http://dx.doi.org/10.1109/TVCG.2018.2865117,501,511,J,"The diverse and vibrant ecosystem of interactive visualizations on the web presents an opportunity for researchers and practitioners to observe and analyze how everyday people interact with data visualizations. However, existing metrics of visualization interaction behavior used in research do not fully reveal the breadth of peoples' open-ended explorations with visualizations. One possible way to address this challenge is to determine high-level goals for visualization interaction metrics, and infer corresponding features from user interaction data that characterize different aspects of peoples' explorations of visualizations. In this paper, we identify needs for visualization behavior measurement, and develop corresponding candidate features that can be inferred from users' interaction data. We then propose metrics that capture novel aspects of peoples' open-ended explorations, including exploration uniqueness and exploration pacing. We evaluate these metrics along with four other metrics recently proposed in visualization literature by applying them to interaction data from prior visualization studies. The results of these evaluations suggest that these new metrics 1) reveal new characteristics of peoples' use of visualizations, 2) can be used to evaluate statistical differences between visualization designs, and 3) are statistically independent of prior metrics used in visualization research. We discuss implications of these results for future studies, including the potential for applying these metrics in visualization interaction analysis, as well as emerging challenges in developing and selecting metrics depicting visualization explorations.",Mi Feng;Evan M. Peck;Lane Harrison,Mi Feng;Evan Peck;Lane Harrison,Worcester Polytechnic Institute;Bucknell University;Worcester Polytechnic Institute,10.1109/TVCG.2011.229;10.1109/TVCG.2015.2467871;10.1109/TVCG.2015.2467201;10.1109/TVCG.2014.2346575;10.1109/VAST.2007.4389009;10.1109/TVCG.2016.2599058;10.1109/TVCG.2015.2467613;10.1109/TVCG.2008.137;10.1109/VAST.2007.4389008;10.1109/TVCG.2014.2346452;10.1109/TVCG.2016.2598797;10.1109/TVCG.2013.200;10.1109/TVCG.2016.2598466;10.1109/VAST.2017.8585669;10.1109/TVCG.2017.2745958;10.1109/TVCG.2007.70515,"Interaction,Visualization,Quantitative Evaluation",0.0,0.0,2.0,46.0,nan,papers/infovis-tvcg/501_feng.pdf
InfoVis,2018,IDMVis: Temporal Event Sequence Visualization for Type 1 Diabetes Treatment Decision Support,10.1109/TVCG.2018.2865076,http://dx.doi.org/10.1109/TVCG.2018.2865076,512,522,J,"Type 1 diabetes is a chronic, incurable autoimmune disease affecting millions of Americans in which the body stops producing insulin and blood glucose levels rise. The goal of intensive diabetes management is to lower average blood glucose through frequent adjustments to insulin protocol, diet, and behavior. Manual logs and medical device data are collected by patients, but these multiple sources are presented in disparate visualization designs to the clinician-making temporal inference difficult. We conducted a design study over 18 months with clinicians performing intensive diabetes management. We present a data abstraction and novel hierarchical task abstraction for this domain. We also contribute IDMVis: a visualization tool for temporal event sequences with multidimensional, interrelated data. IDMVis includes a novel technique for folding and aligning records by dual sentinel events and scaling the intermediate timeline. We validate our design decisions based on our domain abstractions, best practices, and through a qualitative evaluation with six clinicians. The results of this study indicate that IDMVis accurately reflects the workflow of clinicians. Using IDMVis, clinicians are able to identify issues of data quality such as missing or conflicting data, reconstruct patient records when data is missing, differentiate between days with different patterns, and promote educational interventions after identifying discrepancies.",Yixuan Zhang;Kartik Chanana;Cody Dunne,Yixuan Zhang;Kartik Chanana;Cody Dunne,Northeastern University;Northeastern University;Northeastern University,10.1109/INFVIS.2005.1532136;10.1109/TVCG.2013.124;10.1109/TVCG.2017.2744319;10.1109/TVCG.2009.111;10.1109/TVCG.2012.213;10.1109/VISUAL.1992.235203,"Design study,task analysis,event sequence visualization,time series data,qualitative evaluation,health applications",0.0,6.0,6.0,60.0,nan,papers/infovis-tvcg/512_zhang.pdf
SciVis,2018,Interactive Visualization of RNA and DNA Structures,10.1109/TVCG.2018.2864507,http://dx.doi.org/10.1109/TVCG.2018.2864507,967,976,J,"The analysis and visualization of nucleic acids (RNA and DNA) is playing an increasingly important role due to their fundamental importance for all forms of life and the growing number of known 3D structures of such molecules. The great complexity of these structures, in particular, those of RNA, demands interactive visualization to get deeper insights into the relationship between the 2D secondary structure motifs and their 3D tertiary structures. Over the last decades, a lot of research in molecular visualization has focused on the visual exploration of protein structures while nucleic acids have only been marginally addressed. In contrast to proteins, which are composed of amino acids, the ingredients of nucleic acids are nucleotides. They form structuring patterns that differ from those of proteins and, hence, also require different visualization and exploration techniques. In order to support interactive exploration of nucleic acids, the computation of secondary structure motifs as well as their visualization in 2D and 3D must be fast. Therefore, in this paper, we focus on the performance of both the computation and visualization of nucleic acid structure. We present a ray casting-based visualization of RNA and DNA secondary and tertiary structures, which enables for the first time real-time visualization of even large molecular dynamics trajectories. Furthermore, we provide a detailed description of all important aspects to visualize nucleic acid secondary and tertiary structures. With this, we close an important gap in molecular visualization.",Norbert Lindow;Daniel Baum;Morgan Leborgne;Hans-Christian Hege,Norbert Lindow;Daniel Baum;Morgan Leborgne;Hans-Christian Hege,"Zuse Institute, Berlin;Zuse Institute, Berlin;Zuse Institute, Berlin;Zuse Institute, Berlin",nan,"Ribonucleic acids,DNA,RNA,secondary & tertiary structures,interactive rendering,ray casting,brushing & linking",0.0,5.0,1.0,48.0,nan,papers/scivis-tvcg/967_lindow.pdf
SciVis,2018,Labels on Levels: Labeling of Multi-Scale Multi-Instance and Crowded 3D Biological Environments,10.1109/TVCG.2018.2864491,http://dx.doi.org/10.1109/TVCG.2018.2864491,977,986,J,"Labeling is intrinsically important for exploring and understanding complex environments and models in a variety of domains. We present a method for interactive labeling of crowded 3D scenes containing very many instances of objects spanning multiple scales in size. In contrast to previous labeling methods, we target cases where many instances of dozens of types are present and where the hierarchical structure of the objects in the scene presents an opportunity to choose the most suitable level for each placed label. Our solution builds on and goes beyond labeling techniques in medical 3D visualization, cartography, and biological illustrations from books and prints. In contrast to these techniques, the main characteristics of our new technique are: 1) a novel way of labeling objects as part of a bigger structure when appropriate, 2) visual clutter reduction by labeling only representative instances for each type of an object, and a strategy of selecting those. The appropriate level of label is chosen by analyzing the scene's depth buffer and the scene objects' hierarchy tree. We address the topic of communicating the parent-children relationship between labels by employing visual hierarchy concepts adapted from graphic design. Selecting representative instances considers several criteria tailored to the character of the data and is combined with a greedy optimization approach. We demonstrate the usage of our method with models from mesoscale biology where these two characteristics-multi-scale and multi-instance-are abundant, along with the fact that these scenes are extraordinarily dense.",David Kouril;Ladislav Cmolík;Barbora Kozlíková;Hsiang-Yun Wu;Graham Johnson;David S. Goodsell;Arthur J. Olson;M. Eduard Gröller;Ivan Viola,David Kouřil;Ladislav Čmolík;Barbora Kozlíková;Hslanc-Yun Wu;Graham Johnson;David S. Goodsell;Arthur Olson;M. Eduard Gröller;Ivan Viola,"TU Wien;Faculty of Electrical Engineering, Czech Technical University, Prague;Masaryk University;TU Wien;Allen Institute for Cell Science;The Scripps Research Institute;The Scripps Research Institute;TU Wien;TU Wien",10.1109/TVCG.2006.136;10.1109/TVCG.2008.168;10.1109/TVCG.2017.2744518,"labeling,multi-scale data,multi-instance data",nan,5.0,0.0,54.0,HM,papers/scivis-tvcg/977_kouril.pdf
SciVis,2018,Visualization of Large Molecular Trajectories,10.1109/TVCG.2018.2864851,http://dx.doi.org/10.1109/TVCG.2018.2864851,987,996,J,"The analysis of protein-ligand interactions is a time-intensive task. Researchers have to analyze multiple physico-chemical properties of the protein at once and combine them to derive conclusions about the protein-ligand interplay. Typically, several charts are inspected, and 3D animations can be played side-by-side to obtain a deeper understanding of the data. With the advances in simulation techniques, larger and larger datasets are available, with up to hundreds of thousands of steps. Unfortunately, such large trajectories are very difficult to investigate with traditional approaches. Therefore, the need for special tools that facilitate inspection of these large trajectories becomes substantial. In this paper, we present a novel system for visual exploration of very large trajectories in an interactive and user-friendly way. Several visualization motifs are automatically derived from the data to give the user the information about interactions between protein and ligand. Our system offers specialized widgets to ease and accelerate data inspection and navigation to interesting parts of the simulation. The system is suitable also for simulations where multiple ligands are involved. We have tested the usefulness of our tool on a set of datasets obtained from protein engineers, and we describe the expert feedback.",David Duran;Pedro Hermosilla;Timo Ropinski;Barbora Kozlíková;Alvar Vinacua;Pere-Pau Vázquez,David Duran;Pedro Hermosilla;Timo Ropinski;Barbora Kozlíková;Álvar Vinacua;Pere-Pau Vázquez,"ViRVIG Group, UPC, Barcelona;Visual Computing GroupU. Ulm.;Visual Computing GroupU. Ulm.;Masaryk University;ViRVIG Group, UPC, Barcelona;ViRVIG Group, UPC, Barcelona",10.1109/TVCG.2015.2467434;10.1109/VISUAL.2005.1532792;10.1109/TVCG.2016.2598825;10.1109/TVCG.2016.2598797;10.1109/TVCG.2014.2346574;10.1109/TVCG.2012.225,"Molecular visualization,simulation inspection,long trajectories",0.0,4.0,1.0,43.0,nan,papers/scivis-tvcg/987_duranrosich.pdf
VAST,2018,Visual Analysis of the Temporal Evolution of Ensemble Forecast Sensitivities,10.1109/TVCG.2018.2864901,http://dx.doi.org/10.1109/TVCG.2018.2864901,98,108,J,"Ensemble sensitivity analysis (ESA) has been established in the atmospheric sciences as a correlation-based approach to determine the sensitivity of a scalar forecast quantity computed by a numerical weather prediction model to changes in another model variable at a different model state. Its applications include determining the origin of forecast errors and placing targeted observations to improve future forecasts. We - a team of visualization scientists and meteorologists - present a visual analysis framework to improve upon current practice of ESA. We support the user in selecting regions to compute a meaningful target forecast quantity by embedding correlation-based grid-point clustering to obtain statistically coherent regions. The evolution of sensitivity features computed via ESA are then traced through time, by integrating a quantitative measure of feature matching into optical-flow-based feature assignment, and displayed by means of a swipe-path showing the geo-spatial evolution of the sensitivities. Visualization of the internal correlation structure of computed features guides the user towards those features robustly predicting a certain weather event. We demonstrate the use of our method by application to real-world 2D and 3D cases that occurred during the 2016 NAWDEX field campaign, showing the interactive generation of hypothesis chains to explore how atmospheric processes sensitive to each other are interrelated.",Alexander Kumpf;Marc Rautenhaus;Michael Riemer;Rüdiger Westermann,Alexander Kumpf;Marc Rautenhaus;Michael Riemer;Rüdiger Westermann,"Computer Graphics & Visualization Group, Technische Universitiit München, Garching, Germany;Computer Graphics & Visualization Group, Technische Universitiit München, Garching, Germany;Universität Hamburg, Regional Computing Center, Hamburg, Germany;Computer Graphics & Visualization Group, Technische Universitiit München, Garching, Germany",10.1109/TVCG.2013.131;10.1109/VISUAL.2004.46;10.1109/TVCG.2017.2743989;10.1109/TVCG.2017.2745178;10.1109/TVCG.2006.165,"Correlation,clustering,tracking,ensemble visualization",0.0,4.0,0.0,47.0,nan,papers/vast-tvcg/98_kumpf.pdf
VAST,2018,EnsembleLens: Ensemble-based Visual Exploration of Anomaly Detection Algorithms with Multidimensional Data,10.1109/TVCG.2018.2864825,http://dx.doi.org/10.1109/TVCG.2018.2864825,109,119,J,"The results of anomaly detection are sensitive to the choice of detection algorithms as they are specialized for different properties of data, especially for multidimensional data. Thus, it is vital to select the algorithm appropriately. To systematically select the algorithms, ensemble analysis techniques have been developed to support the assembly and comparison of heterogeneous algorithms. However, challenges remain due to the absence of the ground truth, interpretation, or evaluation of these anomaly detectors. In this paper, we present a visual analytics system named EnsembleLens that evaluates anomaly detection algorithms based on the ensemble analysis process. The system visualizes the ensemble processes and results by a set of novel visual designs and multiple coordinated contextual views to meet the requirements of correlation analysis, assessment and reasoning of anomaly detection algorithms. We also introduce an interactive analysis workflow that dynamically produces contextualized and interpretable data summaries that allow further refinements of exploration results based on user feedback. We demonstrate the effectiveness of EnsembleLens through a quantitative evaluation, three case studies with real-world data and interviews with two domain experts.",Ke Xu;Meng Xia;Xing Mu;Yun Wang 0012;Nan Cao,Ke Xu;Meng Xia;Xing Mu;Yun Wang;Nan Cao,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;iDVxLabTongji University,10.1109/SciVis.2015.7429487;10.1109/TVCG.2017.2744419;10.1109/TVCG.2014.2346448;10.1109/TVCG.2015.2468093;10.1109/VISUAL.1990.146402;10.1109/TVCG.2017.2745178;10.1109/TVCG.2010.181;10.1109/TVCG.2016.2598830;10.1109/TVCG.2014.2346922,"Algorithm Evaluation,Ensemble Analysis,Anomaly Detection,Visual Analysis,Multidimensional Data",nan,3.0,0.0,80.0,nan,papers/vast-tvcg/109_xu.pdf
VAST,2018,KnowledgePearls: Provenance-Based Visualization Retrieval,10.1109/TVCG.2018.2865024,http://dx.doi.org/10.1109/TVCG.2018.2865024,120,130,J,"Storing analytical provenance generates a knowledge base with a large potential for recalling previous results and guiding users in future analyses. However, without extensive manual creation of meta information and annotations by the users, search and retrieval of analysis states can become tedious. We present KnowledgePearls, a solution for efficient retrieval of analysis states that are structured as provenance graphs containing automatically recorded user interactions and visualizations. As a core component, we describe a visual interface for querying and exploring analysis states based on their similarity to a partial definition of a requested analysis state. Depending on the use case, this definition may be provided explicitly by the user by formulating a search query or inferred from given reference states. We explain our approach using the example of efficient retrieval of demographic analyses by Hans Rosling and discuss our implementation for a fast look-up of previous states. Our approach is independent of the underlying visualization framework. We discuss the applicability for visualizations which are based on the declarative grammar Vega and we use a Vega-based implementation of Gapminder as guiding example. We additionally present a biomedical case study to illustrate how KnowledgePearls facilitates the exploration process by recalling states from earlier analyses.",Holger Stitz;Samuel Gratzl;Harald Piringer;Thomas Zichner;Marc Streit,Holger Stitz;Samuel Gratzl;Harald Piringer;Thomas Zichner;Marc Streit,"Johannes Kepler University, Linz, Austria;Johannes Kepler University, Linz, Austria;VRVis Research Center, Austria;Boehringer Ingelheim RCV GmbH & Co KG, Austria;Johannes Kepler University, Linz, Austria",10.1109/VISUAL.2005.1532788;10.1109/TVCG.2011.229;10.1109/TVCG.2013.155;10.1109/TVCG.2009.176;10.1109/VAST.2008.4677365;10.1109/INFVIS.2004.2;10.1109/TVCG.2012.271;10.1109/TVCG.2016.2598589;10.1109/TVCG.2017.2744320;10.1109/TVCG.2015.2467551;10.1109/TVCG.2016.2599030;10.1109/TVCG.2015.2467091;10.1109/TVCG.2006.142;10.1109/TVCG.2017.2745219;10.1109/INFVIS.2000.885086;10.1109/INFVIS.2005.1532142,"Visualization provenance,interaction provenance,retrieval",0.0,2.0,2.0,47.0,nan,papers/vast-tvcg/120_stitz.pdf
VAST,2018,Enhancing Web-based Analytics Applications through Provenance,10.1109/TVCG.2018.2865039,http://dx.doi.org/10.1109/TVCG.2018.2865039,131,141,J,"Visual analytics systems continue to integrate new technologies and leverage modern environments for exploration and collaboration, making tools and techniques available to a wide audience through web browsers. Many of these systems have been developed with rich interactions, offering users the opportunity to examine details and explore hypotheses that have not been directly encoded by a designer. Understanding is enhanced when users can replay and revisit the steps in the sensemaking process, and in collaborative settings, it is especially important to be able to review not only the current state but also what decisions were made along the way. Unfortunately, many web-based systems lack the ability to capture such reasoning, and the path to a result is transient, forgotten when a user moves to a new view. This paper explores the requirements to augment existing client-side web applications with support for capturing, reviewing, sharing, and reusing steps in the reasoning process. Furthermore, it considers situations where decisions are made with streaming data, and the insights gained from revisiting those choices when more data is available. It presents a proof of concept, the Shareable Interactive Manipulation Provenance framework (SIMProv.js), that addresses these requirements in a modern, client-side JavaScript library, and describes how it can be integrated with existing frameworks.",Akhilesh Camisetty;Chaitanya Chandurkar;Maoyuan Sun;David Koop,Akhilesh Camisetty;Chaitanya Chandurkar;Maoyuan Sun;David Koop,UMass Dartmouth;UMass Dartmouth;UMass Dartmouth;UMass Dartmouth,10.1109/VISUAL.1993.398857;10.1109/VAST.2011.6102447;10.1109/VAST.2010.5652932;10.1109/TVCG.2016.2598471;10.1109/TVCG.2016.2599058;10.1109/VAST.2008.4677365;10.1109/TVCG.2013.197;10.1109/VAST.2007.4389011;10.1109/VISUAL.1999.809871;10.1109/TVCG.2014.2346573;10.1109/TVCG.2015.2467551;10.1109/TVCG.2015.2467191;10.1109/TVCG.2017.2745279,"Collaboration,provenance,streaming data,history,web",0.0,1.0,0.0,60.0,nan,papers/vast-tvcg/131_camisetty.pdf
InfoVis,2018,Comparing Similarity Perception in Time Series Visualizations,10.1109/TVCG.2018.2865077,http://dx.doi.org/10.1109/TVCG.2018.2865077,523,533,J,"A common challenge faced by many domain experts working with time series data is how to identify and compare similar patterns. This operation is fundamental in high-level tasks, such as detecting recurring phenomena or creating clusters of similar temporal sequences. While automatic measures exist to compute time series similarity, human intervention is often required to visually inspect these automatically generated results. The visualization literature has examined similarity perception and its relation to automatic similarity measures for line charts, but has not yet considered if alternative visual representations, such as horizon graphs and colorfields, alter this perception. Motivated by how neuroscientists evaluate epileptiform patterns, we conducted two experiments that study how these three visualization techniques affect similarity perception in EEG signals. We seek to understand if the time series results returned from automatic similarity measures are perceived in a similar manner, irrespective of the visualization technique; and if what people perceive as similar with each visualization aligns with different automatic measures and their similarity constraints. Our findings indicate that horizon graphs align with similarity measures that allow local variations in temporal position or speed (i.e., dynamic time warping) more than the two other techniques. On the other hand, horizon graphs do not align with measures that are insensitive to amplitude and y-offset scaling (i.e., measures based on z-normalization), but the inverse seems to be the case for line charts and colorfields. Overall, our work indicates that the choice of visualization affects what temporal patterns we consider as similar, i.e., the notion of similarity in time series is not visualization independent.",Anna Gogolou;Theophanis Tsandilas;Themis Palpanas;Anastasia Bezerianos,Anna Gogolou;Theophanis Tsandilas;Themis Palpanas;Anastasia Bezerianos,"Inria, Univ. Paris-Sud, Univ. Paris-Saclay, France;Inria, Univ. Paris-Sud, CNRS, Univ. Paris-Saclay, France;Univ. Paris-Descartes, France;Inria, Univ. Paris-Sud, CNRS, Univ. Paris-Saclay, France",10.1109/TVCG.2011.232;10.1109/VAST.2007.4389007;10.1109/TVCG.2008.166;10.1109/VAST.2016.7883519;10.1109/TVCG.2010.162;10.1109/VAST.2016.7883518;10.1109/INFVIS.2005.1532144;10.1109/TVCG.2012.196;10.1109/INFVIS.1999.801851;10.1109/INFVIS.2001.963273;10.1109/TVCG.2011.195,"Time series,similarity perception,automatic similarity search,line charts,horizon graphs,colorfields,evaluation",0.0,6.0,7.0,70.0,nan,papers/infovis-tvcg/523_gogolou.pdf
InfoVis,2018,Temporal Treemaps: Static Visualization of Evolving Trees,10.1109/TVCG.2018.2865265,http://dx.doi.org/10.1109/TVCG.2018.2865265,534,543,J,"We consider temporally evolving trees with changing topology and data: tree nodes may persist for a time range, merge or split, and the associated data may change. Essentially, one can think of this as a time series of trees with a node correspondence per hierarchy level between consecutive time steps. Existing visualization approaches for such data include animated 2D treemaps, where the dynamically changing layout makes it difficult to observe the data in its entirety. We present a method to visualize this dynamic data in a static, nested, and space-filling visualization. This is based on two major contributions: First, the layout constitutes a graph drawing problem. We approach it for the entire time span at once using a combination of a heuristic and simulated annealing. Second, we propose a rendering that emphasizes the hierarchy through an adaption of the classic cushion treemaps. We showcase the wide range of applicability using data from feature tracking in time-dependent scalar fields, evolution of file system hierarchies, and world population.",Wiebke Köpp;Tino Weinkauf,Wiebke Köpp;Tino Weinkauf,"KTH Royal Institute of Technology, Stockholm, Sweden;KTH Royal Institute of Technology, Stockholm, Sweden",10.1109/INFVIS.2005.1532128;10.1109/TVCG.2011.226;10.1109/TVCG.2008.166;10.1109/TVCG.2014.2346433;10.1109/TVCG.2017.2743959;10.1109/VISUAL.1991.175815;10.1109/TVCG.2013.196;10.1109/INFVIS.2001.963283;10.1109/TVCG.2017.2745140;10.1109/TVCG.2007.70529;10.1109/INFVIS.1999.801860;10.1109/TVCG.2008.163,"Treemaps,Temporal trees",nan,4.0,3.0,40.0,nan,papers/infovis-tvcg/534_kopp.pdf
SciVis,2018,Visual Analysis of Aneurysm Data using Statistical Graphics,10.1109/TVCG.2018.2864509,http://dx.doi.org/10.1109/TVCG.2018.2864509,997,1007,J,"This paper presents a framework to explore multi-field data of aneurysms occurring at intracranial and cardiac arteries by using statistical graphics. The rupture of an aneurysm is often a fatal scenario, whereas during treatment serious complications for the patient can occur. Whether an aneurysm ruptures or whether a treatment is successful depends on the interaction of different morphological such as wall deformation and thickness, and hemodynamic attributes like wall shear stress and pressure. Therefore, medical researchers are very interested in better understanding these relationships. However, the required analysis is a time-consuming process, where suspicious wall regions are difficult to detect due to the time-dependent behavior of the data. Our proposed visualization framework enables medical researchers to efficiently assess aneurysm risk and treatment options. This comprises a powerful set of views including 2D and 3D depictions of the aneurysm morphology as well as statistical plots of different scalar fields. Brushing and linking aids the user to identify interesting wall regions and to understand the influence of different attributes on the aneurysm's state. Moreover, a visual comparison of pre- and post-treatment as well as different treatment options is provided. Our analysis techniques are designed in collaboration with domain experts, e.g., physicians, and we provide details about the evaluation.",Monique Meuschke;Tobias Günther;Philipp Berg;Ralph Wickenhöfer;Bernhard Preim;Kai Lawonn,Monique Meuschke;Tobias Günther;Philipp Berg;Ralph Wickenhöfer;Bernhard Preim;Kai Lawonn,"University of Magdeburg, Germany;ETH Zürich, Switzerland;University of Magdeburg, Germany;Heart of Jesus Hospital, Germany;University of Magdeburg, Germany;University of Koblenz-Landau, Germany",10.1109/TVCG.2011.235;10.1109/TVCG.2014.2346406;10.1109/VISUAL.2000.885739;10.1109/TVCG.2013.139;10.1109/TVCG.2016.2598795;10.1109/TVCG.2016.2598866;10.1109/TVCG.2010.153,"Medical visualizations,aneurysms,blood flow,parametrization",0.0,0.0,1.0,53.0,nan,papers/scivis-tvcg/997_meuschke.pdf
SciVis,2018,Interactive Visualization of 3D Histopathology in Native Resolution,10.1109/TVCG.2018.2864816,http://dx.doi.org/10.1109/TVCG.2018.2864816,1008,1017,J,"We present a visualization application that enables effective interactive visual analysis of large-scale 3D histopathology, that is, high-resolution 3D microscopy data of human tissue. Clinical work flows and research based on pathology have, until now, largely been dominated by 2D imaging. As we will show in the paper, studying volumetric histology data will open up novel and useful opportunities for both research and clinical practice. Our starting point is the current lack of appropriate visualization tools in histopathology, which has been a limiting factor in the uptake of digital pathology. Visualization of 3D histology data does pose difficult challenges in several aspects. The full-color datasets are dense and large in scale, on the order of 100,000 × 100,000 × 100 voxels. This entails serious demands on both rendering performance and user experience design. Despite this, our developed application supports interactive study of 3D histology datasets at native resolution. Our application is based on tailoring and tuning of existing methods, system integration work, as well as a careful study of domain specific demands emanating from a close participatory design process with domain experts as team members. Results from a user evaluation employing the tool demonstrate a strong agreement among the 14 participating pathologists that 3D histopathology will be a valuable and enabling tool for their work.",Martin Falk;Anders Ynnerman;Darren Treanor;Claes Lundström,Martin Falk;Anders Ynnerman;Darren Treanor;Claes Lundström,"Department of Science and Technology, Linköping University, Sweden;Department of Science and Technology, Linköping University, Sweden;Leeds Teaching Hospitals NHS Trust, United Kingdom;The Center for Medical Image Science and Visualization (CMIV), Linköping University, Sweden",10.1109/TVCG.2009.150;10.1109/VISUAL.2002.1183757;10.1109/TVCG.2012.240,"Histology,Pathology,Volume Rendering,Expert Evaluation",nan,1.0,0.0,35.0,nan,papers/scivis-tvcg/1008_falk.pdf
SciVis,2018,Visualization of Neuronal Structures in Wide-Field Microscopy Brain Images,10.1109/TVCG.2018.2864852,http://dx.doi.org/10.1109/TVCG.2018.2864852,1018,1028,J,"Wide-field microscopes are commonly used in neurobiology for experimental studies of brain samples. Available visualization tools are limited to electron, two-photon, and confocal microscopy datasets, and current volume rendering techniques do not yield effective results when used with wide-field data. We present a workflow for the visualization of neuronal structures in wide-field microscopy images of brain samples. We introduce a novel gradient-based distance transform that overcomes the out-of-focus blur caused by the inherent design of wide-field microscopes. This is followed by the extraction of the 3D structure of neurites using a multi-scale curvilinear filter and cell-bodies using a Hessian-based enhancement filter. The response from these filters is then applied as an opacity map to the raw data. Based on the visualization challenges faced by domain experts, our workflow provides multiple rendering modes to enable qualitative analysis of neuronal structures, which includes separation of cell-bodies from neurites and an intensity-based classification of the structures. Additionally, we evaluate our visualization results against both a standard image processing deconvolution technique and a confocal microscopy image of the same specimen. We show that our method is significantly faster and requires less computational resources, while producing high quality visualizations. We deploy our workflow in an immersive gigapixel facility as a paradigm for the processing and visualization of large, high-resolution, wide-field microscopy brain datasets.",Saeed Boorboor;Shreeraj Jadhav;Mala Ananth;David Talmage;Lorna Role;Arie E. Kaufman,Saeed Boorboor;Shreeraj Jadhav;Mala Ananth;David Talmage;Lorna Role;Arie Kaufman,Department of Computer ScienceStony Brook University;Department of Computer ScienceStony Brook University;Department of Neurobiology & behaviorStony Brook University;Department of Neurobiology & behaviorStony Brook University;Department of Neurobiology & behaviorStony Brook University;Department of Computer ScienceStony Brook University,10.1109/TVCG.2014.2346312;10.1109/TVCG.2013.142;10.1109/TVCG.2012.203;10.1109/TVCG.2017.2744079;10.1109/TVCG.2009.118;10.1109/TVCG.2016.2598472,"Wide-field microscopy,volume visualization,neuron visualization,neuroscience",nan,2.0,0.0,58.0,nan,papers/scivis-tvcg/1018_boorboor.pdf
VAST,2018,Doccurate: A Curation-Based Approach for Clinical Text Visualization,10.1109/TVCG.2018.2864905,http://dx.doi.org/10.1109/TVCG.2018.2864905,142,151,J,"Before seeing a patient, physicians seek to obtain an overview of the patient's medical history. Text plays a major role in this activity since it represents the bulk of the clinical documentation, but reviewing it quickly becomes onerous when patient charts grow too large. Text visualization methods have been widely explored to manage this large scale through visual summaries that rely on information retrieval algorithms to structure text and make it amenable to visualization. However, the integration with such automated approaches comes with a number of limitations, including significant error rates and the need for healthcare providers to fine-tune algorithms without expert knowledge of their inner mechanics. In addition, several of these approaches obscure or substitute the original clinical text and therefore fail to leverage qualitative and rhetorical flavours of the clinical notes. These drawbacks have limited the adoption of text visualization and other summarization technologies in clinical practice. In this work we present Doccurate, a novel system embodying a curation-based approach for the visualization of large clinical text datasets. Our approach offers automation auditing and customizability to physicians while also preserving and extensively linking to the original text. We discuss findings of a formal qualitative evaluation conducted with 6 domain experts, shedding light onto physicians' information needs, perceived strengths and limitations of automated tools, and the importance of customization while balancing efficiency. We also present use case scenarios to showcase Doccurate's envisioned usage in practice.",Nicole Sultanum;Devin Singh;Michael Brudno;Fanny Chevalier,Nicole Sultanum;Devin Singh;Michael Brudno;Fanny Chevalier,Hospital for Sick Children;Hospital for Sick Children;University of Toronto;University of Toronto,10.1109/VAST.2012.6400485;10.1109/TVCG.2015.2467531;10.1109/TVCG.2017.2745118;10.1109/INFVIS.2000.885098;10.1109/TVCG.2017.2744478;10.1109/TVCG.2015.2467591,"Visual Curation,Clinical Text,Text Visualization,Medical Narrative",nan,3.0,2.0,40.0,nan,papers/vast-tvcg/142_sultanum.pdf
VAST,2018,VIS Author Profiles: Interactive Descriptions of Publication Records Combining Text and Visualization,10.1109/TVCG.2018.2865022,http://dx.doi.org/10.1109/TVCG.2018.2865022,152,161,J,"Publication records and collaboration networks are important for assessing the expertise and experience of researchers. Existing digital libraries show the raw publication lists in author profiles, whereas visualization techniques focus on specific subproblems. Instead, we look at publication records from various perspectives mixing low-level publication data with high-level abstractions and background information. This work presents VIS Author Profiles, a novel approach to generate integrated textual and visual descriptions to highlight patterns in publication records. We leverage template-based natural language generation to summarize notable publication statistics, evolution of research topics, and collaboration relationships. Seamlessly integrated visualizations augment the textual description and are interactively connected with each other and the text. The underlying publication data and detailed explanations of the analysis are available on demand. We compare our approach to existing systems by taking into account information needs of users and demonstrate its usefulness in two realistic application examples.",Shahid Latif;Fabian Beck 0001,Shahid Latif;Fabian Beck,"Paluno, University of Duisburg, Essen, Germany;Paluno, University of Duisburg, Essen, Germany",10.1109/TVCG.2015.2467757;10.1109/TVCG.2012.252;10.1109/TVCG.2014.2346435;10.1109/TVCG.2007.70582;10.1109/VAST.2015.7347632;10.1109/TVCG.2015.2468151;10.1109/TVCG.2013.167,"Natural language generation,document visualization,interactive documents,sparklines,digital libraries",nan,4.0,2.0,36.0,nan,papers/vast-tvcg/152_latif.pdf
VAST,2018,BitExTract: Interactive Visualization for Extracting Bitcoin Exchange Intelligence,10.1109/TVCG.2018.2864814,http://dx.doi.org/10.1109/TVCG.2018.2864814,162,171,J,"The emerging prosperity of cryptocurrencies, such as Bitcoin, has come into the spotlight during the past few years. Cryptocurrency exchanges, which act as the gateway to this world, now play a dominant role in the circulation of Bitcoin. Thus, delving into the analysis of the transaction patterns of exchanges can shed light on the evolution and trends in the Bitcoin market, and participants can gain hints for identifying credible exchanges as well. Not only Bitcoin practitioners but also researchers in the financial domains are interested in the business intelligence behind the curtain. However, the task of multiple exchanges exploration and comparisons has been limited owing to the lack of efficient tools. Previous methods of visualizing Bitcoin data have mainly concentrated on tracking suspicious transaction logs, but it is cumbersome to analyze exchanges and their relationships with existing tools and methods. In this paper, we present BitExTract, an interactive visual analytics system, which, to the best of our knowledge, is the first attempt to explore the evolutionary transaction patterns of Bitcoin exchanges from two perspectives, namely, exchange versus exchange and exchange versus client. In particular, BitExTract summarizes the evolution of the Bitcoin market by observing the transactions between exchanges over time via a massive sequence view. A node-link diagram with ego-centered views depicts the trading network of exchanges and their temporal transaction distribution. Moreover, BitExTract embeds multiple parallel bars on a timeline to examine and compare the evolution patterns of transactions between different exchanges. Three case studies with novel insights demonstrate the effectiveness and usability of our system.",Xuanwu Yue;Xinhuan Shu;Xinyu Zhu;Xinnan Du;Zheqing Yu;Dimitrios Papadopoulos;Siyuan Liu,Xuanwu Yue;Xinhuan Shu;Xinyu Zhu;Xinnan Du;Zheqing Yu;Dimitrios Papadopoulos;Siyuan Liu,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Penn State University,10.1109/TVCG.2008.117;10.1109/TVCG.2011.169;10.1109/VAST.2007.4389009;10.1109/TVCG.2009.180;10.1109/TVCG.2014.2346913,"Bitcoin exchange,transaction data,comparative analysis,visual analytics,FinTech",nan,3.0,2.0,50.0,nan,papers/vast-tvcg/162_yue.pdf
VAST,2018,VUSphere: Visual Analysis of Video Utilization in Online Distance Education,10.1109/VAST.2018.8802383,http://dx.doi.org/10.1109/VAST.2018.8802383,25,35,C,"Online Distance Education (ODE) provides massive course videos of various specialties for students across the country to learn professional knowledge anytime and anywhere. Analyzing the utilization of these videos from user log data can help academics better understand the learning process of students, evaluate the quality of service provided by regional learning centers, and improve the quality of program curriculum in the future. However, due to the lack of comparable indicators, it is a great challenge to discover the utilization patterns of massive videos and analyze the learning process of large-scale student population from learning log data. In this paper, we introduce a visual analytics system, called VUSphere, to explore the video utilization from multiple perspectives with two proposed indicators. This system offers three coordinated views: a spherical layout overview to depict the overall utilization distribution of videos, courses, and students; a detailed statistics view with four panels to present video utilization statistics of each element from multiple perspectives; and a comparison view to examine the differences in individual elements. Based on the real dataset from our ODE school, several patterns related to video utilization and enrollment are found in the case study with our domain experts.",Huan He;Qinghua Zheng;Bo Dong,Huan He;Oinghua Zheng;Bo Dong,"Key Laboratory of Intelligent Networks and Network Security, Ministry of Education, Xi’an Jiaotong University;Key Laboratory of Intelligent Networks and Network Security, Ministry of Education, Xi’an Jiaotong University;School of Continuing Education, Xi’an Jiaotong University",10.1109/TVCG.2016.2598444;10.1109/VAST.2016.7883517;10.1109/TVCG.2015.2467322,"Video utilization pattern,online distance education,visual analytics",nan,0.0,nan,0.0,nan,papers/vast-conference/he.pdf
InfoVis,2018,Juniper: A Tree+Table Approach to Multivariate Graph Visualization,10.1109/TVCG.2018.2865149,http://dx.doi.org/10.1109/TVCG.2018.2865149,544,554,J,"Analyzing large, multivariate graphs is an important problem in many domains, yet such graphs are challenging to visualize. In this paper, we introduce a novel, scalable, tree-table multivariate graph visualization technique, which makes many tasks related to multivariate graph analysis easier to achieve. The core principle we follow is to selectively query for nodes or subgraphs of interest and visualize these subgraphs as a spanning tree of the graph. The tree is laid out linearly, which enables us to juxtapose the nodes with a table visualization where diverse attributes can be shown. We also use this table as an adjacency matrix, so that the resulting technique is a hybrid node-link/adjacency matrix technique. We implement this concept in Juniper and complement it with a set of interaction techniques that enable analysts to dynamically grow, restructure, and aggregate the tree, as well as change the layout or show paths between nodes. We demonstrate the utility of our tool in usage scenarios for different multivariate networks: a bipartite network of scholars, papers, and citation metrics and a multitype network of story characters, places, books, etc.",Carolina Nobre;Marc Streit;Alexander Lex,Carolina Nobre;Marc Streit;Alexander Lex,"University of Utah;Johannes Kepler University, Linz;University of Utah",10.1109/TVCG.2011.247;10.1109/INFVIS.2005.1532126;10.1109/TVCG.2007.70582;10.1109/TVCG.2006.147;10.1109/INFVIS.2003.1249009;10.1109/VISUAL.1991.175815;10.1109/TVCG.2014.2346248;10.1109/TVCG.2017.2744898;10.1109/INFVIS.2000.885091;10.1109/TVCG.2015.2468078;10.1109/TVCG.2014.2346441;10.1109/TVCG.2009.108;10.1109/TVCG.2016.2598885;10.1109/INFVIS.2001.963279,"Multivariate graphs,networks,tree-based graph visualization,adjacency matrix,spanning trees,visualization",0.0,4.0,4.0,52.0,nan,papers/infovis-tvcg/544_nobre.pdf
InfoVis,2018,Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks,10.1109/TVCG.2018.2865139,http://dx.doi.org/10.1109/TVCG.2018.2865139,555,565,J,"When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.",Wei Chen 0001;Fangzhou Guo;Dongming Han;Jacheng Pan;Xiaotao Nie;Jiazhi Xia;Xiaolong Zhang,Wei Chen;Fangzhou Guo;Dongming Han;Jacheng Pan;Xiaotao Nie;Jiazhi Xia;Xiaolong Zhang,State Key Lab of CAD and CGZhejiang University;State Key Lab of CAD and CGZhejiang University;State Key Lab of CAD and CGZhejiang University;State Key Lab of CAD and CGZhejiang University;State Key Lab of CAD and CGZhejiang University;Central South University;Pennsylvania State University,10.1109/TVCG.2006.120;10.1109/TVCG.2016.2598958;10.1109/INFVIS.2004.1;10.1109/INFVIS.2005.1532126;10.1109/TVCG.2007.70582;10.1109/TVCG.2006.147;10.1109/TVCG.2008.151;10.1109/TVCG.2017.2743858;10.1109/VAST.2014.7042485;10.1109/TVCG.2017.2744938;10.1109/TVCG.2016.2598831;10.1109/TVCG.2017.2744898;10.1109/TVCG.2017.2745219;10.1109/TVCG.2015.2468078;10.1109/TVCG.2009.108;10.1109/VAST.2009.5333893;10.1109/TVCG.2013.167,"Large Network Exploration,Structure-Based Exploration,Suggestive Exploration",0.0,5.0,2.0,80.0,nan,papers/infovis-tvcg/555_guo.pdf
InfoVis,2018,Structure-aware Fisheye Views for Efficient Large Graph Exploration,10.1109/TVCG.2018.2864911,http://dx.doi.org/10.1109/TVCG.2018.2864911,566,575,J,"Traditional fisheye views for exploring large graphs introduce substantial distortions that often lead to a decreased readability of paths and other interesting structures. To overcome these problems, we propose a framework for structure-aware fisheye views. Using edge orientations as constraints for graph layout optimization allows us not only to reduce spatial and temporal distortions during fisheye zooms, but also to improve the readability of the graph structure. Furthermore, the framework enables us to optimize fisheye lenses towards specific tasks and design a family of new lenses: polyfocal, cluster, and path lenses. A GPU implementation lets us process large graphs with up to 15,000 nodes at interactive rates. A comprehensive evaluation, a user study, and two case studies demonstrate that our structure-aware fisheye views improve layout readability and user performance.",Yunhai Wang;Yanyan Wang;Haifeng Zhang;Yinqi Sun;Chi-Wing Fu;Michael Sedlmair;Baoquan Chen;Oliver Deussen,Yunhai Wang;Yanyan Wang;Haifeng Zhang;Yinqi Sun;Chi-Wing Fu;Michael Sedlmair;Baoquan Chen;Oliver Deussen,"Shandong University;Shandong University;Shandong University;Shandong University;Chinese University of Hong Kong;VISUSUniversity of Stuttgart;Peking University;Konstanz University, Germany",10.1109/TVCG.2015.2467035;10.1109/INFVIS.2005.1532130;10.1109/TVCG.2006.156;10.1109/TVCG.2008.130;10.1109/INFVIS.2004.66;10.1109/TVCG.2011.223;10.1109/TVCG.2009.108;10.1109/TVCG.2007.70577;10.1109/TVCG.2017.2745919;10.1109/INFVIS.2003.1249008;10.1109/TVCG.2012.189,"Graph Visualization,Focus-Context Technique,Structure-aware Zoom,Graph Layout Technique",0.0,2.0,2.0,56.0,nan,papers/infovis-tvcg/566_wang.pdf
InfoVis,2018,"Graphicle: Exploring Units, Networks, and Context in a Blended Visualization Approach",10.1109/TVCG.2018.2865151,http://dx.doi.org/10.1109/TVCG.2018.2865151,576,585,J,"Many real-world datasets are large, multivariate, and relational in nature and relevant associated decisions frequently require a simultaneous consideration of both attributes and connections. Existing visualization systems and approaches, however, often make an explicit trade-off between either affording rich exploration of individual data units and their attributes or exploration of the underlying network structure. In doing so, important analysis opportunities and insights are potentially missed. In this study, we aim to address this gap by (1) considering visualizations and interaction techniques that blend the spectrum between unit and network visualizations, (2) discussing the nature of different forms of contexts and the challenges in implementing them, and (3) demonstrating the value of our approach for visual exploration of multivariate, relational data for a real-world use case. Specifically, we demonstrate through a system called Graphicle how network structure can be layered on top of unit visualization techniques to create new opportunities for visual exploration of physician characteristics and referral data. We report on the design, implementation, and evaluation of the system and effectiveness of our blended approach.",Timothy Major;Rahul C. Basole,Timothy Major;Rahul C. Basole,Georgia Institute of Technology;Georgia Institute of Technology,10.1109/TVCG.2012.252;10.1109/INFVIS.2005.1532126;10.1109/TVCG.2007.70539;10.1109/TVCG.2007.70582;10.1109/TVCG.2014.2346292;10.1109/TVCG.2013.227;10.1109/TVCG.2006.166;10.1109/TVCG.2014.2346441;10.1109/TVCG.2009.108;10.1109/INFVIS.2003.1249004;10.1109/TVCG.2010.205;10.1109/TVCG.2007.70589;10.1109/VAST.2009.5333880;10.1109/TVCG.2013.167,"Unit visualization,network visualization,context",0.0,1.0,1.0,46.0,nan,papers/infovis-tvcg/576_major.pdf
SciVis,2018,Interactive obstruction-free lensing for volumetric data visualization,10.1109/TVCG.2018.2864690,http://dx.doi.org/10.1109/TVCG.2018.2864690,1029,1039,J,"Occlusion is an issue in volumetric visualization as it prevents direct visualization of the region of interest. While many techniques such as transfer functions, volume segmentation or view distortion have been developed to address this, there is still room for improvement to better support the understanding of objects' vicinity. However, most existing Focus+Context fail to solve partial occlusion in datasets where the target and the occluder are very similar density-wise. For these reasons, we investigate a new technique which maintains the general structure of the investigated volumetric dataset while addressing occlusion issues. With our technique, the user interactively defines an area of interest where an occluded region or object is partially visible. Then our lens starts pushing at its border occluding objects, thus revealing hidden volumetric data. Next, the lens is modified with an extended field of view (fish-eye deformation) to better see the vicinity of the selected region. Finally, the user can freely explore the surroundings of the area under investigation within the lens. To provide real-time exploration, we implemented our lens using a GPU accelerated ray-casting framework to handle ray deformations, local lighting, and local viewpoint manipulation. We illustrate our technique with five application scenarios in baggage inspection, 3D fluid flow visualization, chest radiology, air traffic planning, and DTI fiber exploration.",Michael Traoré;Christophe Hurter;Alexandru Telea,Michael Traoré;Christophe Hurter;Alexandru Telea,ENACFrench Civil Aviation University;ENACFrench Civil Aviation University;Institute Johan BernoulliUniversity of Groningen,10.1109/TVCG.2006.140;10.1109/TVCG.2006.144;10.1109/TVCG.2007.70565;10.1109/TVCG.2010.127;10.1109/TVCG.2009.138;10.1109/VISUAL.2004.32;10.1109/TVCG.2011.223;10.1109/TVCG.2010.193;10.1109/TVCG.2006.124;10.1109/VISUAL.2003.1250400;10.1109/VISUAL.1999.809865;10.1109/TVCG.2012.265;10.1109/TVCG.2016.2599049;10.1109/VISUAL.2005.1532818,"Interaction techniques,focus + context,volume visualization,volume rendering,raycasting",0.0,2.0,1.0,51.0,nan,papers/scivis-tvcg/1029_traore.pdf
SciVis,2018,Dynamic Volume Lines: Visual Comparison of 3D Volumes through Space-filling Curves,10.1109/TVCG.2018.2864510,http://dx.doi.org/10.1109/TVCG.2018.2864510,1040,1049,J,"The comparison of many members of an ensemble is difficult, tedious, and error-prone, which is aggravated by often just subtle differences. In this paper, we introduce Dynamic Volume Lines for the interactive visual analysis and comparison of sets of 3D volumes. Each volume is linearized along a Hilbert space-filling curve into a 1D Hilbert line plot, which depicts the intensities over the Hilbert indices. We present a nonlinear scaling of these 1D Hilbert line plots based on the intensity variations in the ensemble of 3D volumes, which enables a more effective use of the available screen space. The nonlinear scaling builds the basis for our interactive visualization techniques. An interactive histogram heatmap of the intensity frequencies serves as overview visualization. When zooming in, the frequencies are replaced by detailed 1D Hilbert line plots and optional functional boxplots. To focus on important regions of the volume ensemble, nonlinear scaling is incorporated into the plots. An interactive scaling widget depicts the local ensemble variations. Our brushing and linking interface reveals, for example, regions with a high ensemble variation by showing the affected voxels in a 3D spatial view. We show the applicability of our concepts using two case studies on ensembles of 3D volumes resulting from tomographic reconstruction. In the first case study, we evaluate an artificial specimen from simulated industrial 3D X-ray computed tomography (XCT). In the second case study, a real-world XCT foam specimen is investigated. Our results show that Dynamic Volume Lines can identify regions with high local intensity variations, allowing the user to draw conclusions, for example, about the choice of reconstruction parameters. Furthermore, it is possible to detect ring artifacts in reconstructions volumes.",Johannes Weissenböck;Bernhard Fröhler;M. Eduard Gröller;Johann Kastner;Christoph Heinzl,Johannes Weissenböck;Bernhard Fröhler;Eduard Gröller;Johann Kastner;Christoph Heinzl,"University of Applied Sciences Upper Austria, Wels, Austria;University of Applied Sciences Upper Austria, Wels, Austria;TU Wien, Vienna, Austria;University of Applied Sciences Upper Austria, Wels, Austria;University of Applied Sciences Upper Austria, Wels, Austria",10.1109/TVCG.2014.2346448;10.1109/VAST.2015.7347634;10.1109/TVCG.2009.155;10.1109/TVCG.2014.2346455;10.1109/VISUAL.2005.1532847;10.1109/TVCG.2013.213;10.1109/VAST.2014.7042491;10.1109/TVCG.2014.2346321;10.1109/VAST.2016.7883516;10.1109/TVCG.2013.143,"Ensemble data,comparative visualization,visual analysis,Hilbert curve,nonlinear scaling,X-ray computed tomography",nan,2.0,1.0,43.0,nan,papers/scivis-tvcg/1040_weissenbock.pdf
SciVis,2018,A Declarative Grammar of Flexible Volume Visualization Pipelines,10.1109/TVCG.2018.2864841,http://dx.doi.org/10.1109/TVCG.2018.2864841,1050,1059,J,"This paper presents a declarative grammar for conveniently and effectively specifying advanced volume visualizations. Existing methods for creating volume visualizations either lack the flexibility to specify sophisticated visualizations or are difficult to use for those unfamiliar with volume rendering implementation and parameterization. Our design provides the ability to quickly create expressive visualizations without knowledge of the volume rendering implementation. It attempts to capture aspects of those difficult but powerful methods while remaining flexible and easy to use. As a proof of concept, our current implementation of the grammar allows users to combine multiple data variables in various ways and define transfer functions for diverse input data. The grammar also has the ability to describe advanced shading effects and create animations. We demonstrate the power and flexibility of our approach using multiple practical volume visualizations.",Min Shih;Charles Rozhon;Kwan-Liu Ma,Min Shih;Charles Rozhon;Kwan-Liu Ma,"University of California, Davis;University of California, Davis;University of California, Davis",10.1109/VISUAL.2005.1532788;10.1109/TVCG.2009.174;10.1109/TVCG.2011.185;10.1109/TVCG.2007.70555;10.1109/TVCG.2014.2346322;10.1109/TVCG.2009.189;10.1109/TVCG.2015.2467449;10.1109/VISUAL.1992.235219;10.1109/VISUAL.2004.95;10.1109/TVCG.2007.70534;10.1109/TVCG.2014.2346318;10.1109/TVCG.2016.2599030;10.1109/TVCG.2015.2467091;10.1109/SciVis.2015.7429514;10.1109/TVCG.2016.2599041,"Volume visualization,direct volume rendering,declarative specification,multivariate/multimodal volume data,animation",0.0,1.0,0.0,41.0,nan,papers/scivis-tvcg/1050_shih.pdf
InfoVis,2018,Vistrates: A Component Model for Ubiquitous Analytics,10.1109/TVCG.2018.2865144,http://dx.doi.org/10.1109/TVCG.2018.2865144,586,596,J,"Visualization tools are often specialized for specific tasks, which turns the user's analytical workflow into a fragmented process performed across many tools. In this paper, we present a component model design for data visualization to promote modular designs of visualization tools that enhance their analytical scope. Rather than fragmenting tasks across tools, the component model supports unification, where components-the building blocks of this model-can be assembled to support a wide range of tasks. Furthermore, the model also provides additional key properties, such as support for collaboration, sharing across multiple devices, and adaptive usage depending on expertise, from creating visualizations using dropdown menus, through instantiating components, to actually modifying components or creating entirely new ones from scratch using JavaScript or Python source code. To realize our model, we introduce Vistrates, a literate computing platform for developing, assembling, and sharing visualization components. From a visualization perspective, Vistrates features cross-cutting components for visual representations, interaction, collaboration, and device responsiveness maintained in a component repository. From a development perspective, Vistrates offers a collaborative programming environment where novices and experts alike can compose component pipelines for specific analytical activities. Finally, we present several Vistrates use cases that span the full range of the classic “anytime” and “anywhere” motto for ubiquitous analysis: from mobile and on-the-go usage, through office settings, to collaborative smart environments covering a variety of tasks and devices..",Sriram Karthik Badam;Andreas Mathisen;Roman Rädle;Clemens Nylandsted Klokmose;Niklas Elmqvist,Sriram Karthik Badam;Andreas Mathisen;Roman Rädle;Clemens N. Klokmose;Niklas Elmqvist,"University of Maryland, College Park, MD, USA;Aarhus University, Aarhus, Denmark;Aarhus University, Aarhus, Denmark;Aarhus University, Aarhus, Denmark;University of Maryland, College Park, MD, USA",10.1109/TVCG.2016.2598647;10.1109/TVCG.2017.2743990;10.1109/TVCG.2009.174;10.1109/TVCG.2011.185;10.1109/TVCG.2017.2745278;10.1109/INFVIS.2000.885092;10.1109/TVCG.2013.197;10.1109/VAST.2007.4389011;10.1109/TVCG.2008.137;10.1109/TVCG.2017.2744019;10.1109/TVCG.2012.204;10.1109/TVCG.2013.191;10.1109/TVCG.2014.2346573;10.1109/TVCG.2013.200;10.1109/TVCG.2014.2346291;10.1109/TVCG.2016.2599030;10.1109/TVCG.2014.2346574;10.1109/INFVIS.2000.885086;10.1109/TVCG.2009.162;10.1109/TVCG.2007.70577;10.1109/TVCG.2007.70589,"Components,literate computing,development,exploration,dissemination,collaboration,heterogeneous devices",0.0,3.0,2.0,80.0,nan,papers/infovis-tvcg/586_badam.pdf
InfoVis,2018,SmartCues: A Multitouch Query Approach for Details-on-Demand through Dynamically Computed Overlays,10.1109/TVCG.2018.2865231,http://dx.doi.org/10.1109/TVCG.2018.2865231,597,607,J,"Details-on-demand is a crucial feature in the visual information-seeking process but is often only implemented in highly constrained settings. The most common solution, hover queries (i.e., tooltips), are fast and expressive but are usually limited to single mark (e.g., a bar in a bar chart). `Queries' to retrieve details for more complex sets of objects (e.g., comparisons between pairs of elements, averages across multiple items, trend lines, etc.) are difficult for end-users to invoke explicitly. Further, the output of these queries require complex annotations and overlays which need to be displayed and dismissed on demand to avoid clutter. In this work we introduce SmartCues, a library to support details-on-demand through dynamically computed overlays. For end-users, SmartCues provides multitouch interactions to construct complex queries for a variety of details. For designers, SmartCues offers an interaction library that can be used out-of-the-box, and can be extended for new charts and detail types. We demonstrate how SmartCues can be implemented across a wide array of visualization types and, through a lab study, show that end users can effectively use SmartCues.",Hariharan Subramonyam;Eytan Adar,Hariharan Subramonyam;Eytan Adar,School of InformationUniversity of Michigan;School of InformationUniversity of Michigan,10.1109/INFVIS.1995.528688;10.1109/INFVIS.2005.1532136;10.1109/TVCG.2013.124;10.1109/TVCG.2006.187;10.1109/VAST.2010.5652885;10.1109/TVCG.2013.119;10.1109/VAST.2012.6400487;10.1109/TVCG.2014.2346250;10.1109/TVCG.2012.229;10.1109/TVCG.2008.109;10.1109/TVCG.2012.204;10.1109/TVCG.2010.179;10.1109/TVCG.2017.2745958;10.1109/TVCG.2007.70515,"Graphical overlays,details-on-demand,graph comprehension",0.0,1.0,0.0,73.0,nan,papers/infovis-tvcg/597_subramonyam.pdf
InfoVis,2018,"Multiple Coordinated Views at Large Displays for Multiple Users: Empirical Findings on User Behavior, Movements, and Distances",10.1109/TVCG.2018.2865235,http://dx.doi.org/10.1109/TVCG.2018.2865235,608,618,J,"Interactive wall-sized displays benefit data visualization. Due to their sheer display size, they make it possible to show large amounts of data in multiple coordinated views (MCV) and facilitate collaborative data analysis. In this work, we propose a set of important design considerations and contribute a fundamental input vocabulary and interaction mapping for MCV functionality. We also developed a fully functional application with more than 45 coordinated views visualizing a real-world, multivariate data set of crime activities, which we used in a comprehensive qualitative user study investigating how pairs of users behave. Most importantly, we found that flexible movement is essential and-depending on user goals-is connected to collaboration, perception, and interaction. Therefore, we argue that for future systems interaction from the distance is required and needs good support. We show that our consistent design for both direct touch at the large display and distant interaction using mobile phones enables the seamless exploration of large-scale MCV at wall-sized displays. Our MCV application builds on design aspects such as simplicity, flexibility, and visual consistency and, therefore, supports realistic workflows. We believe that in the future, many visual data analysis scenarios will benefit from wall-sized displays presenting numerous coordinated visualizations, for which our findings provide a valuable foundation.",Ricardo Langner;Ulrike Kister;Raimund Dachselt,Ricardo Langner;Ulrike Kister;Raimund Dachselt,"Interactive Media Lab, Technische Universität Dresden, Germany;Interactive Media Lab, Technische Universität Dresden, Germany;Interactive Media Lab, Technische Universität Dresden, Germany",10.1109/VAST.2016.7883506;10.1109/TVCG.2012.251;10.1109/VAST.2010.5652880;10.1109/TVCG.2013.166;10.1109/TVCG.2013.134;10.1109/TVCG.2017.2743859;10.1109/TVCG.2017.2744019;10.1109/TVCG.2012.204;10.1109/TVCG.2017.2744198;10.1109/TVCG.2017.2745219;10.1109/TVCG.2009.162;10.1109/TVCG.2012.237;10.1109/TVCG.2012.275;10.1109/INFVIS.1996.559216;10.1109/TVCG.2006.184,"Multiple coordinated views,wall-sized displays,mobile devices,distant interaction,physical navigation,user behavior,user movements,multi-user,collaborative data analysis",0.0,3.0,2.0,79.0,nan,papers/infovis-tvcg/608_langner.pdf
InfoVis,2018,Visualizing Ranges over Time on Mobile Phones: A Task-Based Crowdsourced Evaluation,10.1109/TVCG.2018.2865234,http://dx.doi.org/10.1109/TVCG.2018.2865234,619,629,J,"In the first crowdsourced visualization experiment conducted exclusively on mobile phones, we compare approaches to visualizing ranges over time on small displays. People routinely consume such data via a mobile phone, from temperatures in weather forecasting apps to sleep and blood pressure readings in personal health apps. However, we lack guidance on how to effectively visualize ranges on small displays in the context of different value retrieval and comparison tasks, or with respect to different data characteristics such as periodicity, seasonality, or the cardinality of ranges. Central to our experiment is a comparison between two ways to lay out ranges: a more conventional linear layout strikes a balance between quantitative and chronological scale resolution, while a less conventional radial layout emphasizes the cyclicality of time and may prioritize discrimination between values at its periphery. With results from 87 crowd workers, we found that while participants completed tasks more quickly with linear layouts than with radial ones, there were few differences in terms of error rate between layout conditions. We also found that participants performed similarly with both layouts in tasks that involved comparing superimposed observed and average ranges.",Matthew Brehmer;Bongshin Lee;Petra Isenberg;Eun Kyoung Choe,Matthew Brehmer;Bongshin Lee;Petra Isenberg;Eun Kyoung Choe,"Microsoft Research;Microsoft Research;Inria;The University of Maryland, College Park",10.1109/TVCG.2011.185;10.1109/TVCG.2013.124;10.1109/TVCG.2010.209;10.1109/TVCG.2010.162;10.1109/VAST.2007.4388994,"Evaluation,graphical perception,mobile phones,range visualization,crowdsourcing",0.0,4.0,3.0,54.0,nan,papers/infovis-tvcg/619_brehmer.pdf
InfoVis,2018,Glanceable Visualization: Studies of Data Comparison Performance on Smartwatches,10.1109/TVCG.2018.2865142,http://dx.doi.org/10.1109/TVCG.2018.2865142,630,640,J,"We present the results of two perception studies to assess how quickly people can perform a simple data comparison task for small-scale visualizations on a smartwatch. The main goal of these studies is to extend our understanding of design constraints for smartwatch visualizations. Previous work has shown that a vast majority of smartwatch interactions last under 5 s. It is still unknown what people can actually perceive from visualizations during such short glances, in particular with such a limited display space of smartwatches. To shed light on this question, we conducted two perception studies that assessed the lower bounds of task time for a simple data comparison task. We tested three chart types common on smartwatches: bar charts, donut charts, and radial bar charts with three different data sizes: 7, 12, and 24 data values. In our first study, we controlled the differences of the two target bars to be compared, while the second study varied the difference randomly. For both studies, we found that participants performed the task on average in &lt;;300 ms for the bar chart, &lt;;220 ms for the donut chart, and in &lt;; 1780 ms for the radial bar chart. Thresholds in the second study per chart type were on average 1.14-1.35× higher than in the first study. Our results show that bar and donut charts should be preferred on smartwatch displays when quick data comparisons are necessary.",Tanja Blascheck;Lonni Besançon;Anastasia Bezerianos;Bongshin Lee;Petra Isenberg,Tanja Blascheck;Lonni Besançon;Anastasia Bezerianos;Bongshin Lee;Petra Isenberg,Inria;Université Paris Saclay;Université Paris SudInriaCNRSUniversité Paris Saclay;Microsoft Research;Inria,10.1109/TVCG.2014.2346435;10.1109/TVCG.2012.233;10.1109/TVCG.2010.162;10.1109/TVCG.2013.192;10.1109/INFVIS.2005.1532144;10.1109/TVCG.2012.196;10.1109/TVCG.2014.2346320;10.1109/TVCG.2007.70589,"Glanceable visualization,smartwatch,perception,quantitative evaluation,data comparison",0.0,7.0,3.0,52.0,nan,papers/infovis-tvcg/630_blascheck.pdf
VAST,2018,"SIRIUS: Dual, Symmetric, Interactive Dimension Reductions",10.1109/TVCG.2018.2865047,http://dx.doi.org/10.1109/TVCG.2018.2865047,172,182,J,"Much research has been done regarding how to visualize and interact with observations and attributes of high-dimensional data for exploratory data analysis. From the analyst's perceptual and cognitive perspective, current visualization approaches typically treat the observations of the high-dimensional dataset very differently from the attributes. Often, the attributes are treated as inputs (e.g., sliders), and observations as outputs (e.g., projection plots), thus emphasizing investigation of the observations. However, there are many cases in which analysts wish to investigate both the observations and the attributes of the dataset, suggesting a symmetry between how analysts think about attributes and observations. To address this, we define SIRIUS (Symmetric Interactive Representations In a Unified System), a symmetric, dual projection technique to support exploratory data analysis of high-dimensional data. We provide an example implementation of SIRIUS and demonstrate how this symmetry affords additional insights.",Michelle Dowling;John E. Wenskovitch;J. T. Fry;Scotland Leman;Leanna House;Chris North,Michelle Dowling;John Wenskovitch;J.T. Fry;Scotland Leman;Leanna House;Chris North,Virginia Tech Department of Computer Science;Virginia Tech Department of Computer Science;Virginia Tech Department of Statistics;Virginia Tech Department of Statistics;Virginia Tech Department of Statistics;Virginia Tech Department of Computer Science,10.1109/VAST.2012.6400493;10.1109/INFVIS.2005.1532136;10.1109/VAST.2014.7042492;10.1109/VAST.2012.6400486;10.1109/TVCG.2015.2467552;10.1109/VAST.2010.5652443;10.1109/TVCG.2012.260;10.1109/VAST.2011.6102449;10.1109/TVCG.2011.220;10.1109/TVCG.2016.2598445;10.1109/TVCG.2016.2598446;10.1109/INFVIS.2003.1249020;10.1109/TVCG.2008.173;10.1109/TVCG.2011.178;10.1109/TVCG.2012.256;10.1109/TVCG.2016.2598479;10.1109/TVCG.2013.150,"Dimension reduction,semantic interaction,exploratory data analysis,observation projection,attribute projection",0.0,3.0,1.0,59.0,nan,papers/vast-tvcg/172_dowling.pdf
VAST,2018,SMARTexplore: Simplifying High-Dimensional Data Analysis through a Table-Based Visual Analytics Approach,10.1109/VAST.2018.8802486,http://dx.doi.org/10.1109/VAST.2018.8802486,36,47,C,"We present SMARTEXPLORE, a novel visual analytics technique that simplifies the identification and understanding of clusters, correlations, and complex patterns in high-dimensional data. The analysis is integrated into an interactive table-based visualization that maintains a consistent and familiar representation throughout the analysis. The visualization is tightly coupled with pattern matching, subspace analysis, reordering, and layout algorithms. To increase the analyst's trust in the revealed patterns, SMARTEXPLORE automatically selects and computes statistical measures based on dimension and data properties. While existing approaches to analyzing high-dimensional data (e.g., planar projections and Parallel coordinates) have proven effective, they typically have steep learning curves for non-visualization experts. Our evaluation, based on three expert case studies, confirms that non-visualization experts successfully reveal patterns in high-dimensional data when using SMARTEXPLORE.",Michael Blumenschein;Michael Behrisch 0001;Stefanie Schmid;Simon Butscher;Deborah Wahl;Karoline Villinger;Britta Renner;Harald Reiterer;Daniel A. Keim,Michael Blumenschein;Michael Behrisch;Stefanie Schmid;Simon Butscher;Deborah R. Wahl;Karoline Villinger;Britta Renner;Harald Reiterer;Daniel A. Keim,"University of Konstanz, Germany;Harvard University, USA;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany",10.1109/INFVIS.2004.46;10.1109/VAST.2010.5652433;10.1109/INFVIS.1998.729559;10.1109/TVCG.2017.2743978;10.1109/TVCG.2011.188;10.1109/VAST.2009.5332611;10.1109/TVCG.2010.184;10.1109/TVCG.2014.2346260;10.1109/TVCG.2013.173;10.1109/TVCG.2015.2467553;10.1109/TVCG.2014.2346248;10.1109/TVCG.2010.138;10.1109/INFVIS.2004.15;10.1109/TVCG.2014.2346279;10.1109/INFVIS.2003.1249016;10.1109/VAST.2009.5332628;10.1109/TVCG.2015.2468078;10.1109/TVCG.2017.2745078;10.1109/TVCG.2017.2744098;10.1109/TVCG.2013.150,"High-dimensional data,visual exploration,pattern-driven analysis,tabular visualization,subspace,aggregation",nan,0.0,7.0,0.0,nan,papers/vast-conference/blumenschein.pdf
VAST,2018,EmbeddingVis: A Visual Analytics Approach to Comparative Network Embedding Inspection,10.1109/VAST.2018.8802454,http://dx.doi.org/10.1109/VAST.2018.8802454,48,59,C,"Constructing latent vector representation for nodes in a network through embedding models has shown its practicality in many graph analysis applications, such as node classification, clustering, and link prediction. However, despite the high efficiency and accuracy of learning an embedding model, people have little clue of what information about the original network is preserved in the embedding vectors. The abstractness of low-dimensional vector representation, stochastic nature of the construction process, and non-transparent hyper-parameters all obscure understanding of network embedding results. Visualization techniques have been introduced to facilitate embedding vector inspection, usually by projecting the embedding space to a two-dimensional display. Although the existing visualization methods allow simple examination of the structure of embedding space, they cannot support in-depth exploration of the embedding vectors. In this paper, we design an exploratory visual analytics system that supports the comparative visual interpretation of embedding vectors at the cluster, instance, and structural levels. To be more specific, it facilitates comparison of what and how node metrics are preserved across different embedding models and investigation of relationships between node metrics and selected embedding vectors. Several case studies confirm the efficacy of our system. Experts' feedback suggests that our approach indeed helps them better embrace the understanding of network embedding models.",Quan Li;Kristanto Sean Njotoprawiro;Hammad Haleem;Qiaoan Chen;Chris Yi;Xiaojuan Ma,Quan Li;Kristanto Sean Njotoprawiro;Hammad Haleem;Qiaoan Chen;Chris Yi;Xiaojuan Ma,"Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong;Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong;Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong;WeChat, Tencent Technology (Shenzhen) Co., Ltd., Shenzhen, China;WeChat, Tencent Technology (Shenzhen) Co., Ltd., Shenzhen, China;Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong",10.1109/TVCG.2007.70521;10.1109/TVCG.2013.173;10.1109/VISUAL.1990.146402;10.1109/TVCG.2016.2598415;10.1109/TVCG.2017.2745141;10.1109/TVCG.2016.2598838;10.1109/TVCG.2015.2468151,"Human-centered computing,Visualization,Visualization application domains,Visual analytics,Human-centered computing,Visualization,Visualization design and evaluation methods",0.0,3.0,3.0,0.0,nan,papers/vast-conference/li.pdf
InfoVis,2018,Evaluating ‘Graphical Perception’ with CNNs,10.1109/TVCG.2018.2865138,http://dx.doi.org/10.1109/TVCG.2018.2865138,641,650,J,"Convolutional neural networks can successfully perform many computer vision tasks on images. For visualization, how do CNNs perform when applied to graphical perception tasks? We investigate this question by reproducing Cleveland and McGill's seminal 1984 experiments, which measured human perception efficiency of different visual encodings and defined elementary perceptual tasks for visualization. We measure the graphical perceptual capabilities of four network architectures on five different visualization tasks and compare to existing and new human performance baselines. While under limited circumstances CNNs are able to meet or outperform human task performance, we find that CNNs are not currently a good model for human graphical perception. We present the results of these experiments to foster the understanding of how CNNs succeed and fail when applied to data visualizations.",Daniel Haehn;James Tompkin;Hanspeter Pfister,Daniel Haehn;James Tompkin;Hanspeter Pfister,Harvard University;Brown University;Harvard University,10.1109/TVCG.2014.2346979;10.1109/TVCG.2014.2346320,"Machine Perception,Graphical Perception,Deep Learning,Convolutional Neural Networks",0.0,2.0,1.0,50.0,nan,papers/infovis-tvcg/641_haehn.pdf
InfoVis,2018,NLIZE: A Perturbation-Driven Visual Interrogation Tool for Analyzing and Interpreting Natural Language Inference Models,10.1109/TVCG.2018.2865230,http://dx.doi.org/10.1109/TVCG.2018.2865230,651,660,J,"With the recent advances in deep learning, neural network models have obtained state-of-the-art performances for many linguistic tasks in natural language processing. However, this rapid progress also brings enormous challenges. The opaque nature of a neural network model leads to hard-to-debug-systems and difficult-to-interpret mechanisms. Here, we introduce a visualization system that, through a tight yet flexible integration between visualization elements and the underlying model, allows a user to interrogate the model by perturbing the input, internal state, and prediction while observing changes in other parts of the pipeline. We use the natural language inference problem as an example to illustrate how a perturbation-driven paradigm can help domain experts assess the potential limitation of a model, probe its inner states, and interpret and form hypotheses about fundamental model mechanisms such as attention.",Shusen Liu;Zhimin Li;Tao Li;Vivek Srikumar;Valerio Pascucci;Peer-Timo Bremer,Shusen Liu;Zhimin Li;Tao Li;Vivek Srikumar;Valerio Pascucci;Peer-Timo Bremer,Lawrence Livermore National Laboratory;SCI InstituteUniversity of Utah;School of ComputingUniversity of Utah;School of ComputingUniversity of Utah;SCI InstituteUniversity of Utah;Lawrence Livermore National Laboratory,10.1109/TVCG.2017.2744683;10.1109/TVCG.2017.2744718;10.1109/TVCG.2017.2744938;10.1109/TVCG.2016.2598831;10.1109/TVCG.2017.2745141;10.1109/TVCG.2017.2744358;10.1109/TVCG.2017.2744158;10.1109/VISUAL.2005.1532820;10.1109/TVCG.2017.2744878,"Natural Language Processing,Interpretable Machine Learning,Natural Language Inference,Attention Visualization",0.0,4.0,5.0,40.0,nan,papers/infovis-tvcg/651_liu.pdf
InfoVis,2018,Elastic Documents: Coupling Text and Tables through Contextual Visualizations for Enhanced Document Reading,10.1109/TVCG.2018.2865119,http://dx.doi.org/10.1109/TVCG.2018.2865119,661,671,J,"Today's data-rich documents are often complex datasets in themselves, consisting of information in different formats such as text, figures, and data tables. These additional media augment the textual narrative in the document. However, the static layout of a traditional for-print document often impedes deep understanding of its content because of the need to navigate to access content scattered throughout the text. In this paper, we seek to facilitate enhanced comprehension of such documents through a contextual visualization technique that couples text content with data tables contained in the document. We parse the text content and data tables, cross-link the components using a keyword-based matching algorithm, and generate on-demand visualizations based on the reader's current focus within a document. We evaluate this technique in a user study comparing our approach to a traditional reading experience. Results from our study show that (1) participants comprehend the content better with tighter coupling of text and data, (2) the contextual visualizations enable participants to develop better summaries that capture the main data-rich insights within the document, and (3) overall, our method enables participants to develop a more detailed understanding of the document content.",Sriram Karthik Badam;Zhicheng Liu;Niklas Elmqvist,Sriram Karthik Badam;Zhicheng Liu;Niklas Elmqvist,"University of Maryland, College Park, MD, USA;Adobe Research, Seattle, WA, USA;University of Maryland, College Park, MD, USA",10.1109/TVCG.2011.185;10.1109/TVCG.2016.2598594;10.1109/TVCG.2014.2346435;10.1109/TVCG.2011.255;10.1109/TVCG.2007.70594;10.1109/TVCG.2014.2346279;10.1109/INFVIS.2000.885091;10.1109/TVCG.2009.139;10.1109/TVCG.2009.165;10.1109/TVCG.2009.171,"Document reading,contextual visualizations,visual aids,comprehension,summarization",nan,2.0,0.0,66.0,nan,papers/infovis-tvcg/661_badam.pdf
InfoVis,2018,Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication,10.1109/TVCG.2018.2865145,http://dx.doi.org/10.1109/TVCG.2018.2865145,672,681,J,"Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.",Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko,Arjun Srinivasan;Steven M. Drucker;Alex Endert;John Stasko,Georgia Institute of Technology;Microsoft Research;Georgia Institute of Technology;Georgia Institute of Technology,10.1109/INFVIS.2005.1532136;10.1109/TVCG.2013.124;10.1109/TVCG.2010.164;10.1109/TVCG.2013.119;10.1109/TVCG.2012.229;10.1109/TVCG.2007.70594;10.1109/VISUAL.1992.235203;10.1109/TVCG.2017.2744843;10.1109/TVCG.2017.2745219;10.1109/VISUAL.1990.146375;10.1109/TVCG.2015.2467191,"Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication",0.0,10.0,5.0,50.0,nan,papers/infovis-tvcg/672_srinivasan.pdf
InfoVis,2018,What Do We Talk About When We Talk About Dashboards?,10.1109/TVCG.2018.2864903,http://dx.doi.org/10.1109/TVCG.2018.2864903,682,692,J,"Dashboards are one of the most common use cases for data visualization, and their design and contexts of use are considerably different from exploratory visualization tools. In this paper, we look at the broad scope of how dashboards are used in practice through an analysis of dashboard examples and documentation about their use. We systematically review the literature surrounding dashboard use, construct a design space for dashboards, and identify major dashboard types. We characterize dashboards by their design goals, levels of interaction, and the practices around them. Our framework and literature review suggest a number of fruitful research directions to better support dashboard design, implementation and use.",Alper Sarikaya;Michael Correll;Lyn Bartram;Melanie Tory;Danyel Fisher,Alper Sarikaya;Michael Correll;Lyn Bartram;Melanie Tory;Danyel Fisher,Microsoft Corporation;Tableau Research;Simon Fraser University;Tableau Research;Honeycomb.io,10.1109/TVCG.2013.124;10.1109/TVCG.2017.2744198;10.1109/TVCG.2013.120,"Dashboards,literature review,survey,design space,open coding",0.0,17.0,1.0,66.0,nan,papers/infovis-tvcg/682_sarikaya.pdf
SciVis,2018,Visualization of Bubble Formation in Porous Media,10.1109/TVCG.2018.2864506,http://dx.doi.org/10.1109/TVCG.2018.2864506,1060,1069,J,"We present a visualization approach for the analysis of CO<sub>2</sub>bubble-induced attenuation in porous rock formations. As a basis for this, we introduce customized techniques to extract CO<sub>2</sub>bubbles and their surrounding porous structure from X-ray computed tomography data (XCT) measurements. To understand how the structure of porous media influences the occurrence and the shape of formed bubbles, we automatically classify and relate them in terms of morphology and geometric features, and further directly support searching for promising porous structures. To allow for the meaningful direct visual comparison of bubbles and their structures, we propose a customized registration technique considering the bubble shape as well as its points of contact with the porous media surface. With our quantitative extraction of geometric bubble features, we further support the analysis as well as the creation of a physical model. We demonstrate that our approach was successfully used to answer several research questions in the domain, and discuss its high practical relevance to identify critical seismic characteristics of fluid-saturated rock that govern its capability to store CO<sub>2</sub>.",Hui Zhang 0027;Steffen Frey;Holger Steeb;David Uribe;Thomas Ertl;Wenping Wang,Hui Zhang;Steffen Frey;Holger Steeb;David Uribe;Thomas Ertl;Wenping Wang,Department of Computer ScienceThe University of Hong Kong;Visualization Research CenterUniversity of Stuttgart;Institute of Applied MechanicsUniversity of Stuttgart;Institute of Applied MechanicsUniversity of Stuttgart;Visualization Research CenterUniversity of Stuttgart;Department of Computer ScienceThe University of Hong Kong,10.1109/VISUAL.2005.1532788;10.1109/TVCG.2007.70603;10.1109/TVCG.2006.124;10.1109/TVCG.2014.2346351;10.1109/TVCG.2013.177;10.1109/TVCG.2012.200;10.1109/VISUAL.2004.48,"3D volume rendering,bubble visualization,porous media",0.0,0.0,0.0,43.0,nan,papers/scivis-tvcg/1060_zhang.pdf
SciVis,2018,Gaia Sky: Navigating the Gaia Catalog,10.1109/TVCG.2018.2864508,http://dx.doi.org/10.1109/TVCG.2018.2864508,1070,1079,J,"In this paper, we present Gaia Sky, a free and open-source multiplatform 3D Universe system, developed since 2014 in the Data Processing and Analysis Consortium framework of ESA's Gaia mission. Gaia's data release 2 represents the largest catalog of the stars of our Galaxy, comprising 1.3 billion star positions, with parallaxes, proper motions, magnitudes, and colors. In this mission, Gaia Sky is the central tool for off-the-shelf visualization of these data, and for aiding production of outreach material. With its capabilities to effectively handle these data, to enable seamless navigation along the high dynamic range of distances, and at the same time to provide advanced visualization techniques including relativistic aberration and gravitational wave effects, currently no actively maintained cross-platform, modern, and open alternative exists.",Antoni Sagristà;Stefan Jordan;Thomas Müller 0005;Filip Sadlo,Antoni Sagristà;Stefan Jordan;Thomas Müller;Filip Sadlo,Heidelberg University;Heidelberg University;Max Planck Institute for Astronomy;Heidelberg University,10.1109/TVCG.2006.176,"Astronomy visualization,3D Universe software,star catalog rendering,Gaia mission",0.0,2.0,3.0,31.0,nan,papers/scivis-tvcg/1070_sagristaselles.pdf
SciVis,2018,Interactive 3D Visual Analysis of Atmospheric Fronts,10.1109/TVCG.2018.2864806,http://dx.doi.org/10.1109/TVCG.2018.2864806,1080,1090,J,"Atmospheric fronts play a central role in meteorology, as the boundaries between different air masses and as fundamental features of extra-tropical cyclones. They appear in numerous conceptual model depictions of extra-tropical weather systems. Conceptually, fronts are three-dimensional surfaces in space possessing an innate structural complexity, yet in meteorology, both manual and objective identification and depiction have historically focused on the structure in two dimensions. In this work, we -a team of visualization scientists and meteorologists-propose a novel visualization approach to analyze the three-dimensional structure of atmospheric fronts and related physical and dynamical processes. We build upon existing approaches to objectively identify fronts as lines in two dimensions and extend these to obtain frontal surfaces in three dimensions, using the magnitude of temperature change along the gradient of a moist potential temperature field as the primary identifying factor. We introduce the use of normal curves in the temperature gradient field to visualize a frontal zone (i.e., the transitional zone between the air masses) and the distribution of atmospheric variables in such zones. To enable for the first time a statistical analysis of frontal zones, we present a new approach to obtain the volume enclosed by a zone, by classifying grid boxes that intersect with normal curves emanating from a selected front. We introduce our method by means of an idealized numerical simulation and demonstrate its use with two real-world cases using numerical weather prediction data.",Michael Kern;Tim Hewson;Andreas Schäfler;Rüdiger Westermann;Marc Rautenhaus,Michael Kern;Tim Hewson;Andreas Schätler;Rüdiger Westermann;Marc Rautenhaus,"Computer Graphics & Visualization Group, Technische Universität München, Garching, Germany;European Centre for Medium-Range Weather Forecasts, Reading, UK;Deutsches Zentrum für Luft- und Raumfahrt (DLR), Germany;Computer Graphics & Visualization Group, Technische Universität München, Garching, Germany;Computer Graphics & Visualization Group, Technische Universität München, Garching, Germany",10.1109/TVCG.2017.2743989;10.1109/TVCG.2007.70554,"Meteorology,Atmospheric Fronts,Feature Detection",0.0,1.0,0.0,56.0,nan,papers/scivis-tvcg/1080_kern.pdf
SciVis,2018,An Interactive Framework for Visualization of Weather Forecast Ensembles,10.1109/TVCG.2018.2864815,http://dx.doi.org/10.1109/TVCG.2018.2864815,1091,1101,J,"Numerical Weather Prediction (NWP) ensembles are commonly used to assess the uncertainty and confidence in weather forecasts. Spaghetti plots are conventional tools for meteorologists to directly examine the uncertainty exhibited by ensembles, where they simultaneously visualize isocontours of all ensemble members. To avoid visual clutter in practical usages, one needs to select a small number of informative isovalues for visual analysis. Moreover, due to the complex topology and variation of ensemble isocontours, it is often a challenging task to interpret the spaghetti plot for even a single isovalue in large ensembles. In this paper, we propose an interactive framework for uncertainty visualization of weather forecast ensembles that significantly improves and expands the utility of spaghetti plots in ensemble analysis. Complementary to state-of-the-art methods, our approach provides a complete framework for visual exploration of ensemble isocontours, including isovalue selection, interactive isocontour variability exploration, and interactive sub-region selection and re-analysis. Our framework is built upon the high-density clustering paradigm, where the mode structure of the density function is represented as a hierarchy of nested subsets of the data. We generalize the high-density clustering for isocontours and propose a bandwidth selection method for estimating the density function of ensemble isocontours. We present novel visualizations based on high-density clustering results, called the mode plot and the simplified spaghetti plot. The proposed mode plot visually encodes the structure provided by the high-density clustering result and summarizes the distribution of ensemble isocontours. It also enables the selection of subsets of interesting isocontours, which are interactively highlighted in a linked spaghetti plot for providing spatial context. To provide an interpretable overview of the positional variability of isocontours, our system allows for selection of informative isovalues from the simplified spaghetti plot. Due to the spatial variability of ensemble isocontours, the system allows for interactive selection and focus on sub-regions for local uncertainty and clustering re-analysis. We examine a number of ensemble datasets to establish the utility of our approach and discuss its advantages over state-of-the-art visual analysis tools for ensemble data.",Bo Ma 0002;Alireza Entezari,Bo Ma;Alireza Entezari,University of Florida;University of Florida,10.1109/TVCG.2013.208;10.1109/TVCG.2015.2467958;10.1109/TVCG.2016.2598869;10.1109/TVCG.2014.2346448;10.1109/TVCG.2015.2467204;10.1109/TVCG.2016.2598868;10.1109/TVCG.2015.2468093;10.1109/TVCG.2017.2745178;10.1109/TVCG.2015.2467754;10.1109/TVCG.2010.181;10.1109/TVCG.2014.2346332;10.1109/TVCG.2016.2598830;10.1109/TVCG.2013.143,"Spaghetti plots,ensemble visualization,uncertainty visualization,high-density clustering,ensemble forecasting",0.0,2.0,0.0,56.0,nan,papers/scivis-tvcg/1091_ma.pdf
VAST,2018,Vulnus: Visual Vulnerability Analysis for Network Security,10.1109/TVCG.2018.2865028,http://dx.doi.org/10.1109/TVCG.2018.2865028,183,192,J,"Vulnerabilities represent one of the main weaknesses of IT systems and the availability of consolidated official data, like CVE (Common Vulnerabilities and Exposures), allows for using them to compute the paths an attacker is likely to follow. However, even if patches are available, business constraints or lack of resources create obstacles to their straightforward application. As a consequence, the security manager of a network needs to deal with a large number of vulnerabilities, making decisions on how to cope with them. This paper presents VULNUS (VULNerabilities visUal aSsessment), a visual analytics solution for dynamically inspecting the vulnerabilities spread on networks, allowing for a quick understanding of the network status and visually classifying nodes according to their vulnerabilities. Moreover, VULNUS computes the approximated optimal sequence of patches able to eliminate all the attack paths and allows for exploring sub-optimal patching strategies, simulating the effect of removing one or more vulnerabilities. VULNUS has been evaluated by domain experts using a lab-test experiment, investigating the effectiveness and efficiency of the proposed solution.",Marco Angelini;Graziano Blasilli;Tiziana Catarci;Simone Lenti;Giuseppe Santucci,Marco Angelini;Graziano Blasilli;Tiziana Catarci;Simone Lenti;Giuseppe Santucci,University of Rome “La Sapienza”;University of Rome “La Sapienza”;University of Rome “La Sapienza”;University of Rome “La Sapienza”;University of Rome “La Sapienza”,10.1109/TVCG.2007.70540;10.1109/TVCG.2007.70522;10.1109/INFVIS.2001.963283,"Visual Analytics,Network security,Vulnerability analysis,CVE,CVSS,Attack Graph,Vulnerability triage and management",0.0,3.0,3.0,46.0,nan,papers/vast-tvcg/183_angelini.pdf
VAST,2018,GraphProtector: A Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms,10.1109/TVCG.2018.2865021,http://dx.doi.org/10.1109/TVCG.2018.2865021,193,203,J,"Analyzing social networks reveals the relationships between individuals and groups in the data. However, such analysis can also lead to privacy exposure (whether intentionally or inadvertently): leaking the real-world identity of ostensibly anonymous individuals. Most sanitization strategies modify the graph's structure based on hypothesized tactics that an adversary would employ. While combining multiple anonymization schemes provides a more comprehensive privacy protection, deciding the appropriate set of techniques-along with evaluating how applying the strategies will affect the utility of the anonymized results-remains a significant challenge. To address this problem, we introduce GraphProtector, a visual interface that guides a user through a privacy preservation pipeline. GraphProtector enables multiple privacy protection schemes which can be simultaneously combined together as a hybrid approach. To demonstrate the effectiveness of GraphPro tector, we report several case studies and feedback collected from interviews with expert users in various scenarios.",Xu-Meng Wang;Wei Chen 0001;Jia-Kai Chou;Chris Bryan;Huihua Guan;Wenlong Chen;Rusheng Pan;Kwan-Liu Ma,Xumeng Wang;Wei Chen;Jia-Kai Chou;Chris Bryan;Huihua Guan;Wenlong Chen;Rusheng Pan;Kwan-Liu Ma,"Zhejiang University;Zhejiang University;University of California, Davis;University of California, Davis;Zhejiang UniversityAlibaba Group;Zhejiang University;Zhejiang University;University of California, Davis",10.1109/TVCG.2011.163;10.1109/TVCG.2017.2745139;10.1109/TVCG.2014.2346920,"Graph privacy,k-anonymity,structural features,privacy preservation",0.0,3.0,1.0,57.0,nan,papers/vast-tvcg/193_wang.pdf
VAST,2018,Situ: Identifying and Explaining Suspicious Behavior in Networks,10.1109/TVCG.2018.2865029,http://dx.doi.org/10.1109/TVCG.2018.2865029,204,214,J,"Despite the best efforts of cyber security analysts, networked computing assets are routinely compromised, resulting in the loss of intellectual property, the disclosure of state secrets, and major financial damages. Anomaly detection methods are beneficial for detecting new types of attacks and abnormal network activity, but such algorithms can be difficult to understand and trust. Network operators and cyber analysts need fast and scalable tools to help identify suspicious behavior that bypasses automated security systems, but operators do not want another automated tool with algorithms they do not trust. Experts need tools to augment their own domain expertise and to provide a contextual understanding of suspicious behavior to help them make decisions. In this paper we present Situ, a visual analytics system for discovering suspicious behavior in streaming network data. Situ provides a scalable solution that combines anomaly detection with information visualization. The system's visualizations enable operators to identify and investigate the most anomalous events and IP addresses, and the tool provides context to help operators understand why they are anomalous. Finally, operators need tools that can be integrated into their workflow and with their existing tools. This paper describes the Situ platform and its deployment in an operational network setting. We discuss how operators are currently using the tool in a large organization's security operations center and present the results of expert reviews with professionals.",John R. Goodall;Eric D. Ragan;Chad A. Steed;Joel W. Reed;G. David Richardson;Kelly M. T. Huffer;Robert A. Bridges;Jason A. Laska,John R. Goodall;Eric D. Ragan;Chad A. Steed;Joel W. Reed;G. David Richardson;Kelly M.T. Huffer;Robert A. Bridges;Jason A. Laska,Oak Ridge National Laboratory;University of Florida;Oak Ridge National Laboratory;Oak Ridge National Laboratory;Oak Ridge National Laboratory;Oak Ridge National Laboratory;Oak Ridge National Laboratory;Oak Ridge National Laboratory,10.1109/TVCG.2007.70589,"Network security,situational awareness,privacy and security,streaming data,machine learning,visualization",0.0,3.0,3.0,55.0,nan,papers/vast-tvcg/204_goodall.pdf
VAST,2018,A Visual Analytics Framework for the Detection of Anomalous Call Stack Trees in High Performance Computing Applications,10.1109/TVCG.2018.2865026,http://dx.doi.org/10.1109/TVCG.2018.2865026,215,224,J,"Anomalous runtime behavior detection is one of the most important tasks for performance diagnosis in High Performance Computing (HPC). Most of the existing methods find anomalous executions based on the properties of individual functions, such as execution time. However, it is insufficient to identify abnormal behavior without taking into account the context of the executions, such as the invocations of children functions and the communications with other HPC nodes. We improve upon the existing anomaly detection approaches by utilizing the call stack structures of the executions, which record rich temporal and contextual information. With our call stack tree (CSTree) representation of the executions, we formulate the anomaly detection problem as finding anomalous tree structures in a call stack forest. The CSTrees are converted to vector representations using our proposed stack2vec embedding. Structural and temporal visualizations of CSTrees are provided to support users in the identification and verification of the anomalies during an active anomaly detection process. Three case studies of real-world HPC applications demonstrate the capabilities of our approach.",Cong Xie;Wei Xu;Klaus Mueller,Cong Xie;Wei Xu;Klaus Mueller,Department of Computer ScienceStony Brook University;Computational Science InitiativeBrookhaven National Laboratory;Department of Computer ScienceStony Brook University,10.1109/TVCG.2015.2467552;10.1109/TVCG.2012.277;10.1109/VAST.2016.7883514;10.1109/TVCG.2016.2598664,"Call Stack,Performance Visualization,Representation Learning,Active Learning,Anomaly Detection",0.0,6.0,5.0,55.0,HM,papers/vast-tvcg/215_xie.pdf
VAST,2018,Lessons Learned Developing a Visual Analytics Solution for Investigative Analysis of Scamming Activities,10.1109/TVCG.2018.2865023,http://dx.doi.org/10.1109/TVCG.2018.2865023,225,234,J,"The forensic investigation of communication datasets which contain unstructured text, social network information, and metadata is a complex task that is becoming more important due to the immense amount of data being collected. Currently there are limited approaches that allow an investigator to explore the network, text and metadata in a unified manner. We developed Beagle as a forensic tool for email datasets that allows investigators to flexibly form complex queries in order to discover important information in email data. Beagle was successfully deployed at a security firm which had a large email dataset that was difficult to properly investigate. We discuss our experience developing Beagle as well as the lessons we learned applying visual analytic techniques to a difficult real-world problem.",Jay Koven;Cristian Felix;Hossein Siadati;Markus Jakobsson;Enrico Bertini,Jay Koven;Cristian Felix;Hossein Siadati;Markus Jakobsson;Enrico Bertini,NYU Tandon School of Engineering;NYU Tandon School of Engineering;NYU Tandon School of Engineering;Amber Solutions Inc.;NYU Tandon School of Engineering,10.1109/VAST.2007.4389011;10.1109/VAST.2010.5652968;10.1109/INFVIS.2003.1249028;10.1109/TVCG.2009.111;10.1109/TVCG.2014.2346481;10.1109/TVCG.2012.213,"Visual Analytics,Email Investigation,Email Forensics",0.0,1.0,0.0,26.0,nan,papers/vast-tvcg/225_koven.pdf
InfoVis,2018,Origin-Destination Flow Maps in Immersive Environments,10.1109/TVCG.2018.2865192,http://dx.doi.org/10.1109/TVCG.2018.2865192,693,703,J,"Immersive virtual- and augmented-reality headsets can overlay a flat image against any surface or hang virtual objects in the space around the user. The technology is rapidly improving and may, in the long term, replace traditional flat panel displays in many situations. When displays are no longer intrinsically flat, how should we use the space around the user for abstract data visualisation? In this paper, we ask this question with respect to origin-destination flow data in a global geographic context. We report on the findings of three studies exploring different spatial encodings for flow maps. The first experiment focuses on different 2D and 3D encodings for flows on flat maps. We find that participants are significantly more accurate with raised flow paths whose height is proportional to flow distance but fastest with traditional straight line 2D flows. In our second and third experiment we compared flat maps, 3D globes and a novel interactive design we call<i>MapsLink</i>, involving a pair of linked flat maps. We find that participants took significantly more time with MapsLink than other flow maps while the 3D globe with raised flows was the fastest, most accurate, and most preferred method. Our work suggests that<i>careful</i>use of the third spatial dimension can resolve visual clutter in complex flow maps.",Yalong Yang 0001;Tim Dwyer;Bernhard Jenny;Kim Marriott;Maxime Cordeil;Haohui Chen,Yalong Yang;Tim Dwyer;Bernhard Jenny;Kim Marriott;Maxime Cordeil;Haohui Chen,"Monash University;Monash University;Monash University;Monash University;Monash University;Data61, CSIRO, Australia",10.1109/TVCG.2016.2598958;10.1109/TVCG.2011.202;10.1109/TVCG.2007.70521;10.1109/INFVIS.1995.528697;10.1109/INFVIS.1996.559226;10.1109/INFVIS.2005.1532150;10.1109/TVCG.2011.181;10.1109/TVCG.2014.2346441;10.1109/INFVIS.2003.1249008;10.1109/TVCG.2016.2598885,"Origin-destination,Flow Map,Virtual Reality,Cartographic Information Visualisation,Immersive Analytics",0.0,8.0,2.0,67.0,nan,papers/infovis-tvcg/693_yang.pdf
InfoVis,2018,FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights,10.1109/TVCG.2018.2865191,http://dx.doi.org/10.1109/TVCG.2018.2865191,704,714,J,"Visualizing 3D trajectories to extract insights about their similarities and spatial configuration is a critical task in several domains. Air traffic controllers for example deal with large quantities of aircrafts routes to optimize safety in airspace and neuroscientists attempt to understand neuronal pathways in the human brain by visualizing bundles of fibers from DTI images. Extracting insights from masses of 3D trajectories is challenging as the multiple three dimensional lines have complex geometries, may overlap, cross or even merge with each other, making it impossible to follow individual ones in dense areas. As trajectories are inherently spatial and three dimensional, we propose FiberClay: a system to display and interact with 3D trajectories in immersive environments. FiberClay renders a large quantity of trajectories in real time using GP-GPU techniques. FiberClay also introduces a new set of interactive techniques for composing complex queries in 3D space leveraging immersive environment controllers and user position. These techniques enable an analyst to select and compare sets of trajectories with specific geometries and data properties. We conclude by discussing insights found using FiberClay with domain experts in air traffic control and neurology.",Christophe Hurter;Nathalie Henry Riche;Steven Mark Drucker;Maxime Cordeil;Richard Alligier;Romain Vuillemot,Christophe Hurter;Nathalie Henry Riche;Steven M. Drucker;Maxime Cordeil;Richard Alligier;Romain Vuillemot,"ENAC, The French Civil Aviation University, Toulouse University, France;Microsoft Research;Microsoft Research;Monash University;ENAC, The French Civil Aviation University, Toulouse University, France;Univ Lyon, École Centrale de Lyon, CNRS UMR52 05, LIRIS, France",10.1109/TVCG.2016.2599217;10.1109/TVCG.2011.192;10.1109/VISUAL.1991.175794;10.1109/TVCG.2008.153;10.1109/TVCG.2011.233;10.1109/TVCG.2013.226;10.1109/TVCG.2017.2744338;10.1109/TVCG.2009.145;10.1109/INFVIS.2004.27;10.1109/TVCG.2011.224;10.1109/TVCG.2015.2467112;10.1109/TVCG.2013.153;10.1109/TVCG.2012.265;10.1109/TVCG.2017.2744079;10.1109/TVCG.2012.217,"Immersive Analytics,3D Visualization,Dynamic Queries,Bimanual Interaction,Multidimensional Data",0.0,7.0,4.0,58.0,nan,papers/infovis-tvcg/704_hurter.pdf
InfoVis,2018,DXR: A Toolkit for Building Immersive Data Visualizations,10.1109/TVCG.2018.2865152,http://dx.doi.org/10.1109/TVCG.2018.2865152,715,725,J,"This paper presents DXR, a toolkit for building immersive data visualizations based on the Unity development platform. Over the past years, immersive data visualizations in augmented and virtual reality (AR, VR) have been emerging as a promising medium for data sense-making beyond the desktop. However, creating immersive visualizations remains challenging, and often require complex low-level programming and tedious manual encoding of data attributes to geometric and visual properties. These can hinder the iterative idea-to-prototype process, especially for developers without experience in 3D graphics, AR, and VR programming. With DXR, developers can efficiently specify visualization designs using a concise declarative visualization grammar inspired by Vega-Lite. DXR further provides a GUI for easy and quick edits and previews of visualization designs in-situ, i.e., while immersed in the virtual world. DXR also provides reusable templates and customizable graphical marks, enabling unique and engaging visualizations. We demonstrate the flexibility of DXR through several examples spanning a wide range of applications.",Ronell Sicat;Jiabao Li;Junyoung Choi;Maxime Cordeil;Won-Ki Jeong;Benjamin Bach;Hanspeter Pfister,Ronell Sicat;Jiabao Li;Junyoung Choi;Maxime Cordeil;Won-Ki Jeong;Benjamin Bach;Hanspeter Pfister,Harvard Visual Computing Group;Harvard Graduate School of Design;Ulsan National Institute of Science and Technology;Immersive Analytics LabMonash University;Ulsan National Institute of Science and Technology;School of InformaticsEdinburgh University;Harvard Visual Computing Group,10.1109/TVCG.2017.2745941;10.1109/TVCG.2016.2598609;10.1109/TVCG.2009.174;10.1109/TVCG.2011.185;10.1109/TVCG.2014.2346322;10.1109/TVCG.2016.2599107;10.1109/INFVIS.2004.64;10.1109/TVCG.2010.144;10.1109/TVCG.2016.2598620;10.1109/TVCG.2015.2467449;10.1109/TVCG.2014.2346318;10.1109/TVCG.2016.2599030;10.1109/TVCG.2015.2467091;10.1109/TVCG.2017.2744079;10.1109/TVCG.2016.2598608,"Augmented Reality,Virtual Reality,Immersive Visualization,Immersive Analytics,Visualization Toolkit",0.0,8.0,2.0,72.0,nan,papers/infovis-tvcg/715_sicat.pdf
InfoVis,2018,Information Olfactation: Harnessing Scent to Convey Data,10.1109/TVCG.2018.2865237,http://dx.doi.org/10.1109/TVCG.2018.2865237,726,736,J,"Olfactory feedback for analytical tasks is a virtually unexplored area in spite of the advantages it offers for information recall, feature identification, and location detection. Here we introduce the concept of information olfactation as the fragrant sibling of information visualization, and discuss how scent can be used to convey data. Building on a review of the human olfactory system and mirroring common visualization practice, we propose olfactory marks, the substrate in which they exist, and their olfactory channels that are available to designers. To exemplify this idea, we present viScent: A six-scent stereo olfactory display capable of conveying olfactory glyphs of varying temperature and direction, as well as a corresponding software system that integrates the display with a traditional visualization display. Finally, we present three applications that make use of the viScent system: A 2D graph visualization, a 2D line and point chart, and an immersive analytics graph visualization in 3D virtual reality. We close the paper with a review of possible extensions of viScent and applications of information olfactation for general visualization beyond the examples in this paper.",Biswaksen Patnaik;Andrea Batch;Niklas Elmqvist,Biswaksen Patnaik;Andrea Batch;Niklas Elmqvist,"University of Maryland, College Park, MD, USA;University of Maryland, College Park, MD, USA;University of Maryland, College Park, MD, USA",10.1109/TVCG.2016.2599107,"Olfaction,smell,scent,olfactory display,immersive analytics,immersion",0.0,2.0,1.0,104.0,nan,papers/infovis-tvcg/726_patnaik.pdf
InfoVis,2018,Dynamic Composite Data Physicalization Using Wheeled Micro-Robots,10.1109/TVCG.2018.2865159,http://dx.doi.org/10.1109/TVCG.2018.2865159,737,747,J,"This paper introduces dynamic composite physicalizations, a new class of physical visualizations that use collections of self-propelled objects to represent data. Dynamic composite physicalizations can be used both to give physical form to well-known interactive visualization techniques, and to explore new visualizations and interaction paradigms. We first propose a design space characterizing composite physicalizations based on previous work in the fields of Information Visualization and Human Computer Interaction. We illustrate dynamic composite physicalizations in two scenarios demonstrating potential benefits for collaboration and decision making, as well as new opportunities for physical interaction. We then describe our implementation using wheeled micro-robots capable of locating themselves and sensing user input, before discussing limitations and opportunities for future work.",Mathieu Le Goc;Charles Perin;Sean Follmer;Jean-Daniel Fekete;Pierre Dragicevic,Mathieu Le Goc;Charles Perin;Sean Follmer;Jean-Daniel Fekete;Pierre Dragicevic,"Stanford University, USA;University of Victoria, Canada;Stanford University, USA;INRIA, Saclay, France;INRIA, Saclay, France",10.1109/TVCG.2014.2346984;10.1109/TVCG.2014.2346424;10.1109/TVCG.2008.153;10.1109/TVCG.2007.70539;10.1109/TVCG.2014.2346292;10.1109/TVCG.2013.227;10.1109/TVCG.2013.134;10.1109/TVCG.2014.2346250;10.1109/TVCG.2017.2743859;10.1109/TVCG.2016.2598920;10.1109/TVCG.2012.199;10.1109/TVCG.2014.2346279;10.1109/TVCG.2007.70541;10.1109/TVCG.2016.2598498,"information visualization,data physicalization,tangible user interfaces",1.0,2.0,1.0,92.0,nan,papers/infovis-tvcg/737_legoc.pdf
SciVis,2018,Robust and Fast Extraction of 3D Symmetric Tensor Field Topology,10.1109/TVCG.2018.2864768,http://dx.doi.org/10.1109/TVCG.2018.2864768,1102,1111,J,"3D symmetric tensor fields appear in many science and engineering fields, and topology-driven analysis is important in many of these application domains, such as solid mechanics and fluid dynamics. Degenerate curves and neutral surfaces are important topological features in 3D symmetric tensor fields. Existing methods to extract degenerate curves and neutral surfaces often miss parts of the curves and surfaces, respectively. Moreover, these methods are computationally expensive due to the lack of knowledge of structures of degenerate curves and neutral surfaces.&lt;;/p&gt; &lt;;p&gt;In this paper, we provide theoretical analysis on the geometric and topological structures of degenerate curves and neutral surfaces of 3D linear tensor fields. These structures lead to parameterizations for degenerate curves and neutral surfaces that can not only provide more robust extraction of these features but also incur less computational cost.&lt;;/p&gt; &lt;;p&gt;We demonstrate the benefits of our approach by applying our degenerate curve and neutral surface detection techniques to solid mechanics simulation data sets.",Lawrence Roy;Prashant Kumar;Yue Zhang 0009;Eugene Zhang,Lawrence Roy;Prashant Kumar;Yue Zhang;Eugene Zhang,Oregon State University;Oregon State University;Oregon State University;Oregon State University,10.1109/TVCG.2009.184;10.1109/VISUAL.1994.346326;10.1109/TVCG.2008.148;10.1109/VISUAL.2000.885716;10.1109/VISUAL.2004.105;10.1109/VISUAL.2005.1532841,"Tensor field visualization,3D symmetric tensor fields,tensor field topology,traceless tensors,degenerate curve extraction,neutral surface extraction",0.0,1.0,1.0,24.0,nan,papers/scivis-tvcg/1102_roy.pdf
SciVis,2018,DT-MRI Streamsurfaces Revisited,10.1109/TVCG.2018.2864845,http://dx.doi.org/10.1109/TVCG.2018.2864845,1112,1121,J,"DT-MRI streamsurfaces, defined as surfaces that are everywhere tangential to the major and medium eigenvector fields, have been proposed as a tool for visualizing regions of predominantly planar behavior in diffusion tensor MRI. Even though it has long been known that their construction assumes that the involved eigenvector fields satisfy an integrability condition, it has never been tested systematically whether this condition is met in real-world data. We introduce a suitable and efficiently computable test to the visualization literature, demonstrate that it can be used to distinguish integrable from nonintegrable configurations in simulations, and apply it to whole-brain datasets of 15 healthy subjects. We conclude that streamsurface integrability is approximately satisfied in a substantial part of the brain, but not everywhere, including some regions of planarity. As a consequence, algorithms for streamsurface extraction should explicitly test local integrability. Finally, we propose a novel patch-based approach to streamsurface visualization that reduces visual artifacts, and is shown to more fully sample the extent of streamsurfaces.",Michael Ankele;Thomas Schultz 0001,Michael Ankele;Thomas Schultz,University of Bonn;University of Bonn,10.1109/TVCG.2007.70551;10.1109/VISUAL.1992.235211;10.1109/TVCG.2007.70554;10.1109/TVCG.2007.70602;10.1109/TVCG.2008.148,"Diffusion Tensor MRI,streamsurfaces,Frobenius theorem,Lie bracket",0.0,1.0,0.0,36.0,nan,papers/scivis-tvcg/1112_ankele.pdf
SciVis,2018,Tensor Field Visualization using Fiber Surfaces of Invariant Space,10.1109/TVCG.2018.2864846,http://dx.doi.org/10.1109/TVCG.2018.2864846,1122,1131,J,"Scientific visualization developed successful methods for scalar and vector fields. For tensor fields, however, effective, interactive visualizations are still missing despite progress over the last decades. We present a general approach for the generation of separating surfaces in symmetric, second-order, three-dimensional tensor fields. These surfaces are defined as fiber surfaces of the invariant space, i.e. as pre-images of surfaces in the range of a complete set of invariants. This approach leads to a generalization of the fiber surface algorithm by Klacansky et al. [16] to three dimensions in the range. This is due to the fact that the invariant space is three-dimensional for symmetric second-order tensors over a spatial domain. We present an algorithm for surface construction for simplicial grids in the domain and simplicial surfaces in the invariant space. We demonstrate our approach by applying it to stress fields from component design in mechanical engineering.",Felix Raith;Christian Blecha;Thomas Nagel;Francesco Parisio;Olaf Kolditz;Fabian Günther;Markus Stommel;Gerik Scheuermann,Felix Raith;Christian Blecha;Thomas Nagel;Francesco Parisio;Olaf Kolditz;Fabian Günther;Markus Stommel;Gerik Scheuermann,"Institute of Computer Science, Leipzig University, Leipzig, Germany;Institute of Computer Science, Leipzig University, Leipzig, Germany;Department of Environmental Informatics, Helmholtz Center for Environmental Research, Leipzig, Germany;Department of Environmental Informatics, Helmholtz Center for Environmental Research, Leipzig, Germany;Department of Environmental Informatics, Helmholtz Center for Environmental Research, Leipzig, Germany;Faculty for Mechanical Engineering, TU Dortmund University, Dortmund;Faculty for Mechanical Engineering, TU Dortmund University, Dortmund;Institute of Computer Science, Leipzig University, Leipzig, Germany",10.1109/VISUAL.1994.346326,"visualization,tensor field,invariants,fiber surface,interaction",0.0,2.0,1.0,36.0,nan,papers/scivis-tvcg/1122_raith.pdf
VAST,2018,An Interactive Method to Improve Crowdsourced Annotations,10.1109/TVCG.2018.2864843,http://dx.doi.org/10.1109/TVCG.2018.2864843,235,245,J,"In order to effectively infer correct labels from noisy crowdsourced annotations, learning-from-crowds models have introduced expert validation. However, little research has been done on facilitating the validation procedure. In this paper, we propose an interactive method to assist experts in verifying uncertain instance labels and unreliable workers. Given the instance labels and worker reliability inferred from a learning-from-crowds model, candidate instances and workers are selected for expert validation. The influence of verified results is propagated to relevant instances and workers through the learning-from-crowds model. To facilitate the validation of annotations, we have developed a confusion visualization to indicate the confusing classes for further exploration, a constrained projection method to show the uncertain labels in context, and a scatter-plot-based visualization to illustrate worker reliability. The three visualizations are tightly integrated with the learning-from-crowds model to provide an iterative and progressive environment for data validation. Two case studies were conducted that demonstrate our approach offers an efficient method for validating and improving crowdsourced annotations.",Shixia Liu;Changjian Chen;Yafeng Lu;Fang-Xin Ou-Yang;Bin Wang,Shixia Liu;Changjian Chen;Yafeng Lu;Fangxin Ouyang;Bin Wang,School of SoftwareTsinghua University;School of SoftwareTsinghua University;Arizona State University;School of SoftwareTsinghua University;School of SoftwareTsinghua University,10.1109/TVCG.2016.2598592;10.1109/VAST.2014.7042480;10.1109/TVCG.2017.2744818;10.1109/VAST.2016.7883520;10.1109/TVCG.2011.202;10.1109/TVCG.2014.2346594;10.1109/TVCG.2013.212;10.1109/TVCG.2011.239;10.1109/TVCG.2012.277;10.1109/VAST.2012.6400492;10.1109/TVCG.2016.2598445;10.1109/TVCG.2015.2467622;10.1109/TVCG.2015.2467554;10.1109/TVCG.2017.2744938;10.1109/TVCG.2016.2598831;10.1109/TVCG.2017.2744378;10.1109/VAST.2016.7883508;10.1109/TVCG.2009.139;10.1109/TVCG.2016.2598829;10.1109/TVCG.2017.2745078;10.1109/VAST.2014.7042494;10.1109/TVCG.2017.2744685;10.1109/TVCG.2013.164;10.1109/VAST.2016.7883514,"Crowdsourcing,learning-from-crowds,interactive visualization,focus + context",nan,4.0,2.0,65.0,nan,papers/vast-tvcg/235_liu.pdf
VAST,2018,RegressionExplorer: Interactive Exploration of Logistic Regression Models with Subgroup Analysis,10.1109/TVCG.2018.2865043,http://dx.doi.org/10.1109/TVCG.2018.2865043,246,255,J,"We present RegressionExplorer, a Visual Analytics tool for the interactive exploration of logistic regression models. Our application domain is Clinical Biostatistics, where models are derived from patient data with the aim to obtain clinically meaningful insights and consequences. Development and interpretation of a proper model requires domain expertise and insight into model characteristics. Because of time constraints, often a limited number of candidate models is evaluated. RegressionExplorer enables experts to quickly generate, evaluate, and compare many different models, taking the workflow for model development as starting point. Global patterns in parameter values of candidate models can be explored effectively. In addition, experts are enabled to compare candidate models across multiple subpopulations. The insights obtained can be used to formulate new hypotheses or to steer model development. The effectiveness of the tool is demonstrated for two uses cases: prediction of a cardiac conduction disorder in patients after receiving a heart valve implant and prediction of hypernatremia in critically ill patients.",Dennis Dingen;Marcel van 't Veer;Patrick Houthuizen;Eveline H. J. Mestrom;Hendrikus H. M. Korsten;Arthur R. A. Bouwman;Jarke J. van Wijk,Dennis Dingen;Marcel van't Veer;Patrick Houthuizen;Eveline H. J. Mestrom;Erik H.H.M. Korsten;Arthur R.A. Bouwman;Jarke van Wijk,Eindhoven University of Technology;Catharina Hospital Eindhoven;Catharina Hospital Eindhoven;Catharina Hospital Eindhoven;Catharina Hospital Eindhoven;Catharina Hospital Eindhoven;University of Technology,10.1109/VAST.2017.8585720;10.1109/TVCG.2015.2467325;10.1109/TVCG.2013.125;10.1109/VAST.2011.6102453;10.1109/TVCG.2015.2467931,"Visual analytics,Predictive visual analytics,Exploratory data analysis,Multivariate statistics,Regression analysis,Variable selection,Subgroup analysis",nan,2.0,1.0,32.0,nan,papers/vast-tvcg/246_dingen.pdf
VAST,2018,Drag and Track: A Direct Manipulation Interface for Contextualizing Data Instances within a Continuous Parameter Space,10.1109/TVCG.2018.2865051,http://dx.doi.org/10.1109/TVCG.2018.2865051,256,266,J,"We present a direct manipulation technique that allows material scientists to interactively highlight relevant parameterized simulation instances located in dimensionally reduced spaces, enabling a user-defined understanding of a continuous parameter space. Our goals are two-fold: first, to build a user-directed intuition of dimensionally reduced data, and second, to provide a mechanism for creatively exploring parameter relationships in parameterized simulation sets, called ensembles. We start by visualizing ensemble data instances in dimensionally reduced scatter plots. To understand these abstract views, we employ user-defined virtual data instances that, through direct manipulation, search an ensemble for similar instances. Users can create multiple of these direct manipulation queries to visually annotate the spaces with sets of highlighted ensemble data instances. User-defined goals are therefore translated into custom illustrations that are projected onto the dimensionally reduced spaces. Combined forward and inverse searches of the parameter space follow naturally allowing for continuous parameter space prediction and visual query comparison in the context of an ensemble. The potential for this visualization technique is confirmed via expert user feedback for a shock physics application and synthetic model analysis.",Daniel Orban;Daniel F. Keefe;Ayan Biswas;James P. Ahrens;David H. Rogers,Daniel Orban;Daniel F. Keefe;Ayan Biswas;James Ahrens;David Rogers,"University of Minnesota, USA;University of Minnesota, USA;Los Alamos National Labs;Los Alamos National Labs;Los Alamos National Labs",10.1109/TVCG.2013.133;10.1109/TVCG.2016.2598869;10.1109/VAST.2012.6400486;10.1109/TVCG.2010.190;10.1109/TVCG.2013.147;10.1109/VAST.2012.6400489;10.1109/TVCG.2015.2467436;10.1109/TVCG.2012.260;10.1109/VAST.2011.6102449;10.1109/TVCG.2015.2467204;10.1109/TVCG.2013.141;10.1109/TVCG.2017.2745178;10.1109/TVCG.2014.2346455;10.1109/TVCG.2016.2598589;10.1109/TVCG.2016.2598495;10.1109/TVCG.2016.2598839;10.1109/TVCG.2014.2346321;10.1109/TVCG.2011.248;10.1109/TVCG.2016.2598830,"Visual Parameter Space Analysis,Ensemble Visualization,Semantic Interaction,Direct Manipulation,Shock Physics",0.0,5.0,3.0,56.0,nan,papers/vast-tvcg/256_orban.pdf
VAST,2018,Clustrophile 2: Guided Visual Clustering Analysis,10.1109/TVCG.2018.2864477,http://dx.doi.org/10.1109/TVCG.2018.2864477,267,276,J,"Data clustering is a common unsupervised learning method frequently used in exploratory data analysis. However, identifying relevant structures in unlabeled, high-dimensional data is nontrivial, requiring iterative experimentation with clustering parameters as well as data features and instances. The number of possible clusterings for a typical dataset is vast, and navigating in this vast space is also challenging. The absence of ground-truth labels makes it impossible to define an optimal solution, thus requiring user judgment to establish what can be considered a satisfiable clustering result. Data scientists need adequate interactive tools to effectively explore and navigate the large clustering space so as to improve the effectiveness of exploratory clustering analysis. We introduce Clustrophile 2, a new interactive tool for guided clustering analysis. Clustrophile 2 guides users in clustering-based exploratory analysis, adapts user feedback to improve user guidance, facilitates the interpretation of clusters, and helps quickly reason about differences between clusterings. To this end, Clustrophile 2 contributes a novel feature, the Clustering Tour, to help users choose clustering parameters and assess the quality of different clustering results in relation to current analysis goals and user expectations. We evaluate Clustrophile 2 through a user study with 12 data scientists, who used our tool to explore and interpret sub-cohorts in a dataset of Parkinson's disease patients. Results suggest that Clustrophile 2 improves the speed and effectiveness of exploratory clustering analysis for both experts and non-experts.",Marco Cavallo;Çagatay Demiralp,Marco Cavallo;Çağatay Demiralp,IBM Research;MIT CSAIL & Fitnescity Labs,10.1109/TVCG.2011.188;10.1109/TVCG.2013.119;10.1109/TVCG.2012.219;10.1109/TVCG.2017.2745085;10.1109/TVCG.2010.138;10.1109/VAST.2007.4388999;10.1109/TVCG.2012.207;10.1109/TVCG.2017.2744805;10.1109/VAST.2008.4677350;10.1109/INFVIS.2004.3;10.1109/TVCG.2015.2467191,"Clustering tour,Guided data analysis,Unsupervised learning,Exploratory data analysis,Interactive clustering analysis,Interpretability,Explainability,Visual data exploration recommendation,Dimensionality reduction,What-if analysis,Clustrophile",0.0,11.0,3.0,46.0,nan,papers/vast-tvcg/267_cavallo.pdf
VAST,2018,InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming,10.1109/TVCG.2018.2864887,http://dx.doi.org/10.1109/TVCG.2018.2864887,277,287,J,"Prewriting is the process of generating and organizing ideas before drafting a document. Although often overlooked by novice writers and writing tool developers, prewriting is a critical process that improves the quality of a final document. To better understand current prewriting practices, we first conducted interviews with writing learners and experts. Based on the learners' needs and experts' recommendations, we then designed and developed InkPlanner, a novel pen and touch visualization tool that allows writers to utilize visual diagramming for ideation during prewriting. InkPlanner further allows writers to sort their ideas into a logical and sequential narrative by using a novel widget- NarrativeLine. Using a NarrativeLine, InkPlanner can automatically generate a document outline to guide later drafting exercises. Inkplanner is powered by machine-generated semantic and structural suggestions that are curated from various texts. To qualitatively review the tool and understand how writers use InkPlanner for prewriting, two writing experts were interviewed and a user study was conducted with university students. The results demonstrated that InkPlanner encouraged writers to generate more diverse ideas and also enabled them to think more strategically about how to organize their ideas for later drafting.",Zhicong Lu;Mingming Fan 0001;Yun Wang;Jian Zhao 0010;Michelle Annett;Daniel J. Wigdor,Zhicong Lu;Mingming Fan;Yun Wang;Jian Zhao;Michelle Annett;Daniel Wigdor,University of Toronto;University of Toronto;Hong Kong University of Science and Technology;FX Palo Alto Laboratory;MishMashMakers;University of Toronto,10.1109/TVCG.2013.191,"Writing,prewriting,diagraming,content and structure recommendation,pen and touch interfaces",0.0,2.0,2.0,60.0,nan,papers/vast-tvcg/277_lu.pdf
VAST,2018,Analyzing the Noise Robustness of Deep Neural Networks,10.1109/VAST.2018.8802509,http://dx.doi.org/10.1109/VAST.2018.8802509,60,71,C,"Deep neural networks (DNNs) are vulnerable to maliciously generated adversarial examples. These examples are intentionally designed by making imperceptible perturbations and often mislead a DNN into making an incorrect prediction. This phenomenon means that there is significant risk in applying DNNs to safety-critical applications, such as driverless cars. To address this issue, we present a visual analytics approach to explain the primary cause of the wrong predictions introduced by adversarial examples. The key is to analyze the datapaths of the adversarial examples and compare them with those of the normal examples. A datapath is a group of critical neurons and their connections. To this end, we formulate the datapath extraction as a subset selection problem and approximately solve it based on back-propagation. A multi-level visualization consisting of a segmented DAG (layer level), an Euler diagram (feature map level), and a heat map (neuron level), has been designed to help experts investigate datapaths from the high-level layers to the detailed neuron activations. Two case studies are conducted that demonstrate the promise of our approach in support of explaining the working mechanism of adversarial examples.",Mengchen Liu;Shixia Liu;Hang Su;Kelei Cao;Jun Zhu 0001,Mengchen Liu;Shixia Liu;Hang Su;Kelei Cao;Jun Zhu,"School of Software, Tsinghua University;School of Software, Tsinghua University;Dept.of Comp.Sci.Tech., Tsinghua University;School of Software, Tsinghua University;Dept.of Comp.Sci.Tech., Tsinghua University",10.1109/TVCG.2015.2467618;10.1109/TVCG.2011.186;10.1109/TVCG.2016.2598496;10.1109/TVCG.2017.2744683;10.1109/TVCG.2014.2346431;10.1109/TVCG.2014.2346433;10.1109/TVCG.2017.2744199;10.1109/TVCG.2017.2744718;10.1109/TVCG.2017.2744938;10.1109/TVCG.2016.2598831;10.1109/TVCG.2013.196;10.1109/TVCG.2011.209;10.1109/TVCG.2017.2744358;10.1109/TVCG.2016.2598838;10.1109/TVCG.2010.210;10.1109/TVCG.2017.2744018;10.1109/TVCG.2011.183;10.1109/TVCG.2017.2744158;10.1109/VISUAL.2005.1532820;10.1109/VAST.2014.7042494;10.1109/TVCG.2017.2744878;10.1109/TVCG.2018.2865041,"Deep neural networks,robustness,adversarial examples,back propagation,multi-level visualization.",0.0,3.0,0.0,0.0,nan,papers/vast-conference/liu.pdf
VAST,2018,DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks,10.1109/TVCG.2018.2864504,http://dx.doi.org/10.1109/TVCG.2018.2864504,288,298,J,"Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.",Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang,Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang,The Ohio State University;Visa Research;The Ohio State University;Visa Research,10.1109/TVCG.2017.2744683;10.1109/TVCG.2014.2346682;10.1109/TVCG.2017.2745320;10.1109/TVCG.2017.2744718;10.1109/TVCG.2011.179;10.1109/TVCG.2017.2744938;10.1109/TVCG.2016.2598831;10.1109/VAST.2017.8585721;10.1109/TVCG.2013.200;10.1109/TVCG.2017.2744358;10.1109/TVCG.2017.2744158,"Deep Q-Network (DQN),reinforcement learning,model interpretation,visual analytics",0.0,11.0,8.0,55.0,HM,papers/vast-tvcg/288_wang.pdf
VAST,2018,RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records,10.1109/TVCG.2018.2865027,http://dx.doi.org/10.1109/TVCG.2018.2865027,299,309,J,"We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.",Bum Chul Kwon;Min-Je Choi;Joanne Taery Kim;Edward Choi;Young Bin Kim;Soonwook Kwon;Jimeng Sun;Jaegul Choo,Bum Chul Kwon;Min-Je Choi;Joanne Taery Kim;Edward Choi;Young Bin Kim;Soonwook Kwon;Jimeng Sun;Jaegul Choo,"IBM T.J. Watson Research CenterKorea University;Georgia Institute of Technology;Georgia Institute of Technology;Chung-Ang University;IBM T.J. Watson Research CenterKorea University;Catholic University, Daegu;IBM T.J. Watson Research CenterKorea University;Georgia Institute of Technology",10.1109/TVCG.2013.212;10.1109/TVCG.2017.2745080;10.1109/TVCG.2012.277;10.1109/TVCG.2017.2744718;10.1109/TVCG.2017.2745085;10.1109/TVCG.2016.2598446;10.1109/TVCG.2015.2467555;10.1109/TVCG.2016.2598831;10.1109/VAST.2017.8585721;10.1109/TVCG.2017.2744358;10.1109/TVCG.2016.2598838;10.1109/TVCG.2012.213;10.1109/TVCG.2017.2744158;10.1109/TVCG.2017.2744878,"Interactive Artificial Intelligence,XAI (Explainable Artificial Intelligence),Interpretable Deep Learning,Healthcare",0.0,18.0,6.0,85.0,nan,papers/vast-tvcg/299_kwon.pdf
VAST,2018,GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation,10.1109/TVCG.2018.2864500,http://dx.doi.org/10.1109/TVCG.2018.2864500,310,320,J,"Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process's intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN's structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.",Minsuk Kahng;Nikhil Thorat;Duen Horng Chau;Fernanda B. Viégas;Martin Wattenberg,Minsuk Kahng;Nikhil Thorat;Duen Horng (Polo) Chau;Fernanda B. Viégas;Martin Wattenberg,Georgia Institute of Technology;Google Brain;Georgia Institute of Technology;Google Brain;Google Brain,10.1109/TVCG.2008.119;10.1109/TVCG.2017.2744683;10.1109/TVCG.2017.2744718;10.1109/TVCG.2017.2744938;10.1109/TVCG.2016.2598831;10.1109/TVCG.2010.177;10.1109/VAST.2017.8585721;10.1109/TVCG.2017.2744358;10.1109/TVCG.2017.2744158;10.1109/TVCG.2017.2744878,"Deep learning,information visualization,visual analytics,generative adversarial networks,machine learning,interactive experimentation,explorable explanations",1.0,16.0,6.0,44.0,nan,papers/vast-tvcg/310_kahng.pdf
InfoVis,2018,A Framework for Creative Visualization-Opportunities Workshops,10.1109/TVCG.2018.2865241,http://dx.doi.org/10.1109/TVCG.2018.2865241,748,758,J,"Applied visualization researchers often work closely with domain collaborators to explore new and useful applications of visualization. The early stages of collaborations are typically time consuming for all stakeholders as researchers piece together an understanding of domain challenges from disparate discussions and meetings. A number of recent projects, however, report on the use of creative visualization-opportunities (CVO) workshops to accelerate the early stages of applied work, eliciting a wealth of requirements in a few days of focused work. Yet, there is no established guidance for how to use such workshops effectively. In this paper, we present the results of a 2-year collaboration in which we analyzed the use of 17 workshops in 10 visualization contexts. Its primary contribution is a framework for CVO workshops that: 1) identifies a process model for using workshops; 2) describes a structure of what happens within effective workshops; 3) recommends 25 actionable guidelines for future workshops; and 4) presents an example workshop and workshop methods. The creation of this framework exemplifies the use of critical reflection to learn about visualization in practice from diverse studies and experience.",Ethan Kerzner;Sarah Goodwin;Jason Dykes;Sara Jones 0001;Miriah D. Meyer,Ethan Kerzner;Sarah Goodwin;Jason Dykes;Sara Jones;Miriah Meyer,"University of Utah;Royal Melbourne Institute of TechnologyMonash University;City, University of London;City, University of London;University of Utah",10.1109/TVCG.2010.191;10.1109/TVCG.2013.145;10.1109/TVCG.2016.2598545;10.1109/TVCG.2016.2599338;10.1109/TVCG.2011.209;10.1109/TVCG.2017.2744459;10.1109/TVCG.2014.2346331;10.1109/TVCG.2009.111;10.1109/TVCG.2015.2467271;10.1109/TVCG.2016.2599030;10.1109/TVCG.2012.213;10.1109/TVCG.2013.132;10.1109/TVCG.2015.2467191,"User-centered visualization design,design studies,creativity workshops,critically reflective practice",0.0,5.0,3.0,90.0,nan,papers/infovis-tvcg/748_kerzner.pdf
InfoVis,2018,Design Exposition with Literate Visualization,10.1109/TVCG.2018.2864836,http://dx.doi.org/10.1109/TVCG.2018.2864836,759,768,J,"We propose a new approach to the visualization design and communication process, literate visualization, based upon and extending, Donald Knuth's idea of literate programming. It integrates the process of writing data visualization code with description of the design choices that led to the implementation (design exposition). We develop a model of design exposition characterised by four visualization designer architypes: the evaluator, the autonomist, the didacticist and the rationalist. The model is used to justify the key characteristics of literate visualization: `notebook' documents that integrate live coding input, rendered output and textual narrative; low cost of authoring textual narrative; guidelines to encourage structured visualization design and its documentation. We propose narrative schemas for structuring and validating a wide range of visualization design approaches and models, and branching narratives for capturing alternative designs and design views. We describe a new open source literate visualization environment, litvis, based on a declarative interface to Vega and Vega-Lite through the functional programming language Elm combined with markdown for formatted narrative. We informally assess the approach, its implementation and potential by considering three examples spanning a range of design abstractions: new visualization idioms; validation though visualization algebra; and feminist data visualization. We argue that the rich documentation of the design process provided by literate visualization offers the potential to improve the validity of visualization design and so benefit both academic visualization and visualization practice.",Jo Wood;Alexander Kachkaev;Jason Dykes,Jo Wood;Alexander Kachkaev;Jason Dykes,giCentreCity University of London;giCentreCity University of London;giCentreCity University of London,10.1109/TVCG.2013.145;10.1109/TVCG.2014.2346325;10.1109/TVCG.2017.2744319;10.1109/TVCG.2011.209;10.1109/TVCG.2014.2346331;10.1109/TVCG.2016.2598542;10.1109/TVCG.2009.111;10.1109/TVCG.2015.2467271;10.1109/TVCG.2016.2599030;10.1109/TVCG.2012.213;10.1109/TVCG.2014.2346323,"storytelling,design,literate programming,theory",0.0,4.0,4.0,41.0,HM,papers/infovis-tvcg/759_wood.pdf
InfoVis,2018,iStoryline: Effective Convergence to Hand-drawn Storylines,10.1109/TVCG.2018.2864899,http://dx.doi.org/10.1109/TVCG.2018.2864899,769,778,J,"Storyline visualization techniques have progressed significantly to generate illustrations of complex stories automatically. However, the visual layouts of storylines are not enhanced accordingly despite the improvement in the performance and extension of its application area. Existing methods attempt to achieve several shared optimization goals, such as reducing empty space and minimizing line crossings and wiggles. However, these goals do not always produce optimal results when compared to hand-drawn storylines. We conducted a preliminary study to learn how users translate a narrative into a hand-drawn storyline and check whether the visual elements in hand-drawn illustrations can be mapped back to appropriate narrative contexts. We also compared the hand-drawn storylines with storylines generated by the state-of-the-art methods and found they have significant differences. Our findings led to a design space that summarizes (1) how artists utilize narrative elements and (2) the sequence of actions artists follow to portray expressive and attractive storylines. We developed iStoryline, an authoring tool for integrating high-level user interactions into optimization algorithms and achieving a balance between hand-drawn storylines and automatic layouts. iStoryline allows users to create novel storyline visualizations easily according to their preferences by modifying the automatically generated layouts. The effectiveness and usability of iStoryline are studied with qualitative evaluations.",Tan Tang;Sadia Rubab;Jiewen Lai;Weiwei Cui;Lingyun Yu;Yingcai Wu,Tan Tang;Sadia Rubab;Jiewen Lai;Weiwei Cui;Lingyun Yu;Yingcai Wu,"State Key Lab of CAD&CGZhejiang UniversityAlibaba-Zhejiang UniversityJoint Institute of Frontier Technologies;State Key Lab of CAD&CGZhejiang UniversityAlibaba-Zhejiang UniversityJoint Institute of Frontier Technologies;State Key Lab of CAD&CGZhejiang UniversityAlibaba-Zhejiang UniversityJoint Institute of Frontier Technologies;Microsoft Research;Bernoulli Institute, University of Groningen, Netherlands;State Key Lab of CAD&CGZhejiang UniversityAlibaba-Zhejiang UniversityJoint Institute of Frontier Technologies",10.1109/TVCG.2016.2598647;10.1109/VAST.2017.8585487;10.1109/TVCG.2017.2743990;10.1109/TVCG.2009.109;10.1109/TVCG.2015.2467531;10.1109/TVCG.2015.2467451;10.1109/TVCG.2016.2598620;10.1109/TVCG.2013.191;10.1109/TVCG.2016.2598831;10.1109/TVCG.2013.196;10.1109/TVCG.2014.2346291;10.1109/TVCG.2017.2745878;10.1109/TVCG.2012.212;10.1109/TVCG.2014.2346913,"Hand-drawn illustrations,automatic layout,design space,interactions,optimization",0.0,0.0,0.0,44.0,nan,papers/infovis-tvcg/769_tang.pdf
InfoVis,2018,Narvis: Authoring Narrative Slideshows for Introducing Data Visualization Designs,10.1109/TVCG.2018.2865232,http://dx.doi.org/10.1109/TVCG.2018.2865232,779,788,J,"Visual designs can be complex in modern data visualization systems, which poses special challenges for explaining them to the non-experts. However, few if any presentation tools are tailored for this purpose. In this study, we present Narvis, a slideshow authoring tool designed for introducing data visualizations to non-experts. Narvis targets two types of end users: teachers, experts in data visualization who produce tutorials for explaining a data visualization, and students, non-experts who try to understand visualization designs through tutorials. We present an analysis of requirements through close discussions with the two types of end users. The resulting considerations guide the design and implementation of Narvis. Additionally, to help teachers better organize their introduction slideshows, we specify a data visualization as a hierarchical combination of components, which are automatically detected and extracted by Narvis. The teachers craft an introduction slideshow through first organizing these components, and then explaining them sequentially. A series of templates are provided for adding annotations and animations to improve efficiency during the authoring process. We evaluate Narvis through a qualitative analysis of the authoring experience, and a preliminary evaluation of the generated slideshows.",Qianwen Wang;Zhen Li;Siwei Fu;Weiwei Cui;Huamin Qu,Qianwen Wang;Zhen Li;Siwei Fu;Weiwei Cui;Huamin Qu,Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Microsoft Research Asia;Hong Kong University of Science and Technology,10.1109/TVCG.2016.2598647;10.1109/TVCG.2009.174;10.1109/TVCG.2016.2598876;10.1109/TVCG.2015.2467196;10.1109/TVCG.2011.239;10.1109/VAST.2007.4388992;10.1109/TVCG.2015.2467531;10.1109/TVCG.2007.70539;10.1109/TVCG.2013.119;10.1109/TVCG.2013.191;10.1109/TVCG.2010.183,"Education,Narrative Visualization,Authoring Tools",0.0,0.0,0.0,42.0,nan,papers/infovis-tvcg/779_wang.pdf
SciVis,2018,Culling for Extreme-Scale Segmentation Volumes: A Hybrid Deterministic and Probabilistic Approach,10.1109/TVCG.2018.2864847,http://dx.doi.org/10.1109/TVCG.2018.2864847,1132,1141,J,"With the rapid increase in raw volume data sizes, such as terabyte-sized microscopy volumes, the corresponding segmentation label volumes have become extremely large as well. We focus on integer label data, whose efficient representation in memory, as well as fast random data access, pose an even greater challenge than the raw image data. Often, it is crucial to be able to rapidly identify which segments are located where, whether for empty space skipping for fast rendering, or for spatial proximity queries. We refer to this process as<i>culling</i>. In order to enable efficient culling of millions of labeled segments, we present a novel hybrid approach that combines deterministic and probabilistic representations of label data in a data-adaptive hierarchical data structure that we call the label list tree. In each node, we adaptively encode label data using either a probabilistic constant-time access representation for fast conservative culling, or a deterministic logarithmic-time access representation for exact queries. We choose the best data structures for representing the labels of each spatial region while building the label list tree. At run time, we further employ a novel<i>query-adaptive</i>culling strategy. While filtering a query down the tree, we prune it successively, and in each node adaptively select the representation that is best suited for evaluating the pruned query, depending on its size. We show an analysis of the efficiency of our approach with several large data sets from connectomics, including a brain scan with more than 13 million labeled segments, and compare our method to conventional culling approaches. Our approach achieves significant reductions in storage size as well as faster query times.",Johanna Beyer;Haneen Mohammed;Marco Agus;Ali K. Al-Awami;Hanspeter Pfister;Markus Hadwiger,Johanna Beyer;Haneen Mohammed;Marco Agus;Ali K. Al-Awami;Hanspeter Pfister;Markus Hadwiger,"Harvard University, Cambridge, MA, USA;Harvard UniversityKAUST;King Abdullah University of Science and Technology (KAUST), Saudi Arabia;KAUST, Saudi Aramco, Dhahran, Saudi Arabia;Harvard University, Cambridge, MA, USA;King Abdullah University of Science and Technology (KAUST), Saudi Arabia",10.1109/VISUAL.1992.235231;10.1109/TVCG.2013.142;10.1109/TVCG.2009.121;10.1109/VISUAL.2003.1250386;10.1109/TVCG.2012.240;10.1109/VISUAL.2003.1250384;10.1109/VISUAL.1999.809908;10.1109/VISUAL.1995.480792;10.1109/VISUAL.2005.1532792;10.1109/VISUAL.1990.146377;10.1109/VISUAL.2001.964521,"Hierarchical Culling,Segmented Volume Data,Bloom Filter,Volume Rendering,Spatial Queries",0.0,0.0,1.0,42.0,nan,papers/scivis-tvcg/1132_beyer.pdf
SciVis,2018,CPU Isosurface Ray Tracing of Adaptive Mesh Refinement Data,10.1109/TVCG.2018.2864850,http://dx.doi.org/10.1109/TVCG.2018.2864850,1142,1151,J,"Adaptive mesh refinement (AMR) is a key technology for large-scale simulations that allows for adaptively changing the simulation mesh resolution, resulting in significant computational and storage savings. However, visualizing such AMR data poses a significant challenge due to the difficulties introduced by the hierarchical representation when reconstructing continuous field values. In this paper, we detail a comprehensive solution for interactive isosurface rendering of block-structured AMR data. We contribute a novel reconstruction strategy-the octant method-which is continuous, adaptive and simple to implement. Furthermore, we present a generally applicable hybrid implicit isosurface ray-tracing method, which provides better rendering quality and performance than the built-in sampling-based approach in OSPRay. Finally, we integrate our octant method and hybrid isosurface geometry into OSPRay as a module, providing the ability to create high-quality interactive visualizations combining volume and isosurface representations of BS-AMR data. We evaluate the rendering performance, memory consumption and quality of our method on two gigascale block-structured AMR datasets.",Feng Wang;Ingo Wald;Qi Wu;William Usher;Christopher R. Johnson 0001,Feng Wang;Ingo Wald;Qi Wu;Will Usher;Chris R. Johnson,"Scientific Computing and Imaging Institute, University of Utah, Salt Lake City, UT;Intel Corporation;Scientific Computing and Imaging Institute, University of Utah, Salt Lake City, UT;Scientific Computing and Imaging Institute, University of Utah, Salt Lake City, UT;Scientific Computing and Imaging Institute, University of Utah, Salt Lake City, UT",10.1109/TVCG.2008.157;10.1109/TVCG.2009.149;10.1109/TVCG.2011.252;10.1109/VISUAL.2002.1183820;10.1109/VISUAL.1998.745713;10.1109/TVCG.2007.70566;10.1109/TVCG.2016.2599041,"AMR,Isosurface,Ray tracing,Reconstruction strategy,OSPRay",0.0,1.0,1.0,46.0,nan,papers/scivis-tvcg/1142_wang.pdf
InfoVis,2018,Charticulator: Interactive Construction of Bespoke Chart Layouts,10.1109/TVCG.2018.2865158,http://dx.doi.org/10.1109/TVCG.2018.2865158,789,799,J,"We present Charticulator, an interactive authoring tool that enables the creation of bespoke and reusable chart layouts. Charticulator is our response to most existing chart construction interfaces that require authors to choose from predefined chart layouts, thereby precluding the construction of novel charts. In contrast, Charticulator transforms a chart specification into mathematical layout constraints and automatically computes a set of layout attributes using a constraint-solving algorithm to realize the chart. It allows for the articulation of compound marks or glyphs as well as links between these glyphs, all without requiring any coding or knowledge of constraint satisfaction. Furthermore, thanks to the constraint-based layout approach, Charticulator can export chart designs into reusable templates that can be imported into other visualization tools. In addition to describing Charticulator's conceptual framework and design, we present three forms of evaluation: a gallery to illustrate its expressiveness, a user study to verify its usability, and a click-count comparison between Charticulator and three existing tools. Finally, we discuss the limitations and potentials of Charticulator as well as directions for future research. Charticulator is available with its source code at https://charticulator.com.",Donghao Ren;Bongshin Lee;Matthew Brehmer,Donghao Ren;Bongshin Lee;Matthew Brehmer,"University of California, Santa Barbara;Microsoft Research;Microsoft Research",10.1109/TVCG.2016.2598609;10.1109/TVCG.2015.2467732;10.1109/TVCG.2009.174;10.1109/TVCG.2011.185;10.1109/TVCG.2006.147;10.1109/TVCG.2016.2598620;10.1109/TVCG.2014.2346291;10.1109/TVCG.2016.2599030;10.1109/TVCG.2015.2467091;10.1109/INFVIS.2000.885086;10.1109/TVCG.2015.2467191,"Interactive visualization authoring,Chart layout design,Glyph design,Constraint-based design,Reusable chart layout",0.0,8.0,5.0,68.0,HM,papers/infovis-tvcg/789_ren.pdf
InfoVis,2018,Embedded Merge & Split: Visual Adjustment of Data Grouping,10.1109/TVCG.2018.2865075,http://dx.doi.org/10.1109/TVCG.2018.2865075,800,809,J,"Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge &amp; Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.",Ali Sarvghad;Bahador Saket;Alex Endert;Nadir Weibel,Ali Sarvghad;Bahador Saket;Alex Endert;Nadir Weibel,"University of California, San Diego;Georgia Institute of Technology;Georgia Institute of Technology;University of California, San Diego",10.1109/VAST.2012.6400486;10.1109/TVCG.2015.2467615;10.1109/TVCG.2008.109;10.1109/TVCG.2016.2598839,"Data Visualization,Direct Manipulation,Embedded Merge & Split,Data Grouping,Embedded Interaction",0.0,3.0,2.0,48.0,nan,papers/infovis-tvcg/800_sarvghad.pdf
VAST,2018,VIBR: Visualizing Bipartite Relations at Scale with the Minimum Description Length Principle,10.1109/TVCG.2018.2864826,http://dx.doi.org/10.1109/TVCG.2018.2864826,321,330,J,"Bipartite graphs model the key relations in many large scale real-world data: customers purchasing items, legislators voting for bills, people's affiliation with different social groups, faults occurring in vehicles, etc. However, it is challenging to visualize large scale bipartite graphs with tens of thousands or even more nodes or edges. In this paper, we propose a novel visual summarization technique for bipartite graphs based on the minimum description length (MDL) principle. The method simultaneously groups the two different set of nodes and constructs aggregated bipartite relations with balanced granularity and precision. It addresses the key trade-off that often occurs for visualizing large scale and noisy data: acquiring a clear and uncluttered overview while maximizing the information content in it. We formulate the visual summarization task as a co-clustering problem and propose an efficient algorithm based on locality sensitive hashing (LSH) that can easily scale to large graphs under reasonable interactive time constraints that previous related methods cannot satisfy. The method leads to the opportunity of introducing a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. In the framework, we also introduce a compact visual design inspired by adjacency list representation of graphs as the building block for a small multiples display to compare the bipartite relations for different subsets of data. We showcase the applicability and effectiveness of our approach by applying it on synthetic data with ground truth and performing case studies on real-world datasets from two application domains including roll-call vote record analysis and vehicle fault pattern analysis. Interviews with experts in the political science community and the automotive industry further highlight the benefits of our approach.",Gromit Yeuk-Yin Chan;Panpan Xu;Zeng Dai;Liu Ren,Gromit Yeuk-Yin Chan;Panpan Xu;Zeng Dai;Liu Ren,"New York University;Bosch Research North America, Sunnyvale;Bosch Research North America, Sunnyvale;Bosch Research North America, Sunnyvale",10.1109/TVCG.2010.154;10.1109/TVCG.2012.252;10.1109/TVCG.2013.223;10.1109/INFVIS.2004.1;10.1109/TVCG.2016.2598831;10.1109/TVCG.2009.111;10.1109/TVCG.2014.2346279;10.1109/TVCG.2006.166;10.1109/VAST.2007.4389006;10.1109/TVCG.2015.2467813;10.1109/TVCG.2014.2346665;10.1109/TVCG.2016.2598591,"Bipartite Graph,Visual Summarization,Minimum Description Length,Information Theory",0.0,3.0,4.0,47.0,nan,papers/vast-tvcg/321_chan.pdf
VAST,2018,Segue: Overviewing Evolution Patterns of Egocentric Networks by Interactive Construction of Spatial Layouts,10.1109/VAST.2018.8802415,http://dx.doi.org/10.1109/VAST.2018.8802415,72,83,C,"Getting the overall picture of how a large number of ego-networks evolve is a common yet challenging task. Existing techniques often require analysts to inspect the evolution patterns of ego-networks one after another. In this study, we explore an approach that allows analysts to interactively create spatial layouts in which each dot is a dynamic ego-network. These spatial layouts provide overviews of the evolution patterns of ego-networks, thereby revealing different global patterns such as trends, clusters and outliers in evolution patterns. To let analysts interactively construct interpretable spatial layouts, we propose a data transformation pipeline, with which analysts can adjust the spatial layouts and convert dynamic ego-networks into event sequences to aid interpretations of the spatial positions. Based on this transformation pipeline, we develop Segue, a visual analysis system that supports thorough exploration of the evolution patterns of ego-networks. Through two usage scenarios, we demonstrate how analysts can gain insights into the overall evolution patterns of a large collection of ego-networks by interactively creating different spatial layouts.",Po-Ming Law;Yanhong Wu;Rahul C. Basole,Po-Ming Law;Yanhong Wu;Rahul C. Basole,Georgia Institute of Technology;Visa Research;Georgia Institute of Technology,10.1109/TVCG.2015.2467851;10.1109/VAST.2012.6400486;10.1109/TVCG.2011.226;10.1109/TVCG.2011.188;10.1109/VAST.2016.7883512;10.1109/VAST.2011.6102449;10.1109/TVCG.2013.198;10.1109/TVCG.2015.2467615;10.1109/TVCG.2016.2598446;10.1109/VAST.2015.7347632;10.1109/TVCG.2016.2598797;10.1109/TVCG.2013.200;10.1109/TVCG.2017.2744198;10.1109/TVCG.2015.2468078;10.1109/VAST.2009.5332595;10.1109/TVCG.2015.2468151,"Human-centered computing,Visualization,Visualization techniques,Graph drawings",nan,0.0,27.0,0.0,nan,papers/vast-conference/law.pdf
VAST,2018,A Visual Analytics Framework for Spatiotemporal Trade Network Analysis,10.1109/TVCG.2018.2864844,http://dx.doi.org/10.1109/TVCG.2018.2864844,331,341,J,"Economic globalization is increasing connectedness among regions of the world, creating complex interdependencies within various supply chains. Recent studies have indicated that changes and disruptions within such networks can serve as indicators for increased risks of violence and armed conflicts. This is especially true of countries that may not be able to compete for scarce commodities during supply shocks. Thus, network-induced vulnerability to supply disruption is typically exported from wealthier populations to disadvantaged populations. As such, researchers and stakeholders concerned with supply chains, political science, environmental studies, etc. need tools to explore the complex dynamics within global trade networks and how the structure of these networks relates to regional instability. However, the multivariate, spatiotemporal nature of the network structure creates a bottleneck in the extraction and analysis of correlations and anomalies for exploratory data analysis and hypothesis generation. Working closely with experts in political science and sustainability, we have developed a highly coordinated, multi-view framework that utilizes anomaly detection, network analytics, and spatiotemporal visualization methods for exploring the relationship between global trade networks and regional instability. Requirements for analysis and initial research questions to be investigated are elicited from domain experts, and a variety of visual encoding techniques for rapid assessment of analysis and correlations between trade goods, network patterns, and time series signatures are explored. We demonstrate the application of our framework through case studies focusing on armed conflicts in Africa, regional instability measures, and their relationship to international global trade.",Hong Wang;Yafeng Lu;Shade T. Shutters;Michael Steptoe;Feng Wang 0012;Steven Landis;Ross Maciejewski,Hong Wang;Yafeng Lu;Shade T. Shutters;Michael Steptoe;Feng Wang;Steven Landis;Ross Maciejewski,Arizona State University;Arizona State University;Arizona State University;Arizona State University;GE Global Research;University of Nevada;Arizona State University,10.1109/VAST.2008.4677356;10.1109/TVCG.2011.202;10.1109/VAST.2012.6400557;10.1109/TVCG.2008.135;10.1109/VAST.2012.6400485;10.1109/TVCG.2014.2346682;10.1109/TVCG.2009.143;10.1109/TVCG.2014.2346271;10.1109/TVCG.2015.2467991;10.1109/VAST.2012.6400491;10.1109/INFVIS.1996.559226;10.1109/INFVIS.2005.1532150;10.1109/TVCG.2016.2598885,"Global trade network,anomaly detection,visual analytics",0.0,1.0,1.0,82.0,nan,papers/vast-tvcg/331_wang.pdf
InfoVis,2018,Mapping Color to Meaning in Colormap Data Visualizations,10.1109/TVCG.2018.2865147,http://dx.doi.org/10.1109/TVCG.2018.2865147,810,819,J,"To interpret data visualizations, people must determine how visual features map onto concepts. For example, to interpret colormaps, people must determine how dimensions of color (e.g., lightness, hue) map onto quantities of a given measure (e.g., brain activity, correlation magnitude). This process is easier when the encoded mappings in the visualization match people's predictions of how visual features will map onto concepts, their inferred mappings. To harness this principle in visualization design, it is necessary to understand what factors determine people's inferred mappings. In this study, we investigated how inferred color-quantity mappings for colormap data visualizations were influenced by the background color. Prior literature presents seemingly conflicting accounts of how the background color affects inferred color-quantity mappings. The present results help resolve those conflicts, demonstrating that sometimes the background has an effect and sometimes it does not, depending on whether the colormap appears to vary in opacity. When there is no apparent variation in opacity, participants infer that darker colors map to larger quantities (dark-is-more bias). As apparent variation in opacity increases, participants become biased toward inferring that more opaque colors map to larger quantities (opaque-is-more bias). These biases work together on light backgrounds and conflict on dark backgrounds. Under such conflicts, the opaque-is-more bias can negate, or even supersede the dark-is-more bias. The results suggest that if a design goal is to produce colormaps that match people's inferred mappings and are robust to changes in background color, it is beneficial to use colormaps that will not appear to vary in opacity on any background color, and to encode larger quantities in darker colors.",Karen B. Schloss;Connor Gramazio;Allison T. Silverman;Madeline L. Parker;Audrey S. Wang,Karen B. Schloss;Connor C. Gramazio;Allison T. Silverman;Madeline L. Parker;Audrey S. Wang,Department of Psychology and Wisconsin Institute for DiscoveryUniversity of Wisconsin-Madison;Department of Computer ScienceBrown University;School of Public HealthBrown University;Department of Psychology and Wisconsin Institute for DiscoveryUniversity of Wisconsin-Madison;Department of Applied and Computational MathematicsCalifornia Institute of Technology,10.1109/TVCG.2017.2743978;10.1109/TVCG.2016.2598918;10.1109/TVCG.2010.162;10.1109/TVCG.2007.70583;10.1109/TVCG.2017.2744359,"Visual Reasoning,Visual Communication,Colormaps,Color Perception,Visual Encoding,Visual Design",0.0,9.0,2.0,49.0,HM,papers/infovis-tvcg/810_schloss.pdf
InfoVis,2018,Optimizing Color Assignment for Perception of Class Separability in Multiclass Scatterplots,10.1109/TVCG.2018.2864912,http://dx.doi.org/10.1109/TVCG.2018.2864912,820,829,J,"Appropriate choice of colors significantly aids viewers in understanding the structures in multiclass scatterplots and becomes more important with a growing number of data points and groups. An appropriate color mapping is also an important parameter for the creation of an aesthetically pleasing scatterplot. Currently, users of visualization software routinely rely on color mappings that have been pre-defined by the software. A default color mapping, however, cannot ensure an optimal perceptual separability between groups, and sometimes may even lead to a misinterpretation of the data. In this paper, we present an effective approach for color assignment based on a set of given colors that is designed to optimize the perception of scatterplots. Our approach takes into account the spatial relationships, density, degree of overlap between point clusters, and also the background color. For this purpose, we use a genetic algorithm that is able to efficiently find good color assignments. We implemented an interactive color assignment system with three extensions of the basic method that incorporates top K suggestions, user-defined color subsets, and classes of interest for the optimization. To demonstrate the effectiveness of our assignment technique, we conducted a numerical study and a controlled user study to compare our approach with default color assignments; our findings were verified by two expert studies. The results show that our approach is able to support users in distinguishing cluster numbers faster and more precisely than default assignment methods.",Yunhai Wang;Xin Chen;Tong Ge;Chen Bao;Michael Sedlmair;Chi-Wing Fu;Oliver Deussen;Baoquan Chen,Yunhai Wang;Xin Chen;Tong Ge;Chen Bao;Michael Sedlmair;Chi-Wing Fu;Oliver Deussen;Baoquan Chen,"Shandong University;Shandong University;Shandong University;Shandong University;VISUS, University of Stuttgart, Germany;Chinese University, Hong Kong;Konstanz University, Germany;Peking University",10.1109/VISUAL.1995.480803;10.1109/TVCG.2016.2599214;10.1109/TVCG.2013.183;10.1109/TVCG.2016.2598918;10.1109/VISUAL.1996.568118;10.1109/TVCG.2017.2744184;10.1109/TVCG.2013.153;10.1109/TVCG.2015.2467471;10.1109/TVCG.2017.2744359;10.1109/VAST.2009.5332628;10.1109/TVCG.2008.118,"Color perception,visual design,scatterplots",0.0,1.0,5.0,50.0,nan,papers/infovis-tvcg/820_wang.pdf
InfoVis,2018,Looks Good To Me: Visualizations As Sanity Checks,10.1109/TVCG.2018.2864907,http://dx.doi.org/10.1109/TVCG.2018.2864907,830,839,J,"Famous examples such as Anscombe's Quartet highlight that one of the core benefits of visualizations is allowing people to discover visual patterns that might otherwise be hidden by summary statistics. This visual inspection is particularly important in exploratory data analysis, where analysts can use visualizations such as histograms and dot plots to identify data quality issues. Yet, these visualizations are driven by parameters such as histogram bin size or mark opacity that have a great deal of impact on the final visual appearance of the chart, but are rarely optimized to make important features visible. In this paper, we show that data flaws have varying impact on the visual features of visualizations, and that the adversarial or merely uncritical setting of design parameters of visualizations can obscure the visual signatures of these flaws. Drawing on the framework of Algebraic Visualization Design, we present the results of a crowdsourced study showing that common visualization types can appear to reasonably summarize distributional data while hiding large and important flaws such as missing data and extraneous modes. We make use of these results to propose additional best practices for visualizations of distributions for data quality tasks.",Michael Correll;Mingwei Li;Gordon L. Kindlmann;Carlos Scheidegger,Michael Correll;Mingwei Li;Gordon Kindlmann;Carlos Scheidegger,Tableau Research;University of Arizona;University of Chicago;University of Arizona,10.1109/TVCG.2016.2598862;10.1109/TVCG.2011.185;10.1109/TVCG.2014.2346298;10.1109/VAST.2016.7883519;10.1109/TVCG.2016.2598618;10.1109/TVCG.2014.2346978;10.1109/TVCG.2014.2346979;10.1109/TVCG.2012.230;10.1109/TVCG.2014.2346325;10.1109/TVCG.2016.2599030;10.1109/TVCG.2017.2744359;10.1109/TVCG.2015.2469125;10.1109/TVCG.2010.161;10.1109/TVCG.2015.2467191,"Graphical perception,data quality,univariate visualizations",0.0,2.0,2.0,51.0,nan,papers/infovis-tvcg/830_correll.pdf
InfoVis,2018,Image-Based Aspect Ratio Selection,10.1109/TVCG.2018.2865266,http://dx.doi.org/10.1109/TVCG.2018.2865266,840,849,J,"Selecting a good aspect ratio is crucial for effective 2D diagrams. There are several aspect ratio selection methods for function plots and line charts, but only few can handle general, discrete diagrams such as 2D scatter plots. However, these methods either lack a perceptual foundation or heavily rely on intermediate isoline representations, which depend on choosing the right isovalues and are time-consuming to compute. This paper introduces a general image-based approach for selecting aspect ratios for a wide variety of 2D diagrams, ranging from scatter plots and density function plots to line charts. Our approach is derived from Federer's co-area formula and a line integral representation that enable us to directly construct image-based versions of existing selection methods using density fields. In contrast to previous methods, our approach bypasses isoline computation, so it is faster to compute, while following the perceptual foundation to select aspect ratios. Furthermore, this approach is complemented by an anisotropic kernel density estimation to construct density fields, allowing us to more faithfully characterize data patterns, such as the subgroups in scatterplots or dense regions in time series. We demonstrate the effectiveness of our approach by quantitatively comparing to previous methods and revisiting a prior user study. Finally, we present extensions for ROI banking, multi-scale banking, and the application to image data.",Yunhai Wang;Zeyu Wang 0005;Chi-Wing Fu;Hansjörg Schmauder;Oliver Deussen;Daniel Weiskopf,Yunhai Wang;Zeyu Wang;Chi-Wing Fu;Hansjörq Schmauder;Oliver Deussen;Daniel Weiskopf,"Shandong University;SIAT, Shenzhen VisuCA Key Lab, China;Chinese University of Hong Kong;VISUS, University of Stuttgart, Germany;Konstanz University, Germany;VISUS, University of Stuttgart, Germany",10.1109/TVCG.2008.119;10.1109/TVCG.2006.168;10.1109/TVCG.2013.187;10.1109/TVCG.2009.131;10.1109/TVCG.2010.146;10.1109/TVCG.2017.2744184;10.1109/TVCG.2006.168;10.1109/TVCG.2011.167;10.1109/TVCG.2012.196;10.1109/VAST.2009.5332628;10.1109/INFVIS.2005.1532142,"Aspect ratio,image-based method,Federer's co-area formula,density field,anisotropic kernel density estimation",0.0,1.0,1.0,42.0,nan,papers/infovis-tvcg/840_wang.pdf
SciVis,2018,Persistence Atlas for Critical Point Variability in Ensembles,10.1109/TVCG.2018.2864432,http://dx.doi.org/10.1109/TVCG.2018.2864432,1152,1162,J,"This paper presents a new approach for the visualization and analysis of the spatial variability of features of interest represented by critical points in ensemble data. Our framework, called Persistence Atlas, enables the visualization of the dominant spatial patterns of critical points, along with statistics regarding their occurrence in the ensemble. The persistence atlas represents in the geometrical domain each dominant pattern in the form of a confidence map for the appearance of critical points. As a by-product, our method also provides 2-dimensional layouts of the entire ensemble, highlighting the main trends at a global level. Our approach is based on the new notion of Persistence Map, a measure of the geometrical density in critical points which leverages the robustness to noise of topological persistence to better emphasize salient features. We show how to leverage spectral embedding to represent the ensemble members as points in a low-dimensional Euclidean space, where distances between points measure the dissimilarities between critical point layouts and where statistical tasks, such as clustering, can be easily carried out. Further, we show how the notion of mandatory critical point can be leveraged to evaluate for each cluster confidence regions for the appearance of critical points. Most of the steps of this framework can be trivially parallelized and we show how to efficiently implement them. Extensive experiments demonstrate the relevance of our approach. The accuracy of the confidence regions provided by the persistence atlas is quantitatively evaluated and compared to a baseline strategy using an off-the-shelf clustering approach. We illustrate the importance of the persistence atlas in a variety of real-life datasets, where clear trends in feature layouts are identified and analyzed. We provide a lightweight VTK-based C++ implementation of our approach that can be used for reproduction purposes.",Guillaume Favelier;Noura Faraj;Brian Summa;Julien Tierny,Guillaume Favelier;Noura Faraj;Brian Summa;Julien Tierny,Sorbonne UniversitéCNRS (LIP6);Tulane University;Tulane University;Sorbonne UniversitéCNRS (LIP6),10.1109/TVCG.2013.208;10.1109/TVCG.2015.2467958;10.1109/TVCG.2015.2467204;10.1109/TVCG.2014.2346403;10.1109/TVCG.2008.110;10.1109/TVCG.2015.2467432;10.1109/TVCG.2013.141;10.1109/TVCG.2011.249;10.1109/TVCG.2006.186;10.1109/TVCG.2014.2346455;10.1109/TVCG.2015.2467754;10.1109/TVCG.2010.181;10.1109/VISUAL.1999.809897;10.1109/TVCG.2012.249;10.1109/TVCG.2014.2346332;10.1109/TVCG.2013.143,"Topological data analysis,scalar data,ensemble data",0.0,0.0,2.0,87.0,nan,papers/scivis-tvcg/1152_favelier.pdf
SciVis,2018,Probabilistic Asymptotic Decider for Topological Ambiguity Resolution in Level-Set Extraction for Uncertain 2D Data,10.1109/TVCG.2018.2864505,http://dx.doi.org/10.1109/TVCG.2018.2864505,1163,1172,J,"We present a framework for the analysis of uncertainty in isocontour extraction. The marching squares (MS) algorithm for isocontour reconstruction generates a linear topology that is consistent with hyperbolic curves of a piecewise bilinear interpolation. The saddle points of the bilinear interpolant cause topological ambiguity in isocontour extraction. The midpoint decider and the asymptotic decider are well-known mathematical techniques for resolving topological ambiguities. The latter technique investigates the data values at the cell saddle points for ambiguity resolution. The uncertainty in data, however, leads to uncertainty in underlying bilinear interpolation functions for the MS algorithm, and hence, their saddle points. In our work, we study the behavior of the asymptotic decider when data at grid vertices is uncertain. First, we derive closed-form distributions characterizing variations in the saddle point values for uncertain bilinear interpolants. The derivation assumes uniform and nonparametric noise models, and it exploits the concept of ratio distribution for analytic formulations. Next, the probabilistic asymptotic decider is devised for ambiguity resolution in uncertain data using distributions of the saddle point values derived in the first step. Finally, the confidence in probabilistic topological decisions is visualized using a colormapping technique. We demonstrate the higher accuracy and stability of the probabilistic asymptotic decider in uncertain data with regard to existing decision frameworks, such as deciders in the mean field and the probabilistic midpoint decider, through the isocontour visualization of synthetic and real datasets.",Tushar M. Athawale;Christopher R. Johnson 0001,Tushar Athawale;Chris R. Johnson,Scientific Computing & Imaging (SCI) InstituteUniversity of Utah;Scientific Computing & Imaging (SCI) InstituteUniversity of Utah,10.1109/TVCG.2013.208;10.1109/TVCG.2015.2467958;10.1109/VISUAL.2002.1183769;10.1109/TVCG.2017.2744099;10.1109/TVCG.2011.203;10.1109/TVCG.2007.70518;10.1109/VISUAL.1991.175782;10.1109/TVCG.2013.143,"Isocontour visualization,topological uncertainty,marching squares,asymptotic decider,bilinear interpolation,probabilistic computation",nan,2.0,1.0,40.0,nan,papers/scivis-tvcg/1163_athawale.pdf
SciVis,2018,Hexahedral Mesh Structure Visualization and Evaluation,10.1109/TVCG.2018.2864827,http://dx.doi.org/10.1109/TVCG.2018.2864827,1173,1182,J,"Understanding hexahedral (hex-) mesh structures is important for a number of hex-mesh generation and optimization tasks. However, due to various configurations of the singularities in a valid pure hex-mesh, the structure (or base complex) of the mesh can be arbitrarily complex. In this work, we present a first and effective method to help meshing practitioners understand the possible configurations in a valid 3D base complex for the characterization of their complexity. In particular, we propose a strategy to decompose the complex hex-mesh structure into multi-level sub-structures so that they can be studied separately, from which we identify a small set of the sub-structures that can most efficiently represent the whole mesh structure. Furthermore, from this set of sub-structures, we attempt to define the first metric for the quantification of the complexity of hex-mesh structure. To aid the exploration of the extracted multi-level structure information, we devise a visual exploration system coupled with a matrix view to help alleviate the common challenge of 3D data exploration (e.g., clutter and occlusion). We have applied our tool and metric to a large number of hex-meshes generated with different approaches to reveal different characteristics of these methods in terms of the mesh structures they can produce. We also use our metric to assess the existing structure simplification techniques in terms of their effectiveness.",Kaoji Xu;Guoning Chen,Kaoji Xu;Guoning Chen,University of Houston;University of Houston,nan,"hexahedral mesh,base complex,sheet decomposition,complexity analysis",0.0,1.0,0.0,42.0,nan,papers/scivis-tvcg/1173_xu.pdf
SciVis,2018,Shared-Memory Parallel Computation of Morse-Smale Complexes with Improved Accuracy,10.1109/TVCG.2018.2864848,http://dx.doi.org/10.1109/TVCG.2018.2864848,1183,1192,J,"Topological techniques have proven to be a powerful tool in the analysis and visualization of large-scale scientific data. In particular, the Morse-Smale complex and its various components provide a rich framework for robust feature definition and computation. Consequently, there now exist a number of approaches to compute Morse-Smale complexes for large-scale data in parallel. However, existing techniques are based on discrete concepts which produce the correct topological structure but are known to introduce grid artifacts in the resulting geometry. Here, we present a new approach that combines parallel streamline computation with combinatorial methods to construct a high-quality discrete Morse-Smale complex. In addition to being invariant to the orientation of the underlying grid, this algorithm allows users to selectively build a subset of features using high-quality geometry. In particular, a user may specifically select which ascending/descending manifolds are reconstructed with improved accuracy, focusing computational effort where it matters for subsequent analysis. This approach computes Morse-Smale complexes for larger data than previously feasible with significant speedups. We demonstrate and validate our approach using several examples from a variety of different scientific domains, and evaluate the performance of our method.",Attila Gyulassy;Peer-Timo Bremer;Valerio Pascucci,Attila Gyulassy;Peer-Timo Bremer;Valerio Pascucci,SCI InstituteUniversity of Utah;Lawrence Livermore National Lab;SCI InstituteUniversity of Utah,10.1109/TVCG.2012.209;10.1109/TVCG.2008.110;10.1109/TVCG.2007.70603;10.1109/TVCG.2014.2346434;10.1109/TVCG.2015.2467432;10.1109/VISUAL.2005.1532839;10.1109/TVCG.2011.249;10.1109/TVCG.2015.2467449;10.1109/TVCG.2006.186,"Morse complex,Parallel Computation,Topology,Accurate Geometry",nan,3.0,2.0,36.0,nan,papers/scivis-tvcg/1183_gyulassy.pdf
SciVis,2018,A Study of the Trade-off Between Reducing Precision and Reducing Resolution for Data Analysis and Visualization,10.1109/TVCG.2018.2864853,http://dx.doi.org/10.1109/TVCG.2018.2864853,1193,1203,J,"There currently exist two dominant strategies to reduce data sizes in analysis and visualization: reducing the precision of the data, e.g., through quantization, or reducing its resolution, e.g., by subsampling. Both have advantages and disadvantages and both face fundamental limits at which the reduced information ceases to be useful. The paper explores the additional gains that could be achieved by combining both strategies. In particular, we present a common framework that allows us to study the trade-off in reducing precision and/or resolution in a principled manner. We represent data reduction schemes as progressive streams of bits and study how various bit orderings such as by resolution, by precision, etc., impact the resulting approximation error across a variety of data sets as well as analysis tasks. Furthermore, we compute streams that are optimized for different tasks to serve as lower bounds on the achievable error. Scientific data management systems can use the results presented in this paper as guidance on how to store and stream data to make efficient use of the limited storage and bandwidth in practice.",Duong Hoang;Pavol Klacansky;Harsh Bhatia;Peer-Timo Bremer;Peter Lindstrom;Valerio Pascucci,Duong Hoang;Pavol Klacansky;Harsh Bhatia;Peer-Timo Bremer;Peter Lindstrom;Valerio Pascucci,"SCI Institute, University of Utah, USA;SCI Institute, University of Utah, USA;Lawrence Livemore National Laboratory, USA;Lawrence Livemore National Laboratory, USA;Lawrence Livemore National Laboratory, USA;SCI Institute, University of Utah, USA",10.1109/TVCG.2009.194;10.1109/TVCG.2007.70516;10.1109/VISUAL.2002.1183757;10.1109/TVCG.2012.240;10.1109/VISUAL.1999.809908;10.1109/TVCG.2014.2346458;10.1109/TVCG.2006.143;10.1109/VISUAL.2004.51;10.1109/VISUAL.2003.1250385;10.1109/TVCG.2011.214;10.1109/TVCG.2012.274;10.1109/TVCG.2015.2467412,"data compression,bit ordering,multi-resolution,data analysis",nan,1.0,0.0,70.0,nan,papers/scivis-tvcg/1193_hoang.pdf
VAST,2018,RuleMatrix: Visualizing and Understanding Classifiers with Rules,10.1109/TVCG.2018.2864812,http://dx.doi.org/10.1109/TVCG.2018.2864812,342,352,J,"With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.",Yao Ming;Huamin Qu;Enrico Bertini,Yao Ming;Huamin Qu;Enrico Bertini,University of Science and Technology;University of Science and Technology;New York University,10.1109/TVCG.2017.2744683;10.1109/TVCG.2017.2744718;10.1109/VAST.2017.8585720;10.1109/TVCG.2016.2598831;10.1109/VAST.2017.8585721;10.1109/TVCG.2017.2744358;10.1109/TVCG.2016.2598838;10.1109/TVCG.2016.2598828;10.1109/TVCG.2017.2744158;10.1109/VISUAL.2005.1532820;10.1109/VAST.2011.6102453;10.1109/TVCG.2017.2744878,"explainable machine learning,rule visualization,visual analytics",0.0,1.0,7.0,48.0,nan,papers/vast-tvcg/342_ming.pdf
VAST,2018,Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models,10.1109/TVCG.2018.2865044,http://dx.doi.org/10.1109/TVCG.2018.2865044,353,363,J,"Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and “what if”-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.",Hendrik Strobelt;Sebastian Gehrmann;Michael Behrisch 0001;Adam Perer;Hanspeter Pfister;Alexander M. Rush,Hendrik Strobelt;Sebastian Gehrmann;Michael Behrisch;Adam Perer;Hanspeter Pfister;Alexander M. Rush,IBM ReseatchMIT-IBM Watson AI Lab.;Harvard NLP group;Hatvatd Visual Computing group;IBM ReseatchMIT-IBM Watson AI Lab.;Hatvatd Visual Computing group;Harvard NLP group,10.1109/TVCG.2017.2744718;10.1109/TVCG.2017.2744478;10.1109/TVCG.2017.2744158,"Explainable AI,Visual Debugging,Visual Analytics,Machine Learning,Deep Learning,NLP",1.0,8.0,12.0,55.0,HM,papers/vast-tvcg/353_strobelt.pdf
VAST,2018,Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models,10.1109/TVCG.2018.2864499,http://dx.doi.org/10.1109/TVCG.2018.2864499,364,373,J,"Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.",Jiawei Zhang;Yang Wang;Piero Molino;Lezhi Li;David S. Ebert,Jiawei Zhang;Yang Wang;Piero Molino;Lezhi Li;David S. Ebert,"Purdue University;Uber Technologies, Inc;Uber AI Labs;Uber Technologies, Inc;Purdue University",10.1109/TVCG.2014.2346660;10.1109/VAST.2015.7347637;10.1109/TVCG.2014.2346594;10.1109/TVCG.2013.212;10.1109/TVCG.2017.2744718;10.1109/TVCG.2017.2744938;10.1109/TVCG.2017.2744378;10.1109/TVCG.2013.125;10.1109/TVCG.2014.2346578;10.1109/TVCG.2009.111;10.1109/TVCG.2016.2598838;10.1109/TVCG.2016.2598828;10.1109/INFVIS.2000.885086;10.1109/TVCG.2017.2744158;10.1109/TVCG.2016.2598829;10.1109/TVCG.2017.2744878,"Interactive machine learning,performance analysis,model comparison,model debugging",0.0,14.0,9.0,43.0,nan,papers/vast-tvcg/364_zhang.pdf
VAST,2018,Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution,10.1109/TVCG.2018.2864769,http://dx.doi.org/10.1109/TVCG.2018.2864769,374,384,J,"To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.",Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel A. Keim;Christopher Collins 0001,Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel Keim;Christopher Collins,"University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Ontario Institute of Technology, Canada",10.1109/VAST.2014.7042493;10.1109/VAST.2011.6102461;10.1109/TVCG.2013.162;10.1109/TVCG.2017.2745080;10.1109/TVCG.2017.2744199;10.1109/TVCG.2017.2743959;10.1109/TVCG.2013.231;10.1109/TVCG.2013.212;10.1109/TVCG.2016.2598445;10.1109/TVCG.2014.2346578;10.1109/TVCG.2013.232,"User-Steerable Topic Modeling,Speculative Execution,Mixed-Initiative Visual Analytics,Explainable Machine Learning",0.0,8.0,4.0,69.0,nan,papers/vast-tvcg/374_el-assady.pdf
VAST,2018,VIS4ML: An Ontology for Visual Analytics Assisted Machine Learning,10.1109/TVCG.2018.2864838,http://dx.doi.org/10.1109/TVCG.2018.2864838,385,395,J,"While many VA workflows make use of machine-learned models to support analytical tasks, VA workflows have become increasingly important in understanding and improving Machine Learning (ML) processes. In this paper, we propose an ontology (VIS4ML) for a subarea of VA, namely “VA-assisted ML”. The purpose of VIS4ML is to describe and understand existing VA workflows used in ML as well as to detect gaps in ML processes and the potential of introducing advanced VA techniques to such processes. Ontologies have been widely used to map out the scope of a topic in biology, medicine, and many other disciplines. We adopt the scholarly methodologies for constructing VIS4ML, including the specification, conceptualization, formalization, implementation, and validation of ontologies. In particular, we reinterpret the traditional VA pipeline to encompass model-development workflows. We introduce necessary definitions, rules, syntaxes, and visual notations for formulating VIS4ML and make use of semantic web technologies for implementing it in the Web Ontology Language (OWL). VIS4ML captures the high-level knowledge about previous workflows where VA is used to assist in ML. It is consistent with the established VA concepts and will continue to evolve along with the future developments in VA and ML. While this ontology is an effort for building the theoretical foundation of VA, it can be used by practitioners in real-world applications to optimize model-development workflows by systematically examining the potential benefits that can be brought about by either machine or human capabilities. Meanwhile, VIS4ML is intended to be extensible and will continue to be updated to reflect future advancements in using VA for building high-quality data-analytical models or for building such models rapidly.",Dominik Sacha;Matthias Kraus;Daniel A. Keim;Min Chen 0001,Dominik Sacha;Matthias Kraus;Daniel A. Keim;Min Chen,University of Konstanz;University of Konstanz;University of Konstanz;University of Oxford,10.1109/TVCG.2017.2744683;10.1109/VISUAL.2004.10;10.1109/VAST.2008.4677361;10.1109/TVCG.2017.2744718;10.1109/TVCG.2017.2745178;10.1109/TVCG.2017.2745085;10.1109/TVCG.2017.2744938;10.1109/TVCG.2016.2598831;10.1109/TVCG.2017.2744378;10.1109/TVCG.2017.2745158;10.1109/TVCG.2017.2744358;10.1109/TVCG.2016.2598838;10.1109/TVCG.2016.2598828;10.1109/TVCG.2017.2744805;10.1109/TVCG.2014.2346481;10.1109/TVCG.2016.2598495;10.1109/TVCG.2017.2744158;10.1109/TVCG.2016.2598829;10.1109/VISUAL.2005.1532781;10.1109/TVCG.2016.2598830;10.1109/TVCG.2017.2744878,"Visual Analytics,Visualization,Machine Learning,Human-Computer Interaction,Ontology,VIS4ML",0.0,12.0,5.0,71.0,nan,papers/vast-tvcg/385_sacha.pdf
InfoVis,2018,Mitigating the Attraction Effect with Visualizations,10.1109/TVCG.2018.2865233,http://dx.doi.org/10.1109/TVCG.2018.2865233,850,860,J,"Human decisions are prone to biases, and this is no less true for decisions made within data visualizations. Bias mitigation strategies often focus on the person, by educating people about their biases, typically with little success. We focus instead on the system, presenting the first evidence that altering the design of an interactive visualization tool can mitigate a strong bias - the attraction effect. Participants viewed 2D scatterplots where choices between superior alternatives were affected by the placement of other suboptimal points. We found that highlighting the superior alternatives weakened the bias, but did not eliminate it. We then tested an interactive approach where participants completely removed locally dominated points from the view, inspired by the elimination by aspects strategy in the decision-making literature. This approach strongly decreased the bias, leading to a counterintuitive suggestion: tools that allow removing inappropriately salient or distracting data from a view may help lead users to make more rational decisions.",Evanthia Dimara;Gilles Bailly;Anastasia Bezerianos;Steven Franconeri,Evanthia Dimara;Gilles Bailly;Anastasia Bezerianos;Steven Franconeri,ISIR;ISIR;Univ. Paris-Sud & CNRS (LRI)Inria;Northwestern Univ.,10.1109/INFVIS.2005.1532136;10.1109/VAST.2017.8585665;10.1109/INFVIS.1996.559213;10.1109/TVCG.2016.2598594;10.1109/TVCG.2017.2745138;10.1109/TVCG.2013.173;10.1109/VAST.2010.5652880;10.1109/TVCG.2012.199;10.1109/TVCG.2015.2467758;10.1109/TVCG.2016.2598589;10.1109/VISUAL.1992.235203;10.1109/TVCG.2017.2744138;10.1109/VAST.2017.8585669;10.1109/VISUAL.1990.146375;10.1109/TVCG.2010.161;10.1109/TVCG.2007.70515,"Decision making,cognitive bias,bias alleviation,bias mitigation,debiasing,information visualization,attraction effect",0.0,4.0,4.0,98.0,nan,papers/infovis-tvcg/850_dimara.pdf
InfoVis,2018,Face to Face: Evaluating Visual Comparison,10.1109/TVCG.2018.2864884,http://dx.doi.org/10.1109/TVCG.2018.2864884,861,871,J,"Data are often viewed as a single set of values, but those values frequently must be compared with another set. The existing evaluations of designs that facilitate these comparisons tend to be based on intuitive reasoning, rather than quantifiable measures. We build on this work with a series of crowdsourced experiments that use low-level perceptual comparison tasks that arise frequently in comparisons within data visualizations (e.g., which value changes the most between the two sets of data?). Participants completed these tasks across a variety of layouts: overlaid, two arrangements of juxtaposed small multiples, mirror-symmetric small multiples, and animated transitions. A staircase procedure sought the difficulty level (e.g., value change delta) that led to equivalent accuracy for each layout. Confirming prior intuition, we observe high levels of performance for overlaid versus standard small multiples. However, we also find performance improvements for both mirror symmetric small multiples and animated transitions. While some results are incongruent with common wisdom in data visualization, they align with previous work in perceptual psychology, and thus have potentially strong implications for visual comparison designs.",Brian D. Ondov;Nicole Jardine;Niklas Elmqvist;Steven Franconeri,Brian Ondov;Nicole Jardine;Niklas Elmqvist;Steven Franconeri,"National Institutes of Health, MD, USA;Northwestern University, Evanston, IL, USA;University of Maryland, College Park, MD, USA;Northwestern University, Evanston, IL, USA",10.1109/TVCG.2017.2744359;10.1109/INFVIS.2005.1532136;10.1109/TVCG.2011.185;10.1109/TVCG.2015.2466971;10.1109/TVCG.2014.2346424;10.1109/TVCG.2017.2744199;10.1109/TVCG.2014.2346979;10.1109/TVCG.2007.70539;10.1109/TVCG.2010.162;10.1109/TVCG.2017.2744198;10.1109/TVCG.2008.125;10.1109/TVCG.2017.2745140;10.1109/INFVIS.2000.885091,"Graphical perception,visual perception,visual comparison,crowdsourced evaluation",0.0,4.0,4.0,70.0,nan,papers/infovis-tvcg/861_ondov.pdf
InfoVis,2018,At a Glance: Pixel Approximate Entropy as a Measure of Line Chart Complexity,10.1109/TVCG.2018.2865264,http://dx.doi.org/10.1109/TVCG.2018.2865264,872,881,J,"When inspecting information visualizations under time critical settings, such as emergency response or monitoring the heart rate in a surgery room, the user only has a small amount of time to view the visualization “at a glance”. In these settings, it is important to provide a quantitative measure of the visualization to understand whether or not the visualization is too “complex” to accurately judge at a glance. This paper proposes Pixel Approximate Entropy (PAE), which adapts the approximate entropy statistical measure commonly used to quantify regularity and unpredictability in time-series data, as a measure of visual complexity for line charts. We show that PAE is correlated with user-perceived chart complexity, and that increased chart PAE correlates with reduced judgement accuracy. `We also find that the correlation between PAE values and participants' judgment increases when the user has less time to examine the line charts.",Gabriel Ryan;Abigail Mosca;Remco Chang;Eugene Wu 0002,Gabriel Ryan;Abigail Mosca;Remco Chang;Eugene Wu,Columbia University;Tufts University;Tufts University;Columbia University,10.1109/TVCG.2011.229;10.1109/TVCG.2013.133;10.1109/TVCG.2010.131;10.1109/TVCG.2010.184;10.1109/VAST.2010.5653598;10.1109/TVCG.2007.70594;10.1109/INFVIS.2004.15;10.1109/VAST.2006.261423;10.1109/TVCG.2008.140;10.1109/TVCG.2010.161;10.1109/INFVIS.2005.1532142,"Visualization,Graphical Perception,Entropy,At-a-glance",nan,4.0,1.0,69.0,nan,papers/infovis-tvcg/872_wu.pdf
SciVis,2018,Firefly: Virtual Illumination Drones for Interactive Visualization,10.1109/TVCG.2018.2864656,http://dx.doi.org/10.1109/TVCG.2018.2864656,1204,1213,J,"Light specification in three dimensional scenes is a complex problem and several approaches have been presented that aim to automate this process. However, there are many scenarios where a static light setup is insufficient, as the scene content and camera position may change. Simultaneous manual control over the camera and light position imposes a high cognitive load on the user. To address this challenge, we introduce a novel approach for automatic scene illumination with Fireflies. Fireflies are intelligent virtual light drones that illuminate the scene by traveling on a closed path. The Firefly path automatically adapts to changes in the scene based on an outcome-oriented energy function. To achieve interactive performance, we employ a parallel rendering pipeline for the light path evaluations. We provide a catalog of energy functions for various application scenarios and discuss the applicability of our method on several examples.",Sergej Stoppel;Magnus Paulson Erga;Stefan Bruckner,Sergej Stoppel;Magnus Paulson Erga;Stefan Bruckner,"University of Bergen, Norway;University of Bergen, Norway;University of Bergen, Norway",10.1109/TVCG.2012.203;10.1109/TVCG.2013.156;10.1109/VISUAL.2002.1183785;10.1109/VISUAL.2003.1250395;10.1109/VISUAL.2004.62;10.1109/TVCG.2011.173;10.1109/TVCG.2013.172,"Dynamic lighting design,lighting drones",0.0,1.0,1.0,47.0,HM,papers/scivis-tvcg/1204_stoppel.pdf
SciVis,2018,CoDDA: A Flexible Copula-based Distribution Driven Analysis Framework for Large-Scale Multivariate Data,10.1109/TVCG.2018.2864801,http://dx.doi.org/10.1109/TVCG.2018.2864801,1214,1224,J,"CoDDA (Copula-based Distribution Driven Analysis) is a flexible framework for large-scale multivariate datasets. A common strategy to deal with large-scale scientific simulation data is to partition the simulation domain and create statistical data summaries. Instead of storing the high-resolution raw data from the simulation, storing the compact statistical data summaries results in reduced storage overhead and alleviated I/O bottleneck. Such summaries, often represented in the form of statistical probability distributions, can serve various post-hoc analysis and visualization tasks. However, for multivariate simulation data using standard multivariate distributions for creating data summaries is not feasible. They are either storage inefficient or are computationally expensive to be estimated in simulation time (in situ) for large number of variables. In this work, using copula functions, we propose a flexible multivariate distribution-based data modeling and analysis framework that offers significant data reduction and can be used in an in situ environment. The framework also facilitates in storing the associated spatial information along with the multivariate distributions in an efficient representation. Using the proposed multivariate data summaries, we perform various multivariate post-hoc analyses like query-driven visualization and sampling-based visualization. We evaluate our proposed method on multiple real-world multivariate scientific datasets. To demonstrate the efficacy of our framework in an in situ environment, we apply it on a large-scale flow simulation.",Subhashis Hazarika;Soumya Dutta;Han-Wei Shen;Jen-Ping Chen,Subhashis Hazarika;Soumya Dutta;Han-Wei Shen;Jen-Ping Chen,The Department of Computer Science and EngineeringGRAVITY research groupThe Ohio State University;The Department of Computer Science and EngineeringGRAVITY research groupThe Ohio State University;The Department of Computer Science and EngineeringGRAVITY research groupThe Ohio State University;The Department of Mechanical and Aerospace EngineeringThe Ohio State University,10.1109/TVCG.2015.2467958;10.1109/TVCG.2007.70519;10.1109/TVCG.2015.2467952;10.1109/TVCG.2016.2598604;10.1109/TVCG.2015.2467436;10.1109/TVCG.2015.2467204;10.1109/TVCG.2017.2744099;10.1109/TVCG.2007.70615;10.1109/VAST.2015.7347634;10.1109/TVCG.2006.165;10.1109/TVCG.2015.2467411,"In situ processing,Distribution-based,Multivariate,Query-driven,Copula",0.0,3.0,1.0,64.0,nan,papers/scivis-tvcg/1214_hazarika.pdf
SciVis,2018,"Details-First, Show Context, Overview Last: Supporting Exploration of Viscous Fingers in Large-Scale Ensemble Simulations",10.1109/TVCG.2018.2864849,http://dx.doi.org/10.1109/TVCG.2018.2864849,1225,1235,J,"Visualization research often seeks designs that first establish an overview of the data, in accordance to the information seeking mantra: “Overview first, zoom and filter, then details on demand”. However, in computational fluid dynamics (CFD), as well as in other domains, there are many situations where such a spatial overview is not relevant or practical for users, for example when the experts already have a good mental overview of the data, or when an analysis of a large overall structure may not be related to the specific, information-driven tasks of users. Using scientific workflow theory and, as a vehicle, the problem of viscous finger evolution, we advocate an alternative model that allows domain experts to explore features of interest first, then explore the context around those features, and finally move to a potentially unfamiliar summarization overview. In a model instantiation, we show how a computational back-end can identify and track over time low-level, small features, then be used to filter the context of those features while controlling the complexity of the visualization, and finally to summarize and compare simulations. We demonstrate the effectiveness of this approach with an online web-based exploration of a total volume of data approaching half a billion seven-dimensional data points, and report supportive feedback provided by domain experts with respect to both the instantiation and the theoretical model.",Timothy Luciani;Andrew Thomas Burks;Cassiano Sugiyama;Jonathan Komperda;G. Elisabeta Marai,Timothy Luciani;Andrew Burks;Cassiano Sugiyama;Jonathan Komperda;G. Elisabeta Marai,"Electronic Visualization Laboratory, University of Illinois, Chicago;Electronic Visualization Laboratory, University of Illinois, Chicago;Favo Urban Agriculture, Brazil;Department of Mechanical & Industrial Engineering, University of Illinois, Chicago;Electronic Visualization Laboratory, University of Illinois, Chicago",10.1109/TVCG.2007.70599;10.1109/TVCG.2014.2346448;10.1109/TVCG.2015.2466838;10.1109/TVCG.2015.2468093;10.1109/TVCG.2009.141;10.1109/TVCG.2011.209;10.1109/TVCG.2017.2744459;10.1109/TVCG.2013.161;10.1109/TVCG.2014.2346744;10.1109/VAST.2006.261451;10.1109/TVCG.2009.111;10.1109/TVCG.2012.213;10.1109/TVCG.2009.108;10.1109/TVCG.2008.140,"theory,visualization design,details-first model,discourse paper,computational fluid dynamics",nan,4.0,1.0,72.0,nan,papers/scivis-tvcg/1225_luciani.pdf
VAST,2018,MAQUI: Interweaving Queries and Pattern Mining for Recursive Event Sequence Exploration,10.1109/TVCG.2018.2864886,http://dx.doi.org/10.1109/TVCG.2018.2864886,396,406,J,"Exploring event sequences by defining queries alone or by using mining algorithms alone is often not sufficient to support analysis. Analysts often interweave querying and mining in a recursive manner during event sequence analysis: sequences extracted as query results are used for mining patterns, patterns generated are incorporated into a new query for segmenting the sequences, and the resulting segments are mined or queried again. To support flexible analysis, we propose a framework that describes the process of interwoven querying and mining. Based on this framework, we developed MAQUI, a Mining And Querying User Interface that enables recursive event sequence exploration. To understand the efficacy of MAQUI, we conducted two case studies with domain experts. The findings suggest that the capability of interweaving querying and mining helps the participants articulate their questions and gain novel insights from their data.",Po-Ming Law;Zhicheng Liu;Sana Malik;Rahul C. Basole,Po-Ming Law;Zhicheng Liu;Sana Malik;Rahul C. Basole,Georgia Institute of Technology;Adobe Research;Adobe Research;Georgia Institute of Technology,10.1109/TVCG.2017.2745278;10.1109/TVCG.2017.2745083;10.1109/VAST.2016.7883512;10.1109/VAST.2006.261421;10.1109/TVCG.2017.2744199;10.1109/TVCG.2014.2346682;10.1109/TVCG.2015.2467622;10.1109/TVCG.2014.2346452;10.1109/TVCG.2016.2598797;10.1109/TVCG.2013.200;10.1109/VAST.2015.7347682;10.1109/TVCG.2014.2346574;10.1109/TVCG.2009.117;10.1109/VAST.2009.5332595,"Sequential pattern mining,temporal query,event sequence exploration",nan,5.0,3.0,46.0,nan,papers/vast-tvcg/396_law.pdf
VAST,2018,iForest: Interpreting Random Forests via Visual Analytics,10.1109/TVCG.2018.2864475,http://dx.doi.org/10.1109/TVCG.2018.2864475,407,416,J,"As an ensemble model that consists of many independent decision trees, random forests generate predictions by feeding the input to internal trees and summarizing their outputs. The ensemble nature of the model helps random forests outperform any individual decision tree. However, it also leads to a poor model interpretability, which significantly hinders the model from being used in fields that require transparent and explainable predictions, such as medical diagnosis and financial fraud detection. The interpretation challenges stem from the variety and complexity of the contained decision trees. Each decision tree has its unique structure and properties, such as the features used in the tree and the feature threshold in each tree node. Thus, a data input may lead to a variety of decision paths. To understand how a final prediction is achieved, it is desired to understand and compare all decision paths in the context of all tree structures, which is a huge challenge for any users. In this paper, we propose a visual analytic system aiming at interpreting random forest models and predictions. In addition to providing users with all the tree information, we summarize the decision paths in random forests, which eventually reflects the working mechanism of the model and reduces users' mental burden of interpretation. To demonstrate the effectiveness of our system, two usage scenarios and a qualitative user study are conducted.",Xun Zhao;Yanhong Wu;Dik Lun Lee;Weiwei Cui,Xun Zhao;Yanhong Wu;Dik Lun Lee;Weiwei Cui,Hong Kong University of Science and Technology;Visa Research;Hong Kong University of Science and Technology;Microsoft Research Asia,10.1109/TVCG.2017.2744378;10.1109/TVCG.2017.2745158;10.1109/VAST.2011.6102453,"Interpretable Machine Learning,Random Forests,Random Forest Visualization,Visual Analytics",nan,5.0,3.0,60.0,nan,papers/vast-tvcg/407_zhao.pdf
VAST,2018,Visual Progression Analysis of Event Sequence Data,10.1109/TVCG.2018.2864885,http://dx.doi.org/10.1109/TVCG.2018.2864885,417,426,J,"Event sequence data is common to a broad range of application domains, from security to health care to scholarly communication. This form of data captures information about the progression of events for an individual entity (e.g., a computer network device; a patient; an author) in the form of a series of time-stamped observations. Moreover, each event is associated with an event type (e.g., a computer login attempt, or a hospital discharge). Analyses of event sequence data have been shown to help reveal important temporal patterns, such as clinical paths resulting in improved outcomes, or an understanding of common career trajectories for scholars. Moreover, recent research has demonstrated a variety of techniques designed to overcome methodological challenges such as large volumes of data and high dimensionality. However, the effective identification and analysis of latent stages of progression, which can allow for variation within different but similarly evolving event sequences, remain a significant challenge with important real-world motivations. In this paper, we propose an unsupervised stage analysis algorithm to identify semantically meaningful progression stages as well as the critical events which help define those stages. The algorithm follows three key steps: (1) event representation estimation, (2) event sequence warping and alignment, and (3) sequence segmentation. We also present a novel visualization system, ET<sup>2</sup>, which interactively illustrates the results of the stage analysis algorithm to help reveal evolution patterns across stages. Finally, we report three forms of evaluation for ET<sup>2</sup>: (1) case studies with two real-world datasets, (2) interviews with domain expert users, and (3) a performance evaluation on the progression analysis algorithm and the visualization design.",Shunan Guo;Zhuochen Jin;David Gotz;Fan Du;Hongyuan Zha;Nan Cao,Shunan Guo;Zhuochen Jin;David Gotz;Fan Du;Hongyuan Zha;Nan Cao,"East China Normal University;iDVX labTongji University;University of North Carolina, Chapel Hill;University of Maryland;East China Normal University;iDVX labTongji University",10.1109/TVCG.2011.188;10.1109/TVCG.2017.2745278;10.1109/TVCG.2017.2745083;10.1109/VAST.2016.7883512;10.1109/TVCG.2014.2346682;10.1109/TVCG.2017.2745320;10.1109/TVCG.2014.2346574;10.1109/TVCG.2009.187;10.1109/TVCG.2014.2346913,"Progression Analysis,Visual Analysis,Event Sequence Data",nan,3.0,3.0,49.0,nan,papers/vast-tvcg/417_guo.pdf
VAST,2018,Duet: Helping Data Analysis Novices Conduct Pairwise Comparisons by Minimal Specification,10.1109/TVCG.2018.2864526,http://dx.doi.org/10.1109/TVCG.2018.2864526,427,437,J,"Data analysis novices often encounter barriers in executing low-level operations for pairwise comparisons. They may also run into barriers in interpreting the artifacts (e.g., visualizations) created as a result of the operations. We developed Duet, a visual analysis system designed to help data analysis novices conduct pairwise comparisons by addressing execution and interpretation barriers. To reduce the barriers in executing low-level operations during pairwise comparison, Duet employs minimal specification: when one object group (i.e. a group of records in a data table) is specified, Duet recommends object groups that are similar to or different from the specified one; when two object groups are specified, Duet recommends similar and different attributes between them. To lower the barriers in interpreting its recommendations, Duet explains the recommended groups and attributes using both visualizations and textual descriptions. We conducted a qualitative evaluation with eight participants to understand the effectiveness of Duet. The results suggest that minimal specification is easy to use and Duet's explanations are helpful for interpreting the recommendations despite some usability issues.",Po-Ming Law;Rahul C. Basole;Yanhong Wu,Po-Ming Law;Rahul C. Basole;Yanhong Wu,Georgia Institute of Technology;Georgia Institute of Technology;Visa Research,10.1109/TVCG.2011.188;10.1109/TVCG.2016.2598468;10.1109/VAST.2011.6102435;10.1109/TVCG.2017.2744199;10.1109/TVCG.2010.164;10.1109/TVCG.2017.2744684;10.1109/TVCG.2008.109;10.1109/TVCG.2015.2467195;10.1109/TVCG.2017.2745219;10.1109/TVCG.2015.2467191,"Pairwise comparison,novices,data analysis,automatic insight generation",nan,4.0,2.0,51.0,nan,papers/vast-tvcg/427_law.pdf
InfoVis,2018,Visualizing Uncertain Tropical Cyclone Predictions using Representative Samples from Ensembles of Forecast Tracks,10.1109/TVCG.2018.2865193,http://dx.doi.org/10.1109/TVCG.2018.2865193,882,891,J,"A common approach to sampling the space of a prediction is the generation of an ensemble of potential outcomes, where the ensemble's distribution reveals the statistical structure of the prediction space. For example, the US National Hurricane Center generates multiple day predictions for a storm's path, size, and wind speed, and then uses a Monte Carlo approach to sample this prediction into a large ensemble of potential storm outcomes. Various forms of summary visualizations are generated from such an ensemble, often using spatial spread to indicate its statistical characteristics. However, studies have shown that changes in the size of such summary glyphs, representing changes in the uncertainty of the prediction, are frequently confounded with other attributes of the phenomenon, such as its size or strength. In addition, simulation ensembles typically encode multivariate information, which can be difficult or confusing to include in a summary display. This problem can be overcome by directly displaying the ensemble as a set of annotated trajectories, however this solution will not be effective if ensembles are densely overdrawn or structurally disorganized. We propose to overcome these difficulties by selectively sampling the original ensemble, constructing a smaller representative and spatially well organized ensemble. This can be drawn directly as a set of paths that implicitly reveals the underlying spatial uncertainty distribution of the prediction. Since this approach does not use a visual channel to encode uncertainty, additional information can more easily be encoded in the display without leading to visual confusion. To demonstrate our argument, we describe the development of a visualization for ensembles of tropical cyclone forecast tracks, explaining how their spatial and temporal predictions, as well as other crucial storm characteristics such as size and intensity, can be clearly revealed. We verify the effectiveness of this visualization approach through a cognitive study exploring how storm damage estimates are affected by the density of tracks drawn, and by the presence or absence of annotating information on storm size and intensity.",Le Liu;Lace M. K. Padilla;Sarah H. Creem-Regehr;Donald H. House,Le Liu;Lace Padilla;Sarah H. Creem-Regehr;Donald H. House,"Magic Weaver Inc., Santa Clara, CA;Department of Psychology, Northwestern University, Evanston, IL;Department of Psychology, University of Utah, Salt Lake City, UT;School of Computing, Clemson University, Clemson, SC",10.1109/TVCG.2017.2743898;10.1109/TVCG.2010.181,"uncertainty visualization,hurricane forecasts,ensemble visualization,ensemble sampling,implicit uncertainty",nan,4.0,1.0,32.0,nan,papers/infovis-tvcg/882_liu.pdf
InfoVis,2018,Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data,10.1109/TVCG.2018.2864909,http://dx.doi.org/10.1109/TVCG.2018.2864909,892,902,J,"Animated representations of outcomes drawn from distributions (hypothetical outcome plots, or HOPs) are used in the media and other public venues to communicate uncertainty. HOPs greatly improve multivariate probability estimation over conventional static uncertainty visualizations and leverage the ability of the visual system to quickly, accurately, and automatically process the summary statistical properties of ensembles. However, it is unclear how well HOPs support applied tasks resembling real world judgments posed in uncertainty communication. We identify and motivate an appropriate task to investigate realistic judgments of uncertainty in the public domain through a qualitative analysis of uncertainty visualizations in the news. We contribute two crowdsourced experiments comparing the effectiveness of HOPs, error bars, and line ensembles for supporting perceptual decision-making from visualized uncertainty. Participants infer which of two possible underlying trends is more likely to have produced a sample of time series data by referencing uncertainty visualizations which depict the two trends with variability due to sampling error. By modeling each participant's accuracy as a function of the level of evidence presented over many repeated judgments, we find that observers are able to correctly infer the underlying trend in samples conveying a lower level of evidence when using HOPs rather than static aggregate uncertainty visualizations as a decision aid. Modeling approaches like ours contribute theoretically grounded and richly descriptive accounts of user perceptions to visualization evaluation.",Alex Kale;Francis Nguyen;Matthew Kay;Jessica Hullman,Alex Kale;Francis Nguyen;Matthew Kay;Jessica Hullman,University of Washington;University of Washington;University of Michigan;Northwestern University,10.1109/TVCG.2017.2743898;10.1109/TVCG.2007.70518;10.1109/TVCG.2017.2744359,"uncertainty visualization,hypothetical outcome plots,psychometric functions",0.0,7.0,2.0,66.0,nan,papers/infovis-tvcg/892_kale.pdf
InfoVis,2018,In Pursuit of Error: A Survey of Uncertainty Visualization Evaluation,10.1109/TVCG.2018.2864889,http://dx.doi.org/10.1109/TVCG.2018.2864889,903,913,J,"Understanding and accounting for uncertainty is critical to effectively reasoning about visualized data. However, evaluating the impact of an uncertainty visualization is complex due to the difficulties that people have interpreting uncertainty and the challenge of defining correct behavior with uncertainty information. Currently, evaluators of uncertainty visualization must rely on general purpose visualization evaluation frameworks which can be ill-equipped to provide guidance with the unique difficulties of assessing judgments under uncertainty. To help evaluators navigate these complexities, we present a taxonomy for characterizing decisions made in designing an evaluation of an uncertainty visualization. Our taxonomy differentiates six levels of decisions that comprise an uncertainty visualization evaluation: the behavioral targets of the study, expected effects from an uncertainty visualization, evaluation goals, measures, elicitation techniques, and analysis approaches. Applying our taxonomy to 86 user studies of uncertainty visualizations, we find that existing evaluation practice, particularly in visualization research, focuses on Performance and Satisfaction-based measures that assume more predictable and statistically-driven judgment behavior than is suggested by research on human judgment and decision making. We reflect on common themes in evaluation practice concerning the interpretation and semantics of uncertainty, the use of confidence reporting, and a bias toward evaluating performance as accuracy rather than decision quality. We conclude with a concrete set of recommendations for evaluators designed to reduce the mismatch between the conceptualization of uncertainty in visualization versus other fields.",Jessica Hullman;Xiaoli Qiao;Michael Correll;Alex Kale;Matthew Kay,Jessica Hullman;Xiaoli Qiao;Michael Correll;Alex Kale;Matthew Kay,Northwestern University;University of Washington;Tableau Software;University of Washington;University of Michigan,10.1109/TVCG.2015.2467752;10.1109/TVCG.2017.2743898;10.1109/TVCG.2013.126;10.1109/TVCG.2007.70530;10.1109/TVCG.2007.70518;10.1109/TVCG.2009.111,"Uncertainty visualization,user study,subjective confidence,probability distribution",0.0,6.0,2.0,94.0,nan,papers/infovis-tvcg/903_hullman.pdf
InfoVis,2018,Where's My Data? Evaluating Visualizations with Missing Data,10.1109/TVCG.2018.2864914,http://dx.doi.org/10.1109/TVCG.2018.2864914,914,924,J,Many real-world datasets are incomplete due to factors such as data collection failures or misalignments between fused datasets. Visualizations of incomplete datasets should allow analysts to draw conclusions from their data while effectively reasoning about the quality of the data and resulting conclusions. We conducted a pair of crowdsourced studies to measure how the methods used to impute and visualize missing data may influence analysts' perceptions of data quality and their confidence in their conclusions. Our experiments used different design choices for line graphs and bar charts to estimate averages and trends in incomplete time series datasets. Our results provide preliminary guidance for visualization designers to consider when working with incomplete data in different domains and scenarios.,Hayeong Song;Danielle Albers Szafir,Hayeong Song;Danielle Albers Szafir,University of Colorado;University of Colorado,10.1109/TVCG.2016.2598592;10.1109/VAST.2015.7347672;10.1109/TVCG.2011.185;10.1109/TVCG.2012.220;10.1109/TVCG.2014.2346298;10.1109/VISUAL.1999.809916;10.1109/TVCG.2013.183;10.1109/TVCG.2015.2467752;10.1109/TVCG.2015.2467951;10.1109/TVCG.2012.279;10.1109/TVCG.2015.2467591;10.1109/TVCG.2012.256;10.1109/VISUAL.1994.346317;10.1109/TVCG.2012.262;10.1109/VAST.2006.261424,"Information Visualization,Graphical Perception,Time Series Data,Data Wrangling,Imputation",nan,6.0,1.0,57.0,nan,papers/infovis-tvcg/914_song.pdf
InfoVis,2018,A Framework for Externalizing Implicit Error Using Visualization,10.1109/TVCG.2018.2864913,http://dx.doi.org/10.1109/TVCG.2018.2864913,925,935,J,"This paper presents a framework for externalizing and analyzing expert knowledge about discrepancies in data through the use of visualization. Grounded in an 18-month design study with global health experts, the framework formalizes the notion of data discrepancies as implicit error, both in global health data and more broadly. We use the term implicit error to describe measurement error that is inherent to and pervasive throughout a dataset, but that isn't explicitly accounted for or defined. Instead, implicit error exists in the minds of experts, is mainly qualitative, and is accounted for subjectively during expert interpretation of the data. Externalizing knowledge surrounding implicit error can assist in synchronizing, validating, and enhancing interpretation, and can inform error analysis and mitigation. The framework consists of a description of implicit error components that are important for downstream analysis, along with a process model for externalizing and analyzing implicit error using visualization. As a second contribution, we provide a rich, reflective, and verifiable description of our research process as an exemplar summary toward the ongoing inquiry into ways of increasing the validity and transferability of design study research.",Nina McCurdy;Julie Gerdes;Miriah D. Meyer,Nina Mccurdy;Julie Gerdes;Miriah Meyer,University of UtahSchool of Computing;Texas Tech UniversityCollege of Arts … Sciences;University of UtahSchool of Computing,10.1109/VAST.2011.6102457;10.1109/VAST.2010.5652885;10.1109/TVCG.2017.2743898;10.1109/TVCG.2017.2745240;10.1109/INFVIS.2005.1532134;10.1109/TVCG.2015.2467551;10.1109/VISUAL.2005.1532781;10.1109/TVCG.2007.70577;10.1109/TVCG.2013.132;10.1109/TVCG.2007.70589;10.1109/TVCG.2016.2598543,"implicit error,knowledge externalization,design study",0.0,4.0,5.0,75.0,nan,papers/infovis-tvcg/925_mccurdy.pdf
SciVis,2018,Exploring Time-Varying Multivariate Volume Data Using Matrix of Isosurface Similarity Maps,10.1109/TVCG.2018.2864808,http://dx.doi.org/10.1109/TVCG.2018.2864808,1236,1245,J,"We present a novel visual representation and interface named the matrix of isosurface similarity maps (MISM) for effective exploration of large time-varying multivariate volumetric data sets. MISM synthesizes three types of similarity maps (i.e., self, temporal, and variable similarity maps) to capture the essential relationships among isosurfaces of different variables and time steps. Additionally, it serves as the main visual mapping and navigation tool for examining the vast number of isosurfaces and exploring the underlying time-varying multivariate data set. We present temporal clustering, variable grouping, and interactive filtering to reduce the huge exploration space of MISM. In conjunction with the isovalue and isosurface views, MISM allows users to identify important isosurfaces or isosurface pairs and compare them over space, time, and value range. More importantly, we introduce path recommendation that suggests, animates, and compares traversal paths for effectively exploring MISM under varied criteria and at different levels-of-detail. A silhouette-based method is applied to render multiple surfaces of interest in a visually succinct manner. We demonstrate the effectiveness of our approach with case studies of several time-varying multivariate data sets and an ensemble data set, and evaluate our work with two domain experts.",Jun Tao;Martin Imre;Chaoli Wang;Nitesh V. Chawla;Hanqi Guo;Gokhan Sever;Seung Hyun Kim,Jun Tao;Martin Imre;Chaoli Wang;Nitesh V. Chawla;Hanqi Guo;Gökhan Sever;Seung Hyun Kim,University of Notre Dame;University of Notre Dame;University of Notre Dame;University of Notre Dame;Argonne National Laboratory;Argonne National Laboratory;The Ohio State University,10.1109/TVCG.2013.133;10.1109/TVCG.2012.284;10.1109/TVCG.2008.184;10.1109/TVCG.2011.246;10.1109/TVCG.2011.258;10.1109/TVCG.2008.116;10.1109/VISUAL.2005.1532857;10.1109/TVCG.2009.136;10.1109/TVCG.2015.2467431;10.1109/TVCG.2006.165;10.1109/TVCG.2013.213;10.1109/TVCG.2008.143;10.1109/VISUAL.1999.809910;10.1109/VISUAL.2005.1532792;10.1109/TVCG.2008.140;10.1109/TVCG.2006.164;10.1109/VISUAL.2003.1250402,"Time-varying multivariate data visualization,isosurface,similarity map,visual interface,path recommendation",0.0,1.0,1.0,41.0,nan,papers/scivis-tvcg/1236_tao.pdf
SciVis,2018,Visual Analysis of Spatia-temporal Relations of Pairwise Attributes in Unsteady Flow,10.1109/TVCG.2018.2864817,http://dx.doi.org/10.1109/TVCG.2018.2864817,1246,1256,J,"Despite significant advances in the analysis and visualization of unsteady flow, the interpretation of it's behavior still remains a challenge. In this work, we focus on the linear correlation and non-linear dependency of different physical attributes of unsteady flows to aid their study from a new perspective. Specifically, we extend the existing spatial correlation quantification, i.e. the Local Correlation Coefficient (LCC), to the spatio-temporal domain to study the correlation of attribute-pairs from both the Eulerian and Lagrangian views. To study the dependency among attributes, which need not be linear, we extend and compute the mutual information (MI) among attributes over time. To help visualize and interpret the derived correlation and dependency among attributes associated with a particle, we encode the correlation and dependency values on individual pathlines. Finally, to utilize the correlation and MI computation results to identify regions with interesting flow behavior, we propose a segmentation strategy of the flow domain based on the ranking of the strength of the attributes relations. We have applied our correlation and dependency metrics to a number of 2D and 3D unsteady flows with varying spatio-temporal kernel sizes to demonstrate and assess their effectiveness.",Marzieh Berenjkoub;Rodolfo Ostilla Monico;Robert S. Laramee;Guoning Chen,Marzieh Berenjkoub;Rodolfo Ostilla Monico;Robert S. Laramee;Guoning Chen,University of Houston;University of Houston;Swansea University;University of Houston,10.1109/TVCG.2010.131;10.1109/VISUAL.2004.99;10.1109/TVCG.2010.198;10.1109/TVCG.2015.2467200;10.1109/TVCG.2009.200;10.1109/TVCG.2010.131;10.1109/TVCG.2013.133;10.1109/TVCG.2011.249,"Unsteady flow,correlation study,mutual information",0.0,1.0,2.0,63.0,nan,papers/scivis-tvcg/1246_berenjkoub.pdf
SciVis,2018,Time-Dependent Flow seen through Approximate Observer Killing Fields,10.1109/TVCG.2018.2864839,http://dx.doi.org/10.1109/TVCG.2018.2864839,1257,1266,J,"Flow fields are usually visualized relative to a global observer, i.e., a single frame of reference. However, often no global frame can depict all flow features equally well. Likewise, objective criteria for detecting features such as vortices often use either a global reference frame, or compute a separate frame for each point in space and time. We propose the first general framework that enables choosing a smooth trade-off between these two extremes. Using global optimization to minimize specific differential geometric properties, we compute a time-dependent observer velocity field that describes the motion of a continuous field of observers adapted to the input flow. This requires developing the novel notion of an observed time derivative. While individual observers are restricted to rigid motions, overall we compute an approximate Killing field, corresponding to almost-rigid motion. This enables continuous transitions between different observers. Instead of focusing only on flow features, we furthermore develop a novel general notion of visualizing how all observers jointly perceive the input field. This in fact requires introducing the concept of an observation time, with respect to which a visualization is computed. We develop the corresponding notions of observed stream, path, streak, and time lines. For efficiency, these characteristic curves can be computed using standard approaches, by first transforming the input field accordingly. Finally, we prove that the input flow perceived by the observer field is objective. This makes derived flow features, such as vortices, objective as well.",Markus Hadwiger;Matej Mlejnek;Thomas Theußl;Peter Rautek,Markus Hadwiger;Matej Mlejnek;Thomas Theußl;Peter Rautek,"King Abdullah University of Science and Technology (KAUST), Visual Computing Center, Saudi Arabia;King Abdullah University of Science and Technology (KAUST), Visual Computing Center, Saudi Arabia;King Abdullah University of Science and Technology (KAUST), Core Labs, Saudi Arabia;King Abdullah University of Science and Technology (KAUST), Visual Computing Center, Saudi Arabia",10.1109/TVCG.2015.2467200;10.1109/VISUAL.1999.809896;10.1109/VISUAL.1997.663898;10.1109/TVCG.2008.163;10.1109/TVCG.2007.70545;10.1109/TVCG.2010.198;10.1109/TVCG.2007.70557,"Flow visualization,observer frames of reference,Killing vector fields,infinitesimal isometries,Lie derivatives,objectivity",0.0,1.0,1.0,52.0,nan,papers/scivis-tvcg/1257_hadwiger.pdf
